<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"slashyouth.github.io","root":"/","scheme":"Gemini","version":"8.0.0-rc.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="决策树 概述决策树（Decision Tree）算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树。 决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。 决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。">
<meta property="og:type" content="article">
<meta property="og:title" content="第3章 决策树">
<meta property="og:url" content="https://slashyouth.github.io/2020/06/01/3.%E5%86%B3%E7%AD%96%E6%A0%91/index.html">
<meta property="og:site_name" content="Slash Youth">
<meta property="og:description" content="决策树 概述决策树（Decision Tree）算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树。 决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。 决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/%E5%86%B3%E7%AD%96%E6%A0%91-%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg">
<meta property="og:image" content="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/DT_%E6%B5%B7%E6%B4%8B%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE.png">
<meta property="og:image" content="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/%E7%86%B5%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.jpg">
<meta property="article:published_time" content="2020-06-01T13:07:50.961Z">
<meta property="article:modified_time" content="2020-06-01T13:01:58.743Z">
<meta property="article:author" content="Jack Can">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/%E5%86%B3%E7%AD%96%E6%A0%91-%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg">

<link rel="canonical" href="https://slashyouth.github.io/2020/06/01/3.%E5%86%B3%E7%AD%96%E6%A0%91/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>第3章 决策树 | Slash Youth</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line toggle-line-first"></span>
        <span class="toggle-line toggle-line-middle"></span>
        <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Slash Youth</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Keep Moving...</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">2</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">5</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">19</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/Slashyouth" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://slashyouth.github.io/2020/06/01/3.%E5%86%B3%E7%AD%96%E6%A0%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.png">
      <meta itemprop="name" content="Jack Can">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Slash Youth">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          第3章 决策树
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-06-01 21:07:50 / 修改时间：21:01:58" itemprop="dateCreated datePublished" datetime="2020-06-01T21:07:50+08:00">2020-06-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>8.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="决策树-概述"><a href="#决策树-概述" class="headerlink" title="决策树 概述"></a>决策树 概述</h2><p><code>决策树（Decision Tree）算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树。</code></p>
<p><code>决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。</code></p>
<p><code>决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。</code></p>
<a id="more"></a>
<h2 id="决策树-场景"><a href="#决策树-场景" class="headerlink" title="决策树 场景"></a>决策树 场景</h2><p>一个叫做 “二十个问题” 的游戏，游戏的规则很简单：参与游戏的一方在脑海中想某个事物，其他参与者向他提问，只允许提 20 个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围，最后得到游戏的答案。</p>
<p>一个邮件分类系统，大致工作流程如下：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/%E5%86%B3%E7%AD%96%E6%A0%91-%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg" alt="决策树-流程图" title="决策树示例流程图"></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先检测发送邮件域名地址。如果地址为 myEmployer<span class="selector-class">.com</span>, 则将其放在分类 <span class="string">"无聊时需要阅读的邮件"</span>中。</span><br><span class="line">如果邮件不是来自这个域名，则检测邮件内容里是否包含单词 <span class="string">"曲棍球"</span> , 如果包含则将邮件归类到 <span class="string">"需要及时处理的朋友邮件"</span>, </span><br><span class="line">如果不包含则将邮件归类到 <span class="string">"无需阅读的垃圾邮件"</span> 。</span><br></pre></td></tr></table></figure>

<p>决策树的定义：</p>
<p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。</p>
<p>用决策树对需要测试的实例进行分类：从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。</p>
<h2 id="决策树-原理"><a href="#决策树-原理" class="headerlink" title="决策树 原理"></a>决策树 原理</h2><h3 id="决策树-须知概念"><a href="#决策树-须知概念" class="headerlink" title="决策树 须知概念"></a>决策树 须知概念</h3><h4 id="信息熵-amp-信息增益"><a href="#信息熵-amp-信息增益" class="headerlink" title="信息熵 &amp; 信息增益"></a>信息熵 &amp; 信息增益</h4><p>熵（entropy）：<br>熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。</p>
<p>信息论（information theory）中的熵（香农熵）：<br>是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。</p>
<p>信息增益（information gain）：<br>在划分数据集前后信息发生的变化称为信息增益。</p>
<h3 id="决策树-工作原理"><a href="#决策树-工作原理" class="headerlink" title="决策树 工作原理"></a>决策树 工作原理</h3><p>如何构造一个决策树?<br/><br>我们使用 createBranch() 方法，如下所示：</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def createBranch():</span><br><span class="line">'''</span><br><span class="line">此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。</span><br><span class="line">'''</span><br><span class="line"><span class="code">    检测数据集中的所有数据的分类标签是否相同:</span></span><br><span class="line"><span class="code">        If so return 类标签</span></span><br><span class="line"><span class="code">        Else:</span></span><br><span class="line"><span class="code">            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）</span></span><br><span class="line"><span class="code">            划分数据集</span></span><br><span class="line"><span class="code">            创建分支节点</span></span><br><span class="line"><span class="code">                for 每个划分的子集</span></span><br><span class="line"><span class="code">                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中</span></span><br><span class="line"><span class="code">            return 分支节点</span></span><br></pre></td></tr></table></figure>

<h3 id="决策树-开发流程"><a href="#决策树-开发流程" class="headerlink" title="决策树 开发流程"></a>决策树 开发流程</h3><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据：可以使用任何方法。</span><br><span class="line">准备数据：树构造算法 <span class="comment">(这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)</span></span><br><span class="line">分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。</span><br><span class="line">训练算法：构造树的数据结构。</span><br><span class="line">测试算法：使用训练好的树计算错误率。</span><br><span class="line">使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。</span><br></pre></td></tr></table></figure>

<h3 id="决策树-算法特点"><a href="#决策树-算法特点" class="headerlink" title="决策树 算法特点"></a>决策树 算法特点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">优点：计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。</span><br><span class="line">缺点：容易过拟合。</span><br><span class="line">适用数据类型：数值型和标称型。</span><br></pre></td></tr></table></figure>

<h2 id="决策树-项目案例"><a href="#决策树-项目案例" class="headerlink" title="决策树 项目案例"></a>决策树 项目案例</h2><h3 id="项目案例1-判定鱼类和非鱼类"><a href="#项目案例1-判定鱼类和非鱼类" class="headerlink" title="项目案例1: 判定鱼类和非鱼类"></a>项目案例1: 判定鱼类和非鱼类</h3><h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p>根据以下 2 个特征，将动物分成两类：鱼类和非鱼类。</p>
<p>特征：</p>
<ol>
<li>不浮出水面是否可以生存</li>
<li>是否有脚蹼</li>
</ol>
<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><p><a href="/src/py2.x/ml/3.DecisionTree/DecisionTree.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">收集数据：可以使用任何方法</span><br><span class="line">准备数据：树构造算法（这里使用的是ID3算法，因此数值型数据必须离散化。）</span><br><span class="line">分析数据：可以使用任何方法，构造树完成之后，我们可以将树画出来。</span><br><span class="line">训练算法：构造树结构</span><br><span class="line">测试算法：使用习得的决策树执行分类</span><br><span class="line">使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据：可以使用任何方法</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/DT_%E6%B5%B7%E6%B4%8B%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE.png" alt="海洋生物数据"></p>
<p>我们利用 createDataSet() 函数输入数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">            [<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">            [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'no'</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>, <span class="string">'flippers'</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br></pre></td></tr></table></figure>
<blockquote>
<p>准备数据：树构造算法</p>
</blockquote>
<p>此处，由于我们输入的数据本身就是离散化数据，所以这一步就省略了。</p>
<blockquote>
<p>分析数据：可以使用任何方法，构造树完成之后，我们可以将树画出来。</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/%E7%86%B5%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.jpg" alt="熵的计算公式"></p>
<p>计算给定数据集的香农熵的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="comment"># 求list的长度，表示计算参与训练的数据量</span></span><br><span class="line">    numEntries = len(dataSet)</span><br><span class="line">    <span class="comment"># 计算分类标签label出现的次数</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="comment"># the the number of unique elements and their occurrence</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 将当前实例的标签存储，即每一行数据的最后一个数据代表的是标签</span></span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># 为所有可能的分类创建字典，如果当前的键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对于 label 标签的占比，求出 label 标签的香农熵</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        <span class="comment"># 使用所有类标签的发生频率计算类别出现的概率。</span></span><br><span class="line">        prob = float(labelCounts[key])/numEntries</span><br><span class="line">        <span class="comment"># 计算香农熵，以 2 为底求对数</span></span><br><span class="line">        shannonEnt -= prob * log(prob, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure>

<p>按照给定特征划分数据集</p>
<p><code>将指定特征的特征值等于 value 的行剩下列作为子数据集。</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, index, value)</span>:</span></span><br><span class="line">    <span class="string">"""splitDataSet(通过遍历dataSet数据集，求出index对应的colnum列的值为value的行)</span></span><br><span class="line"><span class="string">        就是依据index列进行分类，如果index列的数据等于 value的时候，就要将 index 划分到我们创建的新的数据集中</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 数据集                 待划分的数据集</span></span><br><span class="line"><span class="string">        index 表示每一行的index列        划分数据集的特征</span></span><br><span class="line"><span class="string">        value 表示index列对应的value值   需要返回的特征的值。</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        index列为value的数据集【该数据集需要排除index列】</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet: </span><br><span class="line">        <span class="comment"># index列为value的数据集【该数据集需要排除index列】</span></span><br><span class="line">        <span class="comment"># 判断index列的值是否为value</span></span><br><span class="line">        <span class="keyword">if</span> featVec[index] == value:</span><br><span class="line">            <span class="comment"># chop out index used for splitting</span></span><br><span class="line">            <span class="comment"># [:index]表示前index行，即若 index 为2，就是取 featVec 的前 index 行</span></span><br><span class="line">            reducedFeatVec = featVec[:index]</span><br><span class="line">            <span class="string">'''</span></span><br><span class="line"><span class="string">            请百度查询一下： extend和append的区别</span></span><br><span class="line"><span class="string">            music_media.append(object) 向列表中添加一个对象object</span></span><br><span class="line"><span class="string">            music_media.extend(sequence) 把一个序列seq的内容添加到列表中 (跟 += 在list运用类似， music_media += sequence)</span></span><br><span class="line"><span class="string">            1、使用append的时候，是将object看作一个对象，整体打包添加到music_media对象中。</span></span><br><span class="line"><span class="string">            2、使用extend的时候，是将sequence看作一个序列，将这个序列和music_media序列合并，并放在其后面。</span></span><br><span class="line"><span class="string">            music_media = []</span></span><br><span class="line"><span class="string">            music_media.extend([1,2,3])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果：</span></span><br><span class="line"><span class="string">            #[1, 2, 3]</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            music_media.append([4,5,6])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果：</span></span><br><span class="line"><span class="string">            #[1, 2, 3, [4, 5, 6]]</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            music_media.extend([7,8,9])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果：</span></span><br><span class="line"><span class="string">            #[1, 2, 3, [4, 5, 6], 7, 8, 9]</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line">            reducedFeatVec.extend(featVec[index+<span class="number">1</span>:])</span><br><span class="line">            <span class="comment"># [index+1:]表示从跳过 index 的 index+1行，取接下来的数据</span></span><br><span class="line">            <span class="comment"># 收集结果值 index列为value的行【该行需要排除index列】</span></span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br></pre></td></tr></table></figure>

<p>选择最好的数据集划分方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""chooseBestFeatureToSplit(选择最好的特征)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bestFeature 最优的特征列</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 求第一行有多少列的 Feature, 最后一列是label列嘛</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    <span class="comment"># 数据集的原始信息熵</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    <span class="comment"># 最优的信息增益值, 和最优的Featurn编号</span></span><br><span class="line">    bestInfoGain, bestFeature = <span class="number">0.0</span>, <span class="number">-1</span></span><br><span class="line">    <span class="comment"># iterate over all the features</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">        <span class="comment"># create a list of all the examples of this feature</span></span><br><span class="line">        <span class="comment"># 获取对应的feature下的所有数据</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        <span class="comment"># get a set of unique values</span></span><br><span class="line">        <span class="comment"># 获取剔重后的集合，使用set对list数据进行去重</span></span><br><span class="line">        uniqueVals = set(featList)</span><br><span class="line">        <span class="comment"># 创建一个临时的信息熵</span></span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 遍历某一列的value集合，计算该列的信息熵 </span></span><br><span class="line">        <span class="comment"># 遍历当前特征中的所有唯一属性值，对每个唯一属性值划分一次数据集，计算数据集的新熵值，并对所有唯一特征值得到的熵求和。</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            <span class="comment"># 计算概率</span></span><br><span class="line">            prob = len(subDataSet)/float(len(dataSet))</span><br><span class="line">            <span class="comment"># 计算信息熵</span></span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">        <span class="comment"># gain[信息增益]: 划分数据集前后的信息变化， 获取信息熵最大的值</span></span><br><span class="line">        <span class="comment"># 信息增益是熵的减少或者是数据无序度的减少。最后，比较所有特征中的信息增益，返回最好特征划分的索引值。</span></span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'infoGain='</span>, infoGain, <span class="string">'bestFeature='</span>, i, baseEntropy, newEntropy</span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure>

<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">问：上面的 <span class="keyword">new</span><span class="type">Entropy</span> 为什么是根据子集计算的呢？</span><br><span class="line">答：因为我们在根据一个特征计算香农熵的时候，该特征的分类值是相同，这个特征这个分类的香农熵为 <span class="number">0</span>；</span><br><span class="line">这就是为什么计算新的香农熵的时候使用的是子集。</span><br></pre></td></tr></table></figure>

<blockquote>
<p>训练算法：构造树的数据结构</p>
</blockquote>
<p>创建树的函数代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, labels)</span>:</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    <span class="comment"># 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行</span></span><br><span class="line">    <span class="comment"># 第一个停止条件：所有的类标签完全相同，则直接返回该类标签。</span></span><br><span class="line">    <span class="comment"># count() 函数是统计括号中的值在list中出现的次数</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果</span></span><br><span class="line">    <span class="comment"># 第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。</span></span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择最优的列，得到最优列对应的label含义</span></span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    <span class="comment"># 获取label的名称</span></span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    <span class="comment"># 初始化myTree</span></span><br><span class="line">    myTree = &#123;bestFeatLabel: &#123;&#125;&#125;</span><br><span class="line">    <span class="comment"># 注：labels列表是可变对象，在PYTHON函数中作为参数时传址引用，能够被全局修改</span></span><br><span class="line">    <span class="comment"># 所以这行代码导致函数外的同名变量被删除了元素，造成例句无法执行，提示'no surfacing' is not in list</span></span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    <span class="comment"># 取出最优列，然后它的branch做分类</span></span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        <span class="comment"># 求出剩余的标签label</span></span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        <span class="comment"># 遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree()</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)</span><br><span class="line">        <span class="comment"># print 'myTree', value, myTree</span></span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法：使用决策树执行分类</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree, featLabels, testVec)</span>:</span></span><br><span class="line">    <span class="string">"""classify(给输入的节点，进行分类)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        inputTree  决策树模型</span></span><br><span class="line"><span class="string">        featLabels Feature标签对应的名称</span></span><br><span class="line"><span class="string">        testVec    测试输入的数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        classLabel 分类的结果值，需要映射label才能知道名称</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 获取tree的根节点对于的key值</span></span><br><span class="line">    firstStr = list(inputTree.keys())[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 通过key得到根节点对应的value</span></span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    <span class="comment"># 判断根节点名称获取根节点在label中的先后顺序，这样就知道输入的testVec怎么开始对照树来做分类</span></span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    <span class="comment"># 测试数据，找到根节点对应的label位置，也就知道从输入的数据的第几位来开始分类</span></span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'+++'</span>, firstStr, <span class="string">'xxx'</span>, secondDict, <span class="string">'---'</span>, key, <span class="string">'&gt;&gt;&gt;'</span>, valueOfFeat</span><br><span class="line">    <span class="comment"># 判断分枝是否结束: 判断valueOfFeat是否是dict类型</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(valueOfFeat, dict):</span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。</p>
</blockquote>
<h3 id="项目案例2-使用决策树预测隐形眼镜类型"><a href="#项目案例2-使用决策树预测隐形眼镜类型" class="headerlink" title="项目案例2: 使用决策树预测隐形眼镜类型"></a>项目案例2: 使用决策树预测隐形眼镜类型</h3><p><a href="/src/py2.x/ml/3.DecisionTree/DecisionTree.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py</a></p>
<h4 id="项目概述-1"><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4><p>隐形眼镜类型包括硬材质、软材质以及不适合佩戴隐形眼镜。我们需要使用决策树预测患者需要佩戴的隐形眼镜类型。</p>
<h4 id="开发流程-1"><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4><ol>
<li>收集数据: 提供的文本文件。</li>
<li>解析数据: 解析 tab 键分隔的数据行</li>
<li>分析数据: 快速检查数据，确保正确地解析数据内容，使用 createPlot() 函数绘制最终的树形图。</li>
<li>训练算法: 使用 createTree() 函数。</li>
<li>测试算法: 编写测试函数验证决策树可以正确分类给定的数据实例。</li>
<li>使用算法: 存储树的数据结构，以便下次使用时无需重新构造树。</li>
</ol>
<blockquote>
<p>收集数据：提供的文本文件</p>
</blockquote>
<p>文本文件数据格式如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">young</span>	<span class="string">myope</span>	<span class="literal">no</span>	<span class="string">reduced</span>	<span class="literal">no</span> <span class="string">lenses</span></span><br><span class="line"><span class="string">pre</span>	<span class="string">myope</span>	<span class="literal">no</span>	<span class="string">reduced</span>	<span class="literal">no</span> <span class="string">lenses</span></span><br><span class="line"><span class="string">presbyopic</span>	<span class="string">myope</span>	<span class="literal">no</span>	<span class="string">reduced</span>	<span class="literal">no</span> <span class="string">lenses</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>解析数据：解析 tab 键分隔的数据行</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lecses = [inst.strip().split(<span class="string">'\t'</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readlines()]</span><br><span class="line">lensesLabels = [<span class="string">'age'</span>, <span class="string">'prescript'</span>, <span class="string">'astigmatic'</span>, <span class="string">'tearRate'</span>]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据：快速检查数据，确保正确地解析数据内容，使用 createPlot() 函数绘制最终的树形图。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>treePlotter.createPlot(lensesTree)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>训练算法：使用 createTree() 函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>lensesTree = trees.createTree(lenses, lensesLabels)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lensesTree</span><br><span class="line">&#123;<span class="string">'tearRate'</span>: &#123;<span class="string">'reduced'</span>: <span class="string">'no lenses'</span>, <span class="string">'normal'</span>: &#123;<span class="string">'astigmatic'</span>:&#123;<span class="string">'yes'</span>:</span><br><span class="line">&#123;<span class="string">'prescript'</span>:&#123;<span class="string">'hyper'</span>:&#123;<span class="string">'age'</span>:&#123;<span class="string">'pre'</span>:<span class="string">'no lenses'</span>, <span class="string">'presbyopic'</span>:</span><br><span class="line"><span class="string">'no lenses'</span>, <span class="string">'young'</span>:<span class="string">'hard'</span>&#125;&#125;, <span class="string">'myope'</span>:<span class="string">'hard'</span>&#125;&#125;, <span class="string">'no'</span>:&#123;<span class="string">'age'</span>:&#123;<span class="string">'pre'</span>:</span><br><span class="line"><span class="string">'soft'</span>, <span class="string">'presbyopic'</span>:&#123;<span class="string">'prescript'</span>: &#123;<span class="string">'hyper'</span>:<span class="string">'soft'</span>, <span class="string">'myope'</span>:</span><br><span class="line"><span class="string">'no lenses'</span>&#125;&#125;, <span class="string">'young'</span>:<span class="string">'soft'</span>&#125;&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 编写测试函数验证决策树可以正确分类给定的数据实例。</p>
</blockquote>
<blockquote>
<p>使用算法: 存储树的数据结构，以便下次使用时无需重新构造树。</p>
</blockquote>
<p>使用 pickle 模块存储决策树</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree, filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = open(filename, <span class="string">'wb'</span>)</span><br><span class="line">    pickle.dump(inputTree, fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = open(filename, <span class="string">'rb'</span>)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure>


<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a>      </li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"><i class="fa fa-tag"></i> Python</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/31/2.k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/" rel="prev" title="第2章 k-近邻算法">
      <i class="fa fa-chevron-left"></i> 第2章 k-近邻算法
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/06/01/5.Logistic%E5%9B%9E%E5%BD%92/" rel="next" title="第5章 Logistic回归">
      第5章 Logistic回归 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树-概述"><span class="nav-number">1.</span> <span class="nav-text">决策树 概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树-场景"><span class="nav-number">2.</span> <span class="nav-text">决策树 场景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树-原理"><span class="nav-number">3.</span> <span class="nav-text">决策树 原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树-须知概念"><span class="nav-number">3.1.</span> <span class="nav-text">决策树 须知概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#信息熵-amp-信息增益"><span class="nav-number">3.1.1.</span> <span class="nav-text">信息熵 &amp; 信息增益</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树-工作原理"><span class="nav-number">3.2.</span> <span class="nav-text">决策树 工作原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树-开发流程"><span class="nav-number">3.3.</span> <span class="nav-text">决策树 开发流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树-算法特点"><span class="nav-number">3.4.</span> <span class="nav-text">决策树 算法特点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树-项目案例"><span class="nav-number">4.</span> <span class="nav-text">决策树 项目案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#项目案例1-判定鱼类和非鱼类"><span class="nav-number">4.1.</span> <span class="nav-text">项目案例1: 判定鱼类和非鱼类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#项目概述"><span class="nav-number">4.1.1.</span> <span class="nav-text">项目概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#开发流程"><span class="nav-number">4.1.2.</span> <span class="nav-text">开发流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#项目案例2-使用决策树预测隐形眼镜类型"><span class="nav-number">4.2.</span> <span class="nav-text">项目案例2: 使用决策树预测隐形眼镜类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#项目概述-1"><span class="nav-number">4.2.1.</span> <span class="nav-text">项目概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#开发流程-1"><span class="nav-number">4.2.2.</span> <span class="nav-text">开发流程</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jack Can"
      src="/images/%E5%A4%B4%E5%83%8F.png">
  <p class="site-author-name" itemprop="name">Jack Can</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Slashyouth" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Slashyouth" rel="noopener" target="_blank"><i class="fab fa-github-alt fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/slashyouth" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;slashyouth" rel="noopener" target="_blank"><i class="fab fa-connectdevelop fa-fw"></i>CSDN</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jack Can</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">173k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:38</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '9afe362730e6bf584da8',
      clientSecret: '705d6e86013d354478bc715c5a85d8bad93499a7',
      repo        : 'gitalk-commnet',
      owner       : 'Slashyouth',
      admin       : ['Slashyouth'],
      id          : 'be355175e0f0a57b9434411a1f63e4c4',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
