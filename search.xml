<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Pandas学习笔记---001</title>
    <url>/2020/05/28/Pandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0---001/</url>
    <content><![CDATA[<h1 id="Pandas学习笔记-——001"><a href="#Pandas学习笔记-——001" class="headerlink" title="Pandas学习笔记 ——001"></a>Pandas学习笔记 ——001</h1><h2 id="1-数据类型"><a href="#1-数据类型" class="headerlink" title="1. 数据类型"></a>1. 数据类型</h2><blockquote>
<p>Series：带标签的一维数组<br>DataFrame：带标签的，大小可变的，二维异构表格</p>
</blockquote>
<hr>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 生成Series</span></span><br><span class="line">s = pd.Series([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, np.nan, <span class="number">6</span>, <span class="number">8</span>])</span><br><span class="line">print(s)</span><br><span class="line"><span class="number">0</span>    <span class="number">1.0</span></span><br><span class="line"><span class="number">1</span>    <span class="number">3.0</span></span><br><span class="line"><span class="number">2</span>    <span class="number">5.0</span></span><br><span class="line"><span class="number">3</span>    NaN</span><br><span class="line"><span class="number">4</span>    <span class="number">6.0</span></span><br><span class="line"><span class="number">5</span>    <span class="number">8.0</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>
<h2 id="2-使用技巧"><a href="#2-使用技巧" class="headerlink" title="2. 使用技巧"></a>2. 使用技巧</h2><h3 id="1-sort-indedx和sort-value"><a href="#1-sort-indedx和sort-value" class="headerlink" title="1. sort_indedx和sort_value"></a>1. sort_indedx和sort_value</h3><blockquote>
<p>sort_index(axis=0, level=None, ascending=True, inplace=False, kind=‘quicksort’, na_position=‘last’, sort_remaining=True, ignore_index: bool = False,)<br>axis:0按行名排序，1按列名排序<br>ascending：默认True升序排列；False降序排列<br>inplace：默认False，否则排序之后的数据直接替换原来的数据框<br>kind：排序方法，{‘quicksort’, ‘mergesort’, ‘heapsort’}, default ‘quicksort’。似乎不用太关心<br>na_position：缺失值默认排在最后{“first”,“last”}       </p>
</blockquote>
<hr>
<blockquote>
<p> sort_values(by, axis=0, ascending=True, inplace=False, kind=”quicksort”, na_position=”last”, ignore_index=False)<br>by: str or list of str；如果axis=0，那么by=”列名”；如果axis=1，那么by=”行名” # 必须给出参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">'b'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>],<span class="string">'a'</span>:[<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>],<span class="string">'c'</span>:[<span class="number">1</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">2</span>]&#125;,index=[<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>]) </span><br><span class="line">    b   a   c</span><br><span class="line"><span class="number">2</span>   <span class="number">1</span>   <span class="number">4</span>   <span class="number">1</span></span><br><span class="line"><span class="number">0</span>   <span class="number">2</span>   <span class="number">3</span>   <span class="number">3</span></span><br><span class="line"><span class="number">1</span>   <span class="number">3</span>   <span class="number">2</span>   <span class="number">8</span></span><br><span class="line"><span class="number">3</span>   <span class="number">2</span>   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">按b列升序排序: df.sort_values(by=<span class="string">'b'</span>) <span class="comment"># 等同于df.sort_values(by='b',axis=0)</span></span><br><span class="line">先按b列降序,再按a列升序排序: df.sort_values(by=[<span class="string">'b'</span>,<span class="string">'a'</span>],axis=<span class="number">0</span>,ascending=[<span class="literal">False</span>,<span class="literal">True</span>]) <span class="comment"># 等同于df.sort_values(by=['b','a'],axis=0,ascending=[False,True])      </span></span><br><span class="line">按行<span class="number">3</span>升序排列: df.sort_values(by=<span class="number">3</span>,axis=<span class="number">1</span>) <span class="comment"># 必须指定axis=1     </span></span><br><span class="line">按行<span class="number">3</span>升序,行<span class="number">0</span>降排列:  df.sort_values(by=[<span class="number">3</span>,<span class="number">0</span>],axis=<span class="number">1</span>,ascending=[<span class="literal">True</span>,<span class="literal">False</span>])</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="2-对于无效数据的处理"><a href="#2-对于无效数据的处理" class="headerlink" title="2. 对于无效数据的处理"></a>2. 对于无效数据的处理</h3><blockquote>
<p>空值：在pandas中的空值是””<br>缺失值：在dataframe中为nan或者naT（缺失时间），在series中为none或者nan即可<br>相关函数： df.dropna()、df.fillna()、df.isnull()、df.isna()</p>
<blockquote>
<p>函数： df.dropna(axis=0, how=’any’, thresh=None, subset=None, inplace=False)<br>删除表中<strong>全部</strong>为NaN的<strong>行</strong>: df.dropna(axis=0,how=’all’)<br>删除表中<strong>含有任何</strong>NaN的<strong>行</strong>： df.dropna(axis=0,how=’any’)<br>删除表中<strong>全部</strong>为NaN的<strong>列</strong>： df.dropna(axis=1,how=’all’)<br>删除表中<strong>含有任何</strong>NaN的<strong>列</strong>:  df.dropna(axis=1,how=’any’)<br>thresh(int): axis中至少有int个非缺失值，否则删除<br>subset: array-like, optional,Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.  </p>
<hr>
<p>函数： df.fillna(self, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None)<br>method{‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default None 说明用前面数据‘ffill’/‘pad’替换后面NaN数据 ‘backfill’/‘bfill’则相反          </p>
</blockquote>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas学习笔记---002</title>
    <url>/2020/05/31/Pandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0---002/</url>
    <content><![CDATA[<h1 id="Pandas学习笔记—002"><a href="#Pandas学习笔记—002" class="headerlink" title="Pandas学习笔记—002"></a>Pandas学习笔记—002</h1><h2 id="基础用法"><a href="#基础用法" class="headerlink" title="基础用法"></a>基础用法</h2><h3 id="1-df-head-n-和-df-tail"><a href="#1-df-head-n-和-df-tail" class="headerlink" title="1. df.head(n) 和 df.tail()"></a>1. df.head(n) 和 df.tail()</h3><blockquote>
<p>默认n=5，同时n还可以取负值，对于head来说就是展示df[:-n],对于tail来说就是展示df[-n:]   </p>
</blockquote>
<h3 id="2-合并重叠数据集"><a href="#2-合并重叠数据集" class="headerlink" title="2. 合并重叠数据集"></a>2. 合并重叠数据集</h3><blockquote>
<p>有时，要合并两个相似的数据集，两个数据集里的其中一个的数据比另一个多。<br>比如，展示特定经济指标的两个数据序列，其中一个是“高质量”指标，<a id="more"></a><br>另一个是“低质量”指标。一般来说，低质量序列可能包含更多的历史数据，或覆盖更广的数据。<br>因此，要合并这两个 DataFrame 对象，其中一个 DataFrame 中的缺失值将按指定条件用另一个<br>DataFrame 里类似标签中的数据进行填充。                    </p>
<blockquote>
<p>函数df1.combine_first(df2): 由df2只填df1的NaN值    </p>
</blockquote>
</blockquote>
<h3 id="3-描述性统计"><a href="#3-描述性统计" class="headerlink" title="3. 描述性统计"></a>3. 描述性统计</h3><table>
<thead>
<tr>
<th align="center">函数</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center">count</td>
<td align="center">统计非空值数量</td>
</tr>
<tr>
<td align="center">sum</td>
<td align="center">汇总值</td>
</tr>
<tr>
<td align="center">mean</td>
<td align="center">平均值</td>
</tr>
<tr>
<td align="center">mad</td>
<td align="center">平均绝对偏差</td>
</tr>
<tr>
<td align="center">median</td>
<td align="center">算数中位数</td>
</tr>
<tr>
<td align="center">min</td>
<td align="center">最小值</td>
</tr>
<tr>
<td align="center">max</td>
<td align="center">最大值</td>
</tr>
<tr>
<td align="center">mode</td>
<td align="center">众数</td>
</tr>
<tr>
<td align="center">abs</td>
<td align="center">绝对值</td>
</tr>
<tr>
<td align="center">prod</td>
<td align="center">乘积</td>
</tr>
<tr>
<td align="center">std</td>
<td align="center">贝塞尔校正的样本标准偏差</td>
</tr>
<tr>
<td align="center">var</td>
<td align="center">无偏方差</td>
</tr>
<tr>
<td align="center">sem</td>
<td align="center">平均值的标准误差</td>
</tr>
<tr>
<td align="center">skew</td>
<td align="center">样本偏度 (第三阶)</td>
</tr>
<tr>
<td align="center">kurt</td>
<td align="center">样本峰度 (第四阶)</td>
</tr>
<tr>
<td align="center">quantile</td>
<td align="center">样本分位数 (不同 % 的值)</td>
</tr>
<tr>
<td align="center">cumsum</td>
<td align="center">累加</td>
</tr>
<tr>
<td align="center">cumprod</td>
<td align="center">累乘</td>
</tr>
<tr>
<td align="center">cummax</td>
<td align="center">累积最大值</td>
</tr>
<tr>
<td align="center">cummin</td>
<td align="center">累积最小值</td>
</tr>
<tr>
<td align="center">describe</td>
<td align="center">数据总描述</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第1章 机器学习基础</title>
    <url>/2020/05/31/1.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h2 id="机器学习-概述"><a href="#机器学习-概述" class="headerlink" title="机器学习 概述"></a>机器学习 概述</h2><p><code>机器学习(Machine Learning,ML)</code> 是使用计算机来彰显数据背后的真实含义，它为了把无序的数据转换成有用的信息。是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。<br>它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。</p>
<a id="more"></a>

<ol>
<li>海量的数据</li>
<li>获取有用的信息</li>
</ol>
<h2 id="机器学习-研究意义"><a href="#机器学习-研究意义" class="headerlink" title="机器学习 研究意义"></a>机器学习 研究意义</h2><p>机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”。 “机器学习是对能通过经验自动改进的计算机算法的研究”。 “机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。” 一种经常引用的英文定义是：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
<p>机器学习已经有了十分广泛的应用，例如：数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用。</p>
<h2 id="机器学习-场景"><a href="#机器学习-场景" class="headerlink" title="机器学习 场景"></a>机器学习 场景</h2><ul>
<li><p>例如：识别动物猫</p>
<ul>
<li>模式识别（官方标准）：人们通过大量的经验，得到结论，从而判断它就是猫。</li>
<li>机器学习（数据学习）：人们通过阅读进行学习，观察它会叫、小眼睛、两只耳朵、四条腿、一条尾巴，得到结论，从而判断它就是猫。</li>
<li>深度学习（深入数据）：人们通过深入了解它，发现它会’喵喵’的叫、与同类的猫科动物很类似，得到结论，从而判断它就是猫。（深度学习常用领域：语音识别、图像识别）</li>
</ul>
</li>
<li><p>模式识别（pattern recognition）: 模式识别是最古老的（作为一个术语而言，可以说是很过时的）。</p>
<ul>
<li>我们把环境与客体统称为“模式”，识别是对模式的一种认知，是如何让一个计算机程序去做一些看起来很“智能”的事情。</li>
<li>通过融于智慧和直觉后，通过构建程序，识别一些事物，而不是人，例如: 识别数字。</li>
</ul>
</li>
<li><p>机器学习（machine learning）: 机器学习是最基础的（当下初创公司和研究实验室的热点领域之一）。</p>
<ul>
<li>在90年代初，人们开始意识到一种可以更有效地构建模式识别算法的方法，那就是用数据（可以通过廉价劳动力采集获得）去替换专家（具有很多图像方面知识的人）。</li>
<li>“机器学习”强调的是，在给计算机程序（或者机器）输入一些数据后，它必须做一些事情，那就是学习这些数据，而这个学习的步骤是明确的。</li>
<li>机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身性能的学科。</li>
</ul>
</li>
<li><p>深度学习（deep learning）: 深度学习是非常崭新和有影响力的前沿领域，我们甚至不会去思考-后深度学习时代。</p>
<ul>
<li>深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。</li>
</ul>
</li>
<li><p>参考地址： </p>
<ul>
<li><a href="http://www.csdn.net/article/2015-03-24/2824301" target="_blank" rel="noopener">深度学习 vs 机器学习 vs 模式识别</a></li>
<li><a href="http://baike.baidu.com/link?url=76P-uA4EBrC3G-I__P1tqeO7eoDS709Kp4wYuHxc7GNkz_xn0NxuAtEohbpey7LUa2zUQLJxvIKUx4bnrEfOmsWLKbDmvG1PCoRkJisMTQka6-QReTrIxdYY3v93f55q" target="_blank" rel="noopener">深度学习 百科资料</a></li>
</ul>
</li>
</ul>
<blockquote>
<p>机器学习已应用于多个领域，远远超出大多数人的想象，横跨：计算机科学、工程技术和统计学等多个学科。</p>
</blockquote>
<ul>
<li>搜索引擎: 根据你的搜索点击，优化你下次的搜索结果,是机器学习来帮助搜索引擎判断哪个结果更适合你（也判断哪个广告更适合你）。</li>
<li>垃圾邮件: 会自动的过滤垃圾广告邮件到垃圾箱内。</li>
<li>超市优惠券: 你会发现，你在购买小孩子尿布的时候，售货员会赠送你一张优惠券可以兑换6罐啤酒。</li>
<li>邮局邮寄: 手写软件自动识别寄送贺卡的地址。</li>
<li>申请贷款: 通过你最近的金融活动信息进行综合评定，决定你是否合格。</li>
</ul>
<h2 id="机器学习-组成"><a href="#机器学习-组成" class="headerlink" title="机器学习 组成"></a>机器学习 组成</h2><h3 id="主要任务"><a href="#主要任务" class="headerlink" title="主要任务"></a>主要任务</h3><ul>
<li>分类（classification）：将实例数据划分到合适的类别中。<ul>
<li>应用实例：判断网站是否被黑客入侵（二分类 ），手写数字的自动识别（多分类）</li>
</ul>
</li>
<li>回归（regression）：主要用于预测数值型数据。<ul>
<li>应用实例：股票价格波动的预测，房屋价格的预测等。</li>
</ul>
</li>
</ul>
<h3 id="监督学习（supervised-learning）"><a href="#监督学习（supervised-learning）" class="headerlink" title="监督学习（supervised learning）"></a>监督学习（supervised learning）</h3><ul>
<li>必须确定目标变量的值，以便机器学习算法可以发现特征和目标变量之间的关系。在监督学习中，给定一组数据，我们知道正确的输出结果应该是什么样子，并且知道在输入和输出之间有着一个特定的关系。 (包括：分类和回归)</li>
<li>样本集：训练数据 + 测试数据<ul>
<li>训练样本 = 特征(feature) + 目标变量(label: 分类-离散值/回归-连续值)</li>
<li>特征通常是训练样本集的列，它们是独立测量得到的。</li>
<li>目标变量: 目标变量是机器学习预测算法的测试结果。<ul>
<li>在分类算法中目标变量的类型通常是标称型(如：真与假)，而在回归算法中通常是连续型(如：1~100)。</li>
</ul>
</li>
</ul>
</li>
<li>监督学习需要注意的问题：<ul>
<li>偏置方差权衡</li>
<li>功能的复杂性和数量的训练数据</li>
<li>输入空间的维数</li>
<li>噪声中的输出值</li>
</ul>
</li>
<li><code>知识表示</code>：<ul>
<li>可以采用规则集的形式【例如：数学成绩大于90分为优秀】</li>
<li>可以采用概率分布的形式【例如：通过统计分布发现，90%的同学数学成绩，在70分以下，那么大于70分定为优秀】</li>
<li>可以使用训练样本集中的一个实例【例如：通过样本集合，我们训练出一个模型实例，得出 年轻，数学成绩中高等，谈吐优雅，我们认为是优秀】</li>
</ul>
</li>
</ul>
<h3 id="非监督学习（unsupervised-learing）"><a href="#非监督学习（unsupervised-learing）" class="headerlink" title="非监督学习（unsupervised learing）"></a>非监督学习（unsupervised learing）</h3><ul>
<li>在机器学习，无监督学习的问题是，在未加标签的数据中，试图找到隐藏的结构。因为提供给学习者的实例是未标记的，因此没有错误或报酬信号来评估潜在的解决方案。</li>
<li>无监督学习是密切相关的统计数据密度估计的问题。然而无监督学习还包括寻求，总结和解释数据的主要特点等诸多技术。在无监督学习使用的许多方法是基于用于处理数据的数据挖掘方法。</li>
<li>数据没有类别信息，也不会给定目标值。</li>
<li>非监督学习包括的类型：<ul>
<li>聚类：在无监督学习中，将数据集分成由类似的对象组成多个类的过程称为聚类。</li>
<li>密度估计：通过样本分布的紧密程度，来估计与分组的相似性。</li>
<li>此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3>这个算法可以训练程序做出某一决定。程序在某一情况下尝试所有的可能行动，记录不同行动的结果并试着找出最好的一次尝试来做决定。 属于这一类算法的有马尔可夫决策过程。<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3></li>
</ul>
</li>
</ul>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/1.MLFoundation/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B.jpg" alt="机器学习训练过程图"></p>
<h3 id="算法汇总"><a href="#算法汇总" class="headerlink" title="算法汇总"></a>算法汇总</h3><p><img src="http://data.apachecn.org/img/AiLearning/ml/1.MLFoundation/ml_algorithm.jpg" alt="算法汇总"></p>
<h2 id="机器学习-使用"><a href="#机器学习-使用" class="headerlink" title="机器学习 使用"></a>机器学习 使用</h2><blockquote>
<p>选择算法需要考虑的两个问题</p>
</blockquote>
<ol>
<li>算法场景<ul>
<li>预测明天是否下雨，因为可以用历史的天气情况做预测，所以选择监督学习算法</li>
<li>给一群陌生的人进行分组，但是我们并没有这些人的类别信息，所以选择无监督学习算法、通过他们身高、体重等特征进行处理。</li>
</ul>
</li>
<li>需要收集或分析的数据是什么</li>
</ol>
<blockquote>
<p>举例</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/1.MLFoundation/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95.jpg" alt="选择算法图"></p>
<blockquote>
<p>机器学习 开发流程</p>
</blockquote>
<ol>
<li>收集数据: 收集样本数据</li>
<li>准备数据: 注意数据的格式</li>
<li>分析数据: 为了确保数据集中没有垃圾数据；<ul>
<li>如果是算法可以处理的数据格式或可信任的数据源，则可以跳过该步骤；</li>
<li>另外该步骤需要人工干预，会降低自动化系统的价值。</li>
</ul>
</li>
<li>训练算法: [机器学习算法核心]如果使用无监督学习算法，由于不存在目标变量值，则可以跳过该步骤</li>
<li>测试算法: [机器学习算法核心]评估算法效果</li>
<li>使用算法: 将机器学习算法转为应用程序</li>
</ol>
<h2 id="机器学习-数学基础"><a href="#机器学习-数学基础" class="headerlink" title="机器学习 数学基础"></a>机器学习 数学基础</h2><ul>
<li>微积分</li>
<li>统计学/概率论</li>
<li>线性代数<h2 id="机器学习-工具"><a href="#机器学习-工具" class="headerlink" title="机器学习 工具"></a>机器学习 工具</h2></li>
</ul>
<h3 id="Python语言"><a href="#Python语言" class="headerlink" title="Python语言"></a>Python语言</h3><ol>
<li>可执行伪代码</li>
<li>Python比较流行：使用广泛、代码范例多、丰富模块库，开发周期短</li>
<li>Python语言的特色：清晰简练、易于理解</li>
<li>Python语言的缺点：唯一不足的是性能问题</li>
<li>Python相关的库<ul>
<li>科学函数库：<code>SciPy</code>、<code>NumPy</code>(底层语言：C和Fortran)</li>
<li>绘图工具库：<code>Matplotlib</code></li>
<li>数据分析库 <code>Pandas</code><h3 id="数学工具"><a href="#数学工具" class="headerlink" title="数学工具"></a>数学工具</h3></li>
</ul>
</li>
</ol>
<ul>
<li>Matlab<h2 id="附：机器学习专业术语"><a href="#附：机器学习专业术语" class="headerlink" title="附：机器学习专业术语"></a>附：机器学习专业术语</h2></li>
<li>模型（model）：计算机层面的认知</li>
<li>学习算法（learning algorithm），从数据中产生模型的方法</li>
<li>数据集（data set）：一组记录的合集</li>
<li>示例（instance）：对于某个对象的描述</li>
<li>样本（sample）：也叫示例</li>
<li>属性（attribute）：对象的某方面表现或特征</li>
<li>特征（feature）：同属性</li>
<li>属性值（attribute value）：属性上的取值</li>
<li>属性空间（attribute space）：属性张成的空间</li>
<li>样本空间/输入空间（samplespace）：同属性空间</li>
<li>特征向量（feature vector）：在属性空间里每个点对应一个坐标向量，把一个示例称作特征向量</li>
<li>维数（dimensionality）：描述样本参数的个数（也就是空间是几维的）</li>
<li>学习（learning）/训练（training）：从数据中学得模型</li>
<li>训练数据（training data）：训练过程中用到的数据</li>
<li>训练样本（training sample）:训练用到的每个样本</li>
<li>训练集（training set）：训练样本组成的集合</li>
<li>假设（hypothesis）：学习模型对应了关于数据的某种潜在规则</li>
<li>真相（ground-truth）:真正存在的潜在规律</li>
<li>学习器（learner）：模型的另一种叫法，把学习算法在给定数据和参数空间的实例化</li>
<li>预测（prediction）：判断一个东西的属性</li>
<li>标记（label）：关于示例的结果信息，比如我是一个“好人”。</li>
<li>样例（example）：拥有标记的示例</li>
<li>标记空间/输出空间（label space）：所有标记的集合</li>
<li>分类（classification）：预测是离散值，比如把人分为好人和坏人之类的学习任务</li>
<li>回归（regression）：预测值是连续值，比如你的好人程度达到了0.9，0.6之类的</li>
<li>二分类（binary classification）：只涉及两个类别的分类任务</li>
<li>正类（positive class）：二分类里的一个</li>
<li>反类（negative class）：二分类里的另外一个</li>
<li>多分类（multi-class classification）：涉及多个类别的分类</li>
<li>测试（testing）：学习到模型之后对样本进行预测的过程</li>
<li>测试样本（testing sample）：被预测的样本</li>
<li>聚类（clustering）：把训练集中的对象分为若干组</li>
<li>簇（cluster）：每一个组叫簇</li>
<li>监督学习（supervised learning）：典范–分类和回归</li>
<li>无监督学习（unsupervised learning）：典范–聚类</li>
<li>未见示例（unseen instance）：“新样本“，没训练过的样本</li>
<li>泛化（generalization）能力：学得的模型适用于新样本的能力</li>
<li>分布（distribution）：样本空间的全体样本服从的一种规律</li>
<li>独立同分布（independent and identically distributed，简称i,i,d.）:获得的每个样本都是独立地从这个分布上采样获得的。</li>
</ul>
<h2 id="机器学习基础补充"><a href="#机器学习基础补充" class="headerlink" title="机器学习基础补充"></a>机器学习基础补充</h2><h3 id="数据集的划分"><a href="#数据集的划分" class="headerlink" title="数据集的划分"></a>数据集的划分</h3><ul>
<li>训练集（Training set） —— 学习样本数据集，通过匹配一些参数来建立一个模型，主要用来训练模型。类比考研前做的解题大全。</li>
<li>验证集（validation set） —— 对学习出来的模型，调整模型的参数，如在神经网络中选择隐藏单元数。验证集还用来确定网络结构或者控制模型复杂程度的参数。类比 考研之前做的模拟考试。</li>
<li>测试集（Test set） —— 测试训练好的模型的分辨能力。类比 考研。这次真的是一考定终身。</li>
</ul>
<h3 id="模型拟合程度"><a href="#模型拟合程度" class="headerlink" title="模型拟合程度"></a>模型拟合程度</h3><ul>
<li>欠拟合（Underfitting）：模型没有很好地捕捉到数据特征，不能够很好地拟合数据，对训练样本的一般性质尚未学好。类比，光看书不做题觉得自己什么都会了，上了考场才知道自己啥都不会。</li>
<li>过拟合（Overfitting）：模型把训练样本学习“太好了”，可能把一些训练样本自身的特性当做了所有潜在样本都有的一般性质，导致泛化能力下降。类比，做课后题全都做对了，超纲题也都认为是考试必考题目，上了考场还是啥都不会。 </li>
</ul>
<p>通俗来说，欠拟合和过拟合都可以用一句话来说，欠拟合就是：“你太天真了！”，过拟合就是：“你想太多了！”。</p>
<h3 id="常见的模型指标"><a href="#常见的模型指标" class="headerlink" title="常见的模型指标"></a>常见的模型指标</h3><ul>
<li>正确率 —— 提取出的正确信息条数 / 提取出的信息条数</li>
<li>召回率 —— 提取出的正确信息条数 / 样本中的信息条数</li>
<li>F 值 —— 正确率 * 召回率 * 2 / （正确率 + 召回率）（F值即为正确率和召回率的调和平均值）</li>
</ul>
<p>举个例子如下：</p>
<p>举个例子如下：<br>某池塘有 1400 条鲤鱼，300 只虾，300 只乌龟。现在以捕鲤鱼为目的。撒了一张网，逮住了 700 条鲤鱼，200 只<br>虾， 100 只乌龟。那么这些指标分别如下：<br>正确率 = 700 / (700 + 200 + 100) = 70%<br>召回率 = 700 / 1400 = 50%<br>F 值 = 70% * 50% * 2 / (70% + 50%) = 58.3%</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><ul>
<li>分类问题 —— 说白了就是将一些未知类别的数据分到现在已知的类别中去。比如，根据你的一些信息，判断你是高富帅，还是穷屌丝。评判分类效果好坏的三个指标就是上面介绍的三个指标：正确率，召回率，F值。</li>
<li>回归问题 —— 对数值型连续随机变量进行预测和建模的监督学习算法。回归往往会通过计算 误差（Error）来确定模型的精确性。</li>
<li>聚类问题 —— 聚类是一种无监督学习任务，该算法基于数据的内部结构寻找观察样本的自然族群（即集群）。聚类问题的标准一般基于距离：簇内距离（Intra-cluster Distance） 和 簇间距离（Inter-cluster Distance） 。簇内距离是越小越好，也就是簇内的元素越相似越好；而簇间距离越大越好，也就是说簇间（不同簇）元素越不相同越好。一般的，衡量聚类问题会给出一个结合簇内距离和簇间距离的公式。</li>
</ul>
<p>下面这个图可以比较直观地展示出来：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/1.MLFoundation/ml_add_1.jpg" alt=""></p>
<h3 id="特征工程的一些小东西"><a href="#特征工程的一些小东西" class="headerlink" title="特征工程的一些小东西"></a>特征工程的一些小东西</h3><ul>
<li><p>特征选择 —— 也叫特征子集选择（FSS，Feature Subset Selection）。是指从已有的 M 个特征（Feature）中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。</p>
</li>
<li><p>特征提取 —— 特征提取是计算机视觉和图像处理中的一个概念。它指的是使用计算机提取图像信息，决定每个图像的点是否属于一个图像特征。特征提取的结果是把图像上的点分为不同的子集，这些子集往往属于孤立的点，连续的曲线或者连续的区域。</p>
</li>
</ul>
<p>下面给出一个特征工程的图：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/1.MLFoundation/ml_add_2.jpg" alt=""></p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul>
<li>Learning rate —— 学习率，通俗地理解，可以理解为步长，步子大了，很容易错过最佳结果。就是本来目标尽在咫尺，可是因为我迈的步子很大，却一下子走过了。步子小了呢，就是同样的距离，我却要走很多很多步，这样导致训练的耗时费力还不讨好。</li>
<li>一个总结的知识点很棒的链接 ：<a href="https://zhuanlan.zhihu.com/p/25197792" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25197792</a></li>
</ul>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第2章 k-近邻算法</title>
    <url>/2020/05/31/2.k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="KNN-概述"><a href="#KNN-概述" class="headerlink" title="KNN 概述"></a>KNN 概述</h2><p><code>k-近邻（kNN, k-NearestNeighbor）算法是一种基本分类与回归方法，我们这里只讨论分类问题中的 k-近邻算法。</code></p>
<p><strong>一句话总结：近朱者赤近墨者黑！</strong> </p>
<p><code>k 近邻算法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。k 近邻算法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其 k 个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k近邻算法不具有显式的学习过程。</code></p>
<a id="more"></a>

<p><code>k 近邻算法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。 k值的选择、距离度量以及分类决策规则是k近邻算法的三个基本要素。</code></p>
<h2 id="KNN-场景"><a href="#KNN-场景" class="headerlink" title="KNN 场景"></a>KNN 场景</h2><p>电影可以按照题材分类，那么如何区分 <code>动作片</code> 和 <code>爱情片</code> 呢？<br/></p>
<ol>
<li>动作片：打斗次数更多</li>
<li>爱情片：亲吻次数更多</li>
</ol>
<p>基于电影中的亲吻、打斗出现的次数，使用 k-近邻算法构造程序，就可以自动划分电影的题材类型。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/2.KNN/knn-1-movie.png" alt="电影视频案例" title="电影视频案例"></p>
<figure class="highlight inform7"><table><tr><td class="code"><pre><span class="line">现在根据上面我们得到的样本集中所有电影与未知电影的距离，按照距离递增排序，可以找到 k 个距离最近的电影。</span><br><span class="line">假定 k=3，则三个最靠近的电影依次是， He's Not Really into Dudes 、 Beautiful <span class="keyword">Woman</span> 和 California <span class="keyword">Man</span>。</span><br><span class="line">knn 算法按照距离最近的三部电影的类型，决定未知电影的类型，而这三部电影全是爱情片，因此我们判定未知电影是爱情片。</span><br></pre></td></tr></table></figure>

<h2 id="KNN-原理"><a href="#KNN-原理" class="headerlink" title="KNN 原理"></a>KNN 原理</h2><blockquote>
<p>KNN 工作原理</p>
</blockquote>
<ol>
<li>假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系。</li>
<li>输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。<ol>
<li>计算新数据与样本数据集中每条数据的距离。</li>
<li>对求得的所有距离进行排序（从小到大，越小表示越相似）。</li>
<li>取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。</li>
</ol>
</li>
<li>求 k 个数据中出现次数最多的分类标签作为新数据的分类。</li>
</ol>
<blockquote>
<p>KNN 通俗理解</p>
</blockquote>
<p>给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类。</p>
<blockquote>
<p>KNN 开发流程</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">收集数据：任何方法</span><br><span class="line">准备数据：距离计算所需要的数值，最好是结构化的数据格式</span><br><span class="line">分析数据：任何方法</span><br><span class="line">训练算法：此步骤不适用于 k-近邻算法</span><br><span class="line">测试算法：计算错误率</span><br><span class="line">使用算法：输入样本数据和结构化的输出结果，然后运行 k-近邻算法判断输入数据分类属于哪个分类，最后对计算出的分类执行后续处理</span><br></pre></td></tr></table></figure>

<blockquote>
<p>KNN 算法特点</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">优点：精度高、对异常值不敏感、无数据输入假定</span><br><span class="line">缺点：计算复杂度高、空间复杂度高</span><br><span class="line">适用数据范围：数值型和标称型</span><br></pre></td></tr></table></figure>

<h2 id="KNN-项目案例"><a href="#KNN-项目案例" class="headerlink" title="KNN 项目案例"></a>KNN 项目案例</h2><h3 id="项目案例1-优化约会网站的配对效果"><a href="#项目案例1-优化约会网站的配对效果" class="headerlink" title="项目案例1: 优化约会网站的配对效果"></a>项目案例1: 优化约会网站的配对效果</h3><p><a href="/src/py2.x/ml/2.KNN/kNN.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/2.KNN/kNN.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/2.KNN/kNN.py</a></p>
<h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p>海伦使用约会网站寻找约会对象。经过一段时间之后，她发现曾交往过三种类型的人:</p>
<ul>
<li>不喜欢的人</li>
<li>魅力一般的人</li>
<li>极具魅力的人</li>
</ul>
<p>她希望：</p>
<ol>
<li>工作日与魅力一般的人约会</li>
<li>周末与极具魅力的人约会</li>
<li>不喜欢的人则直接排除掉</li>
</ol>
<p>现在她收集到了一些约会网站未曾记录的数据信息，这更有助于匹配对象的归类。</p>
<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">收集数据：提供文本文件</span><br><span class="line">准备数据：使用 Python 解析文本文件</span><br><span class="line">分析数据：使用 Matplotlib 画二维散点图</span><br><span class="line">训练算法：此步骤不适用于 k-近邻算法</span><br><span class="line">测试算法：使用海伦提供的部分数据作为测试样本。</span><br><span class="line">        测试样本和非测试样本的区别在于：</span><br><span class="line">            测试样本是已经完成分类的数据，如果预测分类与实际类别不同，则标记为一个错误。</span><br><span class="line">使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型。</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据：提供文本文件</p>
</blockquote>
<p>海伦把这些约会对象的数据存放在文本文件 <a href="/data/2.KNN/datingTestSet2.txt">datingTestSet2.txt</a> 中，总共有 1000 行。海伦约会的对象主要包含以下 3 种特征：</p>
<ul>
<li>每年获得的飞行常客里程数</li>
<li>玩视频游戏所耗时间百分比</li>
<li>每周消费的冰淇淋公升数</li>
</ul>
<p>文本文件数据格式如下：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">40920</span>	<span class="number">8.326976</span>	<span class="number">0.953952</span>	<span class="number">3</span></span><br><span class="line"><span class="number">14488</span>	<span class="number">7.153469</span>	<span class="number">1.673904</span>	<span class="number">2</span></span><br><span class="line"><span class="number">26052</span>	<span class="number">1.441871</span>	<span class="number">0.805124</span>	<span class="number">1</span></span><br><span class="line"><span class="number">75136</span>	<span class="number">13.147394</span>	<span class="number">0.428964</span>	<span class="number">1</span></span><br><span class="line"><span class="number">38344</span>	<span class="number">1.669788</span>	<span class="number">0.134296</span>	<span class="number">1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>准备数据：使用 Python 解析文本文件</p>
</blockquote>
<p>将文本记录转换为 NumPy 的解析程序</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file2matrix</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        导入训练数据</span></span><br><span class="line"><span class="string">    parameters:</span></span><br><span class="line"><span class="string">        filename: 数据文件路径</span></span><br><span class="line"><span class="string">    return: </span></span><br><span class="line"><span class="string">        数据矩阵 returnMat 和对应的类别 classLabelVector</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="comment"># 获得文件中的数据行的行数</span></span><br><span class="line">    numberOfLines = len(fr.readlines())</span><br><span class="line">    <span class="comment"># 生成对应的空矩阵</span></span><br><span class="line">    <span class="comment"># 例如：zeros(2，3)就是生成一个 2*3的矩阵，各个位置上全是 0 </span></span><br><span class="line">    returnMat = zeros((numberOfLines, <span class="number">3</span>))  <span class="comment"># prepare matrix to return</span></span><br><span class="line">    classLabelVector = []  <span class="comment"># prepare labels return</span></span><br><span class="line">    fr = open(filename)</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment"># str.strip([chars]) --返回已移除字符串头尾指定字符所生成的新字符串</span></span><br><span class="line">        line = line.strip()</span><br><span class="line">        <span class="comment"># 以 '\t' 切割字符串</span></span><br><span class="line">        listFromLine = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="comment"># 每列的属性数据</span></span><br><span class="line">        returnMat[index, :] = listFromLine[<span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">        <span class="comment"># 每列的类别数据，就是 label 标签数据</span></span><br><span class="line">        classLabelVector.append(int(listFromLine[<span class="number">-1</span>]))</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 返回数据矩阵returnMat和对应的类别classLabelVector</span></span><br><span class="line">    <span class="keyword">return</span> returnMat, classLabelVector</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据：使用 Matplotlib 画二维散点图</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.scatter(datingDataMat[:, <span class="number">0</span>], datingDataMat[:, <span class="number">1</span>], <span class="number">15.0</span>*array(datingLabels), <span class="number">15.0</span>*array(datingLabels))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>下图中采用矩阵的第一和第二列属性得到很好的展示效果，清晰地标识了三个不同的样本分类区域，具有不同爱好的人其类别区域也不同。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/2.KNN/knn_matplotlib_2.png" alt="Matplotlib 散点图"></p>
<ul>
<li>归一化数据 （归一化是一个让权重变为统一的过程，更多细节请参考： <a href="https://www.zhihu.com/question/19951858" target="_blank" rel="noopener">https://www.zhihu.com/question/19951858</a> ）</li>
</ul>
<table>
<thead>
<tr>
<th>序号</th>
<th align="center">玩视频游戏所耗时间百分比</th>
<th align="right">每年获得的飞行常客里程数</th>
<th align="right">每周消费的冰淇淋公升数</th>
<th align="right">样本分类</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td align="center">0.8</td>
<td align="right">400</td>
<td align="right">0.5</td>
<td align="right">1</td>
</tr>
<tr>
<td>2</td>
<td align="center">12</td>
<td align="right">134 000</td>
<td align="right">0.9</td>
<td align="right">3</td>
</tr>
<tr>
<td>3</td>
<td align="center">0</td>
<td align="right">20 000</td>
<td align="right">1.1</td>
<td align="right">2</td>
</tr>
<tr>
<td>4</td>
<td align="center">67</td>
<td align="right">32 000</td>
<td align="right">0.1</td>
<td align="right">2</td>
</tr>
</tbody></table>
<p>样本3和样本4的距离：<br>$$\sqrt{(0-67)^2 + (20000-32000)^2 + (1.1-0.1)^2 }$$</p>
<p>归一化特征值，消除特征之间量级不同导致的影响</p>
<p><strong>归一化定义：</strong> 我是这样认为的，归一化就是要把你需要处理的数据经过处理后（通过某种算法）限制在你需要的一定范围内。首先归一化是为了后面数据处理的方便，其次是保正程序运行时收敛加快。 方法有如下：</p>
<p>1) 线性函数转换，表达式如下：　　</p>
<pre><code>y=(x-MinValue)/(MaxValue-MinValue)　　

说明：x、y分别为转换前、后的值，MaxValue、MinValue分别为样本的最大值和最小值。　　</code></pre><p>2) 对数函数转换，表达式如下：　　</p>
<pre><code>y=log10(x)　　

说明：以10为底的对数函数转换。

如图：

![对数函数图像](http://data.apachecn.org/img/AiLearning/ml/2.KNN/knn_1.png)</code></pre><p>3) 反余切函数转换，表达式如下：</p>
<pre><code>y=arctan(x)*2/PI　

如图：

![反余切函数图像](http://data.apachecn.org/img/AiLearning/ml/2.KNN/arctan_arccot.gif)</code></pre><p>4) 式(1)将输入值换算为[-1,1]区间的值，在输出层用式(2)换算回初始值，其中和分别表示训练样本集中负荷的最大值和最小值。　</p>
<p>在统计学中，归一化的具体作用是归纳统一样本的统计分布性。归一化在0-1之间是统计的概率分布，归一化在-1–+1之间是统计的坐标分布。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoNorm</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        归一化特征值，消除特征之间量级不同导致的影响</span></span><br><span class="line"><span class="string">    parameter:</span></span><br><span class="line"><span class="string">        dataSet: 数据集</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        归一化后的数据集 normDataSet. ranges和minVals即最小值与范围，并没有用到</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    归一化公式：</span></span><br><span class="line"><span class="string">        Y = (X-Xmin)/(Xmax-Xmin)</span></span><br><span class="line"><span class="string">        其中的 min 和 max 分别是数据集中的最小特征值和最大特征值。该函数可以自动将数字特征值转化为0到1的区间。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 计算每种属性的最大值、最小值、范围</span></span><br><span class="line">    minVals = dataSet.min(<span class="number">0</span>)</span><br><span class="line">    maxVals = dataSet.max(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 极差</span></span><br><span class="line">    ranges = maxVals - minVals</span><br><span class="line">    normDataSet = zeros(shape(dataSet))</span><br><span class="line">    m = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 生成与最小值之差组成的矩阵</span></span><br><span class="line">    normDataSet = dataSet - tile(minVals, (m, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 将最小值之差除以范围组成矩阵</span></span><br><span class="line">    normDataSet = normDataSet / tile(ranges, (m, <span class="number">1</span>))  <span class="comment"># element wise divide</span></span><br><span class="line">    <span class="keyword">return</span> normDataSet, ranges, minVals</span><br></pre></td></tr></table></figure>

<blockquote>
<p>训练算法：此步骤不适用于 k-近邻算法</p>
</blockquote>
<p>因为测试数据每一次都要与全量的训练数据进行比较，所以这个过程是没有必要的。</p>
<p>kNN 算法伪代码：</p>
<pre><code>对于每一个在数据集中的数据点：
    计算目标的数据点（需要分类的数据点）与该数据点的距离
    将距离排序：从小到大
    选取前K个最短距离
    选取这K个中最多的分类类别
    返回该类别来作为目标数据点的预测值</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataSet, labels, k)</span>:</span></span><br><span class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">#距离度量 度量公式为欧氏距离</span></span><br><span class="line">    diffMat = tile(inX, (dataSetSize,<span class="number">1</span>)) – dataSet</span><br><span class="line">    sqDiffMat = diffMat**<span class="number">2</span></span><br><span class="line">    sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">    distances = sqDistances**<span class="number">0.5</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#将距离排序：从小到大</span></span><br><span class="line">    sortedDistIndicies = distances.argsort()</span><br><span class="line">    <span class="comment">#选取前K个最短距离， 选取这K个中最多的分类类别</span></span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k)：</span><br><span class="line">        voteIlabel = labels[sortedDistIndicies[i]]</span><br><span class="line">        classCount[voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>) + <span class="number">1</span> </span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>


<blockquote>
<p>测试算法：使用海伦提供的部分数据作为测试样本。如果预测分类与实际类别不同，则标记为一个错误。</p>
</blockquote>
<p>kNN 分类器针对约会网站的测试代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">datingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对约会网站的测试方法</span></span><br><span class="line"><span class="string">    parameters:</span></span><br><span class="line"><span class="string">        none</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        错误数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 设置测试数据的的一个比例（训练数据集比例=1-hoRatio）</span></span><br><span class="line">    hoRatio = <span class="number">0.1</span>  <span class="comment"># 测试范围,一部分测试一部分作为样本</span></span><br><span class="line">    <span class="comment"># 从文件中加载数据</span></span><br><span class="line">    datingDataMat, datingLabels = file2matrix(<span class="string">'data/2.KNN/datingTestSet2.txt'</span>)  <span class="comment"># load data setfrom file</span></span><br><span class="line">    <span class="comment"># 归一化数据</span></span><br><span class="line">    normMat, ranges, minVals = autoNorm(datingDataMat)</span><br><span class="line">    <span class="comment"># m 表示数据的行数，即矩阵的第一维</span></span><br><span class="line">    m = normMat.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 设置测试的样本数量， numTestVecs:m表示训练样本的数量</span></span><br><span class="line">    numTestVecs = int(m * hoRatio)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'numTestVecs='</span>, numTestVecs</span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTestVecs):</span><br><span class="line">        <span class="comment"># 对数据测试</span></span><br><span class="line">        classifierResult = classify0(normMat[i, :], normMat[numTestVecs:m, :], datingLabels[numTestVecs:m], <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"the classifier came back with: %d, the real answer is: %d"</span> % (classifierResult, datingLabels[i])</span><br><span class="line">        <span class="keyword">if</span> (classifierResult != datingLabels[i]): errorCount += <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"the total error rate is: %f"</span> % (errorCount / float(numTestVecs))</span><br><span class="line">    <span class="keyword">print</span> errorCount</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型。</p>
</blockquote>
<p>约会网站预测函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyPerson</span><span class="params">()</span>:</span></span><br><span class="line">    resultList = [<span class="string">'not at all'</span>, <span class="string">'in small doses'</span>, <span class="string">'in large doses'</span>]</span><br><span class="line">    percentTats = float(raw_input(<span class="string">"percentage of time spent playing video games ?"</span>))</span><br><span class="line">    ffMiles = float(raw_input(<span class="string">"frequent filer miles earned per year?"</span>))</span><br><span class="line">    iceCream = float(raw_input(<span class="string">"liters of ice cream consumed per year?"</span>))</span><br><span class="line">    datingDataMat, datingLabels = file2matrix(<span class="string">'datingTestSet2.txt'</span>)</span><br><span class="line">    normMat, ranges, minVals = autoNorm(datingDataMat)</span><br><span class="line">    inArr = array([ffMiles, percentTats, iceCream])</span><br><span class="line">    classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"You will probably like this person: "</span>, resultList[classifierResult - <span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>实际运行效果如下: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classifyPerson()</span><br><span class="line">percentage of time spent playing video games?10</span><br><span class="line">frequent flier miles earned per year?10000</span><br><span class="line">liters of ice cream consumed per year?0.5</span><br><span class="line">You will probably like this person: <span class="keyword">in</span> small doses</span><br></pre></td></tr></table></figure>



<h3 id="项目案例2-手写数字识别系统"><a href="#项目案例2-手写数字识别系统" class="headerlink" title="项目案例2: 手写数字识别系统"></a>项目案例2: 手写数字识别系统</h3><p><a href="/src/py2.x/ml/2.KNN/kNN.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/2.KNN/kNN.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/2.KNN/kNN.py</a></p>
<h4 id="项目概述-1"><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4><p>构造一个能识别数字 0 到 9 的基于 KNN 分类器的手写数字识别系统。</p>
<p>需要识别的数字是存储在文本文件中的具有相同的色彩和大小：宽高是 32 像素 * 32 像素的黑白图像。</p>
<h4 id="开发流程-1"><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight less"><table><tr><td class="code"><pre><span class="line">收集数据：提供文本文件。</span><br><span class="line">准备数据：编写函数 <span class="selector-tag">img2vector</span>(), 将图像格式转换为分类器使用的向量格式</span><br><span class="line">分析数据：在 <span class="selector-tag">Python</span> 命令提示符中检查数据，确保它符合要求</span><br><span class="line">训练算法：此步骤不适用于 <span class="selector-tag">KNN</span></span><br><span class="line">测试算法：编写函数使用提供的部分数据集作为测试样本，测试样本与非测试样本的</span><br><span class="line">         区别在于测试样本是已经完成分类的数据，如果预测分类与实际类别不同，</span><br><span class="line">         则标记为一个错误</span><br><span class="line">使用算法：本例没有完成此步骤，若你感兴趣可以构建完整的应用程序，从图像中提取</span><br><span class="line">         数字，并完成数字识别，美国的邮件分拣系统就是一个实际运行的类似系统</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 提供文本文件</p>
</blockquote>
<p>目录 <a href="/data/2.KNN/trainingDigits">trainingDigits</a> 中包含了大约 2000 个例子，每个例子内容如下图所示，每个数字大约有 200 个样本；目录 <a href="/data/2.KNN/testDigits">testDigits</a> 中包含了大约 900 个测试数据。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/2.KNN/knn_2_handWriting.png" alt="手写数字数据集的例子"></p>
<blockquote>
<p>准备数据: 编写函数 img2vector(), 将图像文本数据转换为分类器使用的向量</p>
</blockquote>
<p>将图像文本数据转换为向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span></span><br><span class="line">    returnVect = zeros((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">        lineStr = fr.readline()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">            returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j] = int(lineStr[j])</span><br><span class="line">    <span class="keyword">return</span> returnVect</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据：在 Python 命令提示符中检查数据，确保它符合要求</p>
</blockquote>
<p>在 Python 命令行中输入下列命令测试 img2vector 函数，然后与文本编辑器打开的文件进行比较: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>testVector = kNN.img2vector(<span class="string">'testDigits/0_13.txt'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>testVector[<span class="number">0</span>,<span class="number">0</span>:<span class="number">32</span>]</span><br><span class="line">array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>testVector[<span class="number">0</span>,<span class="number">32</span>:<span class="number">64</span>]</span><br><span class="line">array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>训练算法：此步骤不适用于 KNN</p>
</blockquote>
<p>因为测试数据每一次都要与全量的训练数据进行比较，所以这个过程是没有必要的。</p>
<blockquote>
<p>测试算法：编写函数使用提供的部分数据集作为测试样本，如果预测分类与实际类别不同，则标记为一个错误</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 1. 导入训练数据</span></span><br><span class="line">    hwLabels = []</span><br><span class="line">    trainingFileList = listdir(<span class="string">'data/2.KNN/trainingDigits'</span>)  <span class="comment"># load the training set</span></span><br><span class="line">    m = len(trainingFileList)</span><br><span class="line">    trainingMat = zeros((m, <span class="number">1024</span>))</span><br><span class="line">    <span class="comment"># hwLabels存储0～9对应的index位置， trainingMat存放的每个位置对应的图片向量</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        fileNameStr = trainingFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]  <span class="comment"># take off .txt</span></span><br><span class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        hwLabels.append(classNumStr)</span><br><span class="line">        <span class="comment"># 将 32*32的矩阵-&gt;1*1024的矩阵</span></span><br><span class="line">        trainingMat[i, :] = img2vector(<span class="string">'data/2.KNN/trainingDigits/%s'</span> % fileNameStr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 导入测试数据</span></span><br><span class="line">    testFileList = listdir(<span class="string">'data/2.KNN/testDigits'</span>)  <span class="comment"># iterate through the test set</span></span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    mTest = len(testFileList)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(mTest):</span><br><span class="line">        fileNameStr = testFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]  <span class="comment"># take off .txt</span></span><br><span class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        vectorUnderTest = img2vector(<span class="string">'data/2.KNN/testDigits/%s'</span> % fileNameStr)</span><br><span class="line">        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"the classifier came back with: %d, the real answer is: %d"</span> % (classifierResult, classNumStr)</span><br><span class="line">        <span class="keyword">if</span> (classifierResult != classNumStr): errorCount += <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"\nthe total number of errors is: %d"</span> % errorCount</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"\nthe total error rate is: %f"</span> % (errorCount / float(mTest))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法：本例没有完成此步骤，若你感兴趣可以构建完整的应用程序，从图像中提取数字，并完成数字识别，美国的邮件分拣系统就是一个实际运行的类似系统。</p>
</blockquote>
<h2 id="KNN-小结"><a href="#KNN-小结" class="headerlink" title="KNN 小结"></a>KNN 小结</h2><p>KNN 是什么？定义： 监督学习？ 非监督学习？</p>
<p>KNN 是一个简单的无显示学习过程，非泛化学习的监督学习模型。在分类和回归中均有应用。</p>
<h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>简单来说： 通过距离度量来计算查询点（query point）与每个训练数据点的距离，然后选出与查询点（query point）相近的K个最邻点（K nearest neighbors），使用分类决策来选出对应的标签来作为该查询点的标签。</p>
<h3 id="KNN-三要素"><a href="#KNN-三要素" class="headerlink" title="KNN 三要素"></a>KNN 三要素</h3><blockquote>
<p>K, K的取值</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>对查询点标签影响显著（效果拔群）。k值小的时候 近似误差小，估计误差大。 k值大 近似误差大，估计误差小。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>如果选择较小的 k 值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差（approximation error）会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是“学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k 值的减小就意味着整体模型变得复杂，容易发生过拟合。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>如果选择较大的 k 值，就相当于用较大的邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。 k 值的增大就意味着整体的模型变得简单。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>太大太小都不太好，可以用交叉验证（cross validation）来选取适合的k值。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>近似误差和估计误差，请看这里：<a href="https://www.zhihu.com/question/60793482" target="_blank" rel="noopener">https://www.zhihu.com/question/60793482</a></p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>距离度量 Metric/Distance Measure </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>距离度量 通常为 欧式距离（Euclidean distance），还可以是 Minkowski 距离 或者 曼哈顿距离。也可以是 地理空间中的一些距离公式。（更多细节可以参看 sklearn 中 valid_metric 部分）</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>分类决策 （decision rule）</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>分类决策 在 分类问题中 通常为通过少数服从多数 来选取票数最多的标签，在回归问题中通常为 K个最邻点的标签的平均值。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<h3 id="算法：（sklearn-上有三种）"><a href="#算法：（sklearn-上有三种）" class="headerlink" title="算法：（sklearn 上有三种）"></a>算法：（sklearn 上有三种）</h3><blockquote>
<p>Brute Force 暴力计算/线性扫描 </p>
</blockquote>
<blockquote>
<p>KD Tree 使用二叉树根据数据维度来平分参数空间。</p>
</blockquote>
<blockquote>
<p>Ball Tree 使用一系列的超球体来平分训练数据集。</p>
</blockquote>
<blockquote>
<p>树结构的算法都有建树和查询两个过程。Brute Force 没有建树的过程。</p>
</blockquote>
<blockquote>
<p>算法特点：   </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>优点： High Accuracy， No Assumption on data， not sensitive to outliers</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>缺点：时间和空间复杂度 高</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>适用范围： continuous values and nominal values</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>相似同源产物： </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>radius neighbors 根据制定的半径来找寻邻点</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>影响算法因素：</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>N 数据集样本数量(number of samples)， D 数据维度 (number of features)</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>总消耗：</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Brute Force:  O[DN^2] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>此处考虑的是最蠢的方法：把所有训练的点之间的距离都算一遍。当然有更快的实现方式, 比如 O(ND + kN)  和  O(NDK) , 最快的是 O[DN] 。感兴趣的可以阅读这个链接： <a href="https://stats.stackexchange.com/questions/219655/k-nn-computational-complexity" target="_blank" rel="noopener">k-NN computational complexity</a></p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>KD Tree: O[DN log(N)] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Ball Tree: O[DN log(N)] 跟 KD Tree 处于相同的数量级，虽然建树时间会比 KD Tree 久一点，但是在高结构的数据，甚至是高纬度的数据中，查询速度有很大的提升。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>查询所需消耗:</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Brute Force:  O[DN] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>KD Tree: 当维度比较小的时候， 比如 D&lt;20,  O[Dlog(N)] 。相反，将会趋向于 O[DN] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Ball Tree: O[Dlog(N)] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>当数据集比较小的时候，比如 N&lt;30的时候，Brute Force 更有优势。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>Intrinsic Dimensionality(本征维数) 和 Sparsity（稀疏度）</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>数据的 intrinsic dimensionality 是指数据所在的流形的维数 d &lt; D , 在参数空间可以是线性或非线性的。稀疏度指的是数据填充参数空间的程度(这与“稀疏”矩阵中使用的概念不同, 数据矩阵可能没有零项, 但是从这个意义上来讲,它的结构 仍然是 “稀疏” 的)。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Brute Force 的查询时间不受影响。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>对于 KD Tree 和 Ball Tree的查询时间, 较小本征维数且更稀疏的数据集的查询时间更快。KD Tree 的改善由于通过坐标轴来平分参数空间的自身特性 没有Ball Tree 显著。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>k的取值 (k 个邻点)</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Brute Force 的查询时间基本不受影响。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>但是对于 KD Tree 和 Ball Tree , k越大，查询时间越慢。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>k 在N的占比较大的时候，使用 Brute Force 比较好。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>Number of Query Points （查询点数量， 即测试数据的数量）</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>查询点较少的时候用Brute Force。查询点较多的时候可以使用树结构算法。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>关于 sklearn 中模型的一些额外干货：</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>如果KD Tree，Ball Tree 和Brute Force 应用场景傻傻分不清楚，可以直接使用 含有algorithm=’auto’的模组。 algorithm=’auto’ 自动为您选择最优算法。<br>有 regressor 和 classifier 可以来选择。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>metric/distance measure 可以选择。 另外距离 可以通过weight 来加权。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>leaf size 对KD Tree 和 Ball Tree 的影响</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>建树时间：leaf size 比较大的时候，建树时间也就快点。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>查询时间： leaf size 太大太小都不太好。如果leaf size 趋向于 N（训练数据的样本数量），算法其实就是 brute force了。如果leaf size 太小了，趋向于1，那查询的时候 遍历树的时间就会大大增加。leaf size 建议的数值是 30，也就是默认值。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>内存： leaf size 变大，存树结构的内存变小。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>Nearest Centroid Classifier</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>分类决策是哪个标签的质心与测试点最近，就选哪个标签。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>该模型假设在所有维度中方差相同。 是一个很好的base line。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>进阶版： Nearest Shrunken Centroid </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>可以通过shrink_threshold来设置。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>作用： 可以移除某些影响分类的特征，例如移除噪音特征的影响</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第3章 决策树</title>
    <url>/2020/06/01/3.%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<h2 id="决策树-概述"><a href="#决策树-概述" class="headerlink" title="决策树 概述"></a>决策树 概述</h2><p><code>决策树（Decision Tree）算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树。</code></p>
<p><code>决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。</code></p>
<p><code>决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。</code></p>
<a id="more"></a>
<h2 id="决策树-场景"><a href="#决策树-场景" class="headerlink" title="决策树 场景"></a>决策树 场景</h2><p>一个叫做 “二十个问题” 的游戏，游戏的规则很简单：参与游戏的一方在脑海中想某个事物，其他参与者向他提问，只允许提 20 个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围，最后得到游戏的答案。</p>
<p>一个邮件分类系统，大致工作流程如下：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/%E5%86%B3%E7%AD%96%E6%A0%91-%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg" alt="决策树-流程图" title="决策树示例流程图"></p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">首先检测发送邮件域名地址。如果地址为 myEmployer<span class="selector-class">.com</span>, 则将其放在分类 <span class="string">"无聊时需要阅读的邮件"</span>中。</span><br><span class="line">如果邮件不是来自这个域名，则检测邮件内容里是否包含单词 <span class="string">"曲棍球"</span> , 如果包含则将邮件归类到 <span class="string">"需要及时处理的朋友邮件"</span>, </span><br><span class="line">如果不包含则将邮件归类到 <span class="string">"无需阅读的垃圾邮件"</span> 。</span><br></pre></td></tr></table></figure>

<p>决策树的定义：</p>
<p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。</p>
<p>用决策树对需要测试的实例进行分类：从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。</p>
<h2 id="决策树-原理"><a href="#决策树-原理" class="headerlink" title="决策树 原理"></a>决策树 原理</h2><h3 id="决策树-须知概念"><a href="#决策树-须知概念" class="headerlink" title="决策树 须知概念"></a>决策树 须知概念</h3><h4 id="信息熵-amp-信息增益"><a href="#信息熵-amp-信息增益" class="headerlink" title="信息熵 &amp; 信息增益"></a>信息熵 &amp; 信息增益</h4><p>熵（entropy）：<br>熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。</p>
<p>信息论（information theory）中的熵（香农熵）：<br>是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。</p>
<p>信息增益（information gain）：<br>在划分数据集前后信息发生的变化称为信息增益。</p>
<h3 id="决策树-工作原理"><a href="#决策树-工作原理" class="headerlink" title="决策树 工作原理"></a>决策树 工作原理</h3><p>如何构造一个决策树?<br/><br>我们使用 createBranch() 方法，如下所示：</p>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line">def createBranch():</span><br><span class="line">'''</span><br><span class="line">此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。</span><br><span class="line">'''</span><br><span class="line"><span class="code">    检测数据集中的所有数据的分类标签是否相同:</span></span><br><span class="line"><span class="code">        If so return 类标签</span></span><br><span class="line"><span class="code">        Else:</span></span><br><span class="line"><span class="code">            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）</span></span><br><span class="line"><span class="code">            划分数据集</span></span><br><span class="line"><span class="code">            创建分支节点</span></span><br><span class="line"><span class="code">                for 每个划分的子集</span></span><br><span class="line"><span class="code">                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中</span></span><br><span class="line"><span class="code">            return 分支节点</span></span><br></pre></td></tr></table></figure>

<h3 id="决策树-开发流程"><a href="#决策树-开发流程" class="headerlink" title="决策树 开发流程"></a>决策树 开发流程</h3><figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">收集数据：可以使用任何方法。</span><br><span class="line">准备数据：树构造算法 <span class="comment">(这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)</span></span><br><span class="line">分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。</span><br><span class="line">训练算法：构造树的数据结构。</span><br><span class="line">测试算法：使用训练好的树计算错误率。</span><br><span class="line">使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。</span><br></pre></td></tr></table></figure>

<h3 id="决策树-算法特点"><a href="#决策树-算法特点" class="headerlink" title="决策树 算法特点"></a>决策树 算法特点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">优点：计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。</span><br><span class="line">缺点：容易过拟合。</span><br><span class="line">适用数据类型：数值型和标称型。</span><br></pre></td></tr></table></figure>

<h2 id="决策树-项目案例"><a href="#决策树-项目案例" class="headerlink" title="决策树 项目案例"></a>决策树 项目案例</h2><h3 id="项目案例1-判定鱼类和非鱼类"><a href="#项目案例1-判定鱼类和非鱼类" class="headerlink" title="项目案例1: 判定鱼类和非鱼类"></a>项目案例1: 判定鱼类和非鱼类</h3><h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p>根据以下 2 个特征，将动物分成两类：鱼类和非鱼类。</p>
<p>特征：</p>
<ol>
<li>不浮出水面是否可以生存</li>
<li>是否有脚蹼</li>
</ol>
<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><p><a href="/src/py2.x/ml/3.DecisionTree/DecisionTree.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">收集数据：可以使用任何方法</span><br><span class="line">准备数据：树构造算法（这里使用的是ID3算法，因此数值型数据必须离散化。）</span><br><span class="line">分析数据：可以使用任何方法，构造树完成之后，我们可以将树画出来。</span><br><span class="line">训练算法：构造树结构</span><br><span class="line">测试算法：使用习得的决策树执行分类</span><br><span class="line">使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据：可以使用任何方法</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/DT_%E6%B5%B7%E6%B4%8B%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE.png" alt="海洋生物数据"></p>
<p>我们利用 createDataSet() 函数输入数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">            [<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">            [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'no'</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>, <span class="string">'flippers'</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br></pre></td></tr></table></figure>
<blockquote>
<p>准备数据：树构造算法</p>
</blockquote>
<p>此处，由于我们输入的数据本身就是离散化数据，所以这一步就省略了。</p>
<blockquote>
<p>分析数据：可以使用任何方法，构造树完成之后，我们可以将树画出来。</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/%E7%86%B5%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.jpg" alt="熵的计算公式"></p>
<p>计算给定数据集的香农熵的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="comment"># 求list的长度，表示计算参与训练的数据量</span></span><br><span class="line">    numEntries = len(dataSet)</span><br><span class="line">    <span class="comment"># 计算分类标签label出现的次数</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="comment"># the the number of unique elements and their occurrence</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 将当前实例的标签存储，即每一行数据的最后一个数据代表的是标签</span></span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># 为所有可能的分类创建字典，如果当前的键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对于 label 标签的占比，求出 label 标签的香农熵</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        <span class="comment"># 使用所有类标签的发生频率计算类别出现的概率。</span></span><br><span class="line">        prob = float(labelCounts[key])/numEntries</span><br><span class="line">        <span class="comment"># 计算香农熵，以 2 为底求对数</span></span><br><span class="line">        shannonEnt -= prob * log(prob, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure>

<p>按照给定特征划分数据集</p>
<p><code>将指定特征的特征值等于 value 的行剩下列作为子数据集。</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, index, value)</span>:</span></span><br><span class="line">    <span class="string">"""splitDataSet(通过遍历dataSet数据集，求出index对应的colnum列的值为value的行)</span></span><br><span class="line"><span class="string">        就是依据index列进行分类，如果index列的数据等于 value的时候，就要将 index 划分到我们创建的新的数据集中</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 数据集                 待划分的数据集</span></span><br><span class="line"><span class="string">        index 表示每一行的index列        划分数据集的特征</span></span><br><span class="line"><span class="string">        value 表示index列对应的value值   需要返回的特征的值。</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        index列为value的数据集【该数据集需要排除index列】</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet: </span><br><span class="line">        <span class="comment"># index列为value的数据集【该数据集需要排除index列】</span></span><br><span class="line">        <span class="comment"># 判断index列的值是否为value</span></span><br><span class="line">        <span class="keyword">if</span> featVec[index] == value:</span><br><span class="line">            <span class="comment"># chop out index used for splitting</span></span><br><span class="line">            <span class="comment"># [:index]表示前index行，即若 index 为2，就是取 featVec 的前 index 行</span></span><br><span class="line">            reducedFeatVec = featVec[:index]</span><br><span class="line">            <span class="string">'''</span></span><br><span class="line"><span class="string">            请百度查询一下： extend和append的区别</span></span><br><span class="line"><span class="string">            music_media.append(object) 向列表中添加一个对象object</span></span><br><span class="line"><span class="string">            music_media.extend(sequence) 把一个序列seq的内容添加到列表中 (跟 += 在list运用类似， music_media += sequence)</span></span><br><span class="line"><span class="string">            1、使用append的时候，是将object看作一个对象，整体打包添加到music_media对象中。</span></span><br><span class="line"><span class="string">            2、使用extend的时候，是将sequence看作一个序列，将这个序列和music_media序列合并，并放在其后面。</span></span><br><span class="line"><span class="string">            music_media = []</span></span><br><span class="line"><span class="string">            music_media.extend([1,2,3])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果：</span></span><br><span class="line"><span class="string">            #[1, 2, 3]</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            music_media.append([4,5,6])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果：</span></span><br><span class="line"><span class="string">            #[1, 2, 3, [4, 5, 6]]</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            music_media.extend([7,8,9])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果：</span></span><br><span class="line"><span class="string">            #[1, 2, 3, [4, 5, 6], 7, 8, 9]</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line">            reducedFeatVec.extend(featVec[index+<span class="number">1</span>:])</span><br><span class="line">            <span class="comment"># [index+1:]表示从跳过 index 的 index+1行，取接下来的数据</span></span><br><span class="line">            <span class="comment"># 收集结果值 index列为value的行【该行需要排除index列】</span></span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br></pre></td></tr></table></figure>

<p>选择最好的数据集划分方式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""chooseBestFeatureToSplit(选择最好的特征)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bestFeature 最优的特征列</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 求第一行有多少列的 Feature, 最后一列是label列嘛</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    <span class="comment"># 数据集的原始信息熵</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    <span class="comment"># 最优的信息增益值, 和最优的Featurn编号</span></span><br><span class="line">    bestInfoGain, bestFeature = <span class="number">0.0</span>, <span class="number">-1</span></span><br><span class="line">    <span class="comment"># iterate over all the features</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">        <span class="comment"># create a list of all the examples of this feature</span></span><br><span class="line">        <span class="comment"># 获取对应的feature下的所有数据</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        <span class="comment"># get a set of unique values</span></span><br><span class="line">        <span class="comment"># 获取剔重后的集合，使用set对list数据进行去重</span></span><br><span class="line">        uniqueVals = set(featList)</span><br><span class="line">        <span class="comment"># 创建一个临时的信息熵</span></span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 遍历某一列的value集合，计算该列的信息熵 </span></span><br><span class="line">        <span class="comment"># 遍历当前特征中的所有唯一属性值，对每个唯一属性值划分一次数据集，计算数据集的新熵值，并对所有唯一特征值得到的熵求和。</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            <span class="comment"># 计算概率</span></span><br><span class="line">            prob = len(subDataSet)/float(len(dataSet))</span><br><span class="line">            <span class="comment"># 计算信息熵</span></span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">        <span class="comment"># gain[信息增益]: 划分数据集前后的信息变化， 获取信息熵最大的值</span></span><br><span class="line">        <span class="comment"># 信息增益是熵的减少或者是数据无序度的减少。最后，比较所有特征中的信息增益，返回最好特征划分的索引值。</span></span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'infoGain='</span>, infoGain, <span class="string">'bestFeature='</span>, i, baseEntropy, newEntropy</span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure>

<figure class="highlight haxe"><table><tr><td class="code"><pre><span class="line">问：上面的 <span class="keyword">new</span><span class="type">Entropy</span> 为什么是根据子集计算的呢？</span><br><span class="line">答：因为我们在根据一个特征计算香农熵的时候，该特征的分类值是相同，这个特征这个分类的香农熵为 <span class="number">0</span>；</span><br><span class="line">这就是为什么计算新的香农熵的时候使用的是子集。</span><br></pre></td></tr></table></figure>

<blockquote>
<p>训练算法：构造树的数据结构</p>
</blockquote>
<p>创建树的函数代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, labels)</span>:</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    <span class="comment"># 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行</span></span><br><span class="line">    <span class="comment"># 第一个停止条件：所有的类标签完全相同，则直接返回该类标签。</span></span><br><span class="line">    <span class="comment"># count() 函数是统计括号中的值在list中出现的次数</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果</span></span><br><span class="line">    <span class="comment"># 第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。</span></span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择最优的列，得到最优列对应的label含义</span></span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    <span class="comment"># 获取label的名称</span></span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    <span class="comment"># 初始化myTree</span></span><br><span class="line">    myTree = &#123;bestFeatLabel: &#123;&#125;&#125;</span><br><span class="line">    <span class="comment"># 注：labels列表是可变对象，在PYTHON函数中作为参数时传址引用，能够被全局修改</span></span><br><span class="line">    <span class="comment"># 所以这行代码导致函数外的同名变量被删除了元素，造成例句无法执行，提示'no surfacing' is not in list</span></span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    <span class="comment"># 取出最优列，然后它的branch做分类</span></span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        <span class="comment"># 求出剩余的标签label</span></span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        <span class="comment"># 遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree()</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)</span><br><span class="line">        <span class="comment"># print 'myTree', value, myTree</span></span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法：使用决策树执行分类</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree, featLabels, testVec)</span>:</span></span><br><span class="line">    <span class="string">"""classify(给输入的节点，进行分类)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        inputTree  决策树模型</span></span><br><span class="line"><span class="string">        featLabels Feature标签对应的名称</span></span><br><span class="line"><span class="string">        testVec    测试输入的数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        classLabel 分类的结果值，需要映射label才能知道名称</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 获取tree的根节点对于的key值</span></span><br><span class="line">    firstStr = list(inputTree.keys())[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 通过key得到根节点对应的value</span></span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    <span class="comment"># 判断根节点名称获取根节点在label中的先后顺序，这样就知道输入的testVec怎么开始对照树来做分类</span></span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    <span class="comment"># 测试数据，找到根节点对应的label位置，也就知道从输入的数据的第几位来开始分类</span></span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'+++'</span>, firstStr, <span class="string">'xxx'</span>, secondDict, <span class="string">'---'</span>, key, <span class="string">'&gt;&gt;&gt;'</span>, valueOfFeat</span><br><span class="line">    <span class="comment"># 判断分枝是否结束: 判断valueOfFeat是否是dict类型</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(valueOfFeat, dict):</span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。</p>
</blockquote>
<h3 id="项目案例2-使用决策树预测隐形眼镜类型"><a href="#项目案例2-使用决策树预测隐形眼镜类型" class="headerlink" title="项目案例2: 使用决策树预测隐形眼镜类型"></a>项目案例2: 使用决策树预测隐形眼镜类型</h3><p><a href="/src/py2.x/ml/3.DecisionTree/DecisionTree.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py</a></p>
<h4 id="项目概述-1"><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4><p>隐形眼镜类型包括硬材质、软材质以及不适合佩戴隐形眼镜。我们需要使用决策树预测患者需要佩戴的隐形眼镜类型。</p>
<h4 id="开发流程-1"><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4><ol>
<li>收集数据: 提供的文本文件。</li>
<li>解析数据: 解析 tab 键分隔的数据行</li>
<li>分析数据: 快速检查数据，确保正确地解析数据内容，使用 createPlot() 函数绘制最终的树形图。</li>
<li>训练算法: 使用 createTree() 函数。</li>
<li>测试算法: 编写测试函数验证决策树可以正确分类给定的数据实例。</li>
<li>使用算法: 存储树的数据结构，以便下次使用时无需重新构造树。</li>
</ol>
<blockquote>
<p>收集数据：提供的文本文件</p>
</blockquote>
<p>文本文件数据格式如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">young</span>	<span class="string">myope</span>	<span class="literal">no</span>	<span class="string">reduced</span>	<span class="literal">no</span> <span class="string">lenses</span></span><br><span class="line"><span class="string">pre</span>	<span class="string">myope</span>	<span class="literal">no</span>	<span class="string">reduced</span>	<span class="literal">no</span> <span class="string">lenses</span></span><br><span class="line"><span class="string">presbyopic</span>	<span class="string">myope</span>	<span class="literal">no</span>	<span class="string">reduced</span>	<span class="literal">no</span> <span class="string">lenses</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>解析数据：解析 tab 键分隔的数据行</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lecses = [inst.strip().split(<span class="string">'\t'</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readlines()]</span><br><span class="line">lensesLabels = [<span class="string">'age'</span>, <span class="string">'prescript'</span>, <span class="string">'astigmatic'</span>, <span class="string">'tearRate'</span>]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据：快速检查数据，确保正确地解析数据内容，使用 createPlot() 函数绘制最终的树形图。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>treePlotter.createPlot(lensesTree)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>训练算法：使用 createTree() 函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>lensesTree = trees.createTree(lenses, lensesLabels)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lensesTree</span><br><span class="line">&#123;<span class="string">'tearRate'</span>: &#123;<span class="string">'reduced'</span>: <span class="string">'no lenses'</span>, <span class="string">'normal'</span>: &#123;<span class="string">'astigmatic'</span>:&#123;<span class="string">'yes'</span>:</span><br><span class="line">&#123;<span class="string">'prescript'</span>:&#123;<span class="string">'hyper'</span>:&#123;<span class="string">'age'</span>:&#123;<span class="string">'pre'</span>:<span class="string">'no lenses'</span>, <span class="string">'presbyopic'</span>:</span><br><span class="line"><span class="string">'no lenses'</span>, <span class="string">'young'</span>:<span class="string">'hard'</span>&#125;&#125;, <span class="string">'myope'</span>:<span class="string">'hard'</span>&#125;&#125;, <span class="string">'no'</span>:&#123;<span class="string">'age'</span>:&#123;<span class="string">'pre'</span>:</span><br><span class="line"><span class="string">'soft'</span>, <span class="string">'presbyopic'</span>:&#123;<span class="string">'prescript'</span>: &#123;<span class="string">'hyper'</span>:<span class="string">'soft'</span>, <span class="string">'myope'</span>:</span><br><span class="line"><span class="string">'no lenses'</span>&#125;&#125;, <span class="string">'young'</span>:<span class="string">'soft'</span>&#125;&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 编写测试函数验证决策树可以正确分类给定的数据实例。</p>
</blockquote>
<blockquote>
<p>使用算法: 存储树的数据结构，以便下次使用时无需重新构造树。</p>
</blockquote>
<p>使用 pickle 模块存储决策树</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree, filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = open(filename, <span class="string">'wb'</span>)</span><br><span class="line">    pickle.dump(inputTree, fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = open(filename, <span class="string">'rb'</span>)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure>


<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a>      </li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第5章 Logistic回归</title>
    <url>/2020/06/01/5.Logistic%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h2 id="Logistic-回归-概述"><a href="#Logistic-回归-概述" class="headerlink" title="Logistic 回归 概述"></a>Logistic 回归 概述</h2><p><code>Logistic 回归 或者叫逻辑回归 虽然名字有回归，但是它是用来做分类的。其主要思想是: 根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类。</code></p>
<a id="more"></a>
<h2 id="须知概念"><a href="#须知概念" class="headerlink" title="须知概念"></a>须知概念</h2><h3 id="Sigmoid-函数"><a href="#Sigmoid-函数" class="headerlink" title="Sigmoid 函数"></a>Sigmoid 函数</h3><h4 id="回归-概念"><a href="#回归-概念" class="headerlink" title="回归 概念"></a>回归 概念</h4><p>假设现在有一些数据点，我们用一条直线对这些点进行拟合（这条直线称为最佳拟合直线），这个拟合的过程就叫做回归。进而可以得到对这些点的拟合直线方程，那么我们根据这个回归方程，怎么进行分类呢？请看下面。</p>
<h4 id="二值型输出分类函数"><a href="#二值型输出分类函数" class="headerlink" title="二值型输出分类函数"></a>二值型输出分类函数</h4><p>我们想要的函数应该是: 能接受所有的输入然后预测出类别。例如，在两个类的情况下，上述函数输出 0 或 1.或许你之前接触过具有这种性质的函数，该函数称为 <code>海维塞得阶跃函数(Heaviside step function)</code>，或者直接称为 <code>单位阶跃函数</code>。然而，海维塞得阶跃函数的问题在于: 该函数在跳跃点上从 0 瞬间跳跃到 1，这个瞬间跳跃过程有时很难处理。幸好，另一个函数也有类似的性质（可以输出 0 或者 1 的性质），且数学上更易处理，这就是 Sigmoid 函数。 Sigmoid 函数具体的计算公式如下: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_1.png" alt="Sigmoid 函数计算公式"></p>
<p>下图给出了 Sigmoid 函数在不同坐标尺度下的两条曲线图。当 x 为 0 时，Sigmoid 函数值为 0.5 。随着 x 的增大，对应的 Sigmoid 值将逼近于 1 ; 而随着 x 的减小， Sigmoid 值将逼近于 0 。如果横坐标刻度足够大， Sigmoid 函数看起来很像一个阶跃函数。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_3.png" alt="Sigmoid 函数在不同坐标下的图片"></p>
<p>因此，为了实现 Logistic 回归分类器，我们可以在每个特征上都乘以一个回归系数（如下公式所示），然后把所有结果值相加，将这个总和代入 Sigmoid 函数中，进而得到一个范围在 0~1 之间的数值。任何大于 0.5 的数据被分入 1 类，小于 0.5 即被归入 0 类。所以，Logistic 回归也是一种概率估计，比如这里Sigmoid 函数得出的值为0.5，可以理解为给定数据和参数，数据被分入 1 类的概率为0.5。想对Sigmoid 函数有更多了解，可以点开<a href="https://www.desmos.com/calculator/bgontvxotm" target="_blank" rel="noopener">此链接</a>跟此函数互动。</p>
<h3 id="基于最优化方法的回归系数确定"><a href="#基于最优化方法的回归系数确定" class="headerlink" title="基于最优化方法的回归系数确定"></a>基于最优化方法的回归系数确定</h3><p>Sigmoid 函数的输入记为 z ，由下面公式得到: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_2.png" alt="Sigmoid 函数计算公式"></p>
<p>如果采用向量的写法，上述公式可以写成 <img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_4.png" alt="Sigmoid 函数计算公式向量形式"> ，它表示将这两个数值向量对应元素相乘然后全部加起来即得到 z 值。其中的向量 x 是分类器的输入数据，向量 w 也就是我们要找到的最佳参数（系数），从而使得分类器尽可能地精确。为了寻找该最佳参数，需要用到最优化理论的一些知识。我们这里使用的是——梯度上升法（Gradient Ascent）。</p>
<h3 id="梯度上升法"><a href="#梯度上升法" class="headerlink" title="梯度上升法"></a>梯度上升法</h3><h4 id="梯度的介绍"><a href="#梯度的介绍" class="headerlink" title="梯度的介绍"></a>梯度的介绍</h4><p>需要一点点向量方面的数学知识</p>
<figure class="highlight fix"><table><tr><td class="code"><pre><span class="line"><span class="attr">向量 </span>=<span class="string"> 值 + 方向  </span></span><br><span class="line"><span class="string">梯度 = 向量</span></span><br><span class="line"><span class="string">梯度 = 梯度值 + 梯度方向</span></span><br></pre></td></tr></table></figure>

<h4 id="梯度上升法的思想"><a href="#梯度上升法的思想" class="headerlink" title="梯度上升法的思想"></a>梯度上升法的思想</h4><p>要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为 ▽ ，则函数 f(x, y) 的梯度由下式表示: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_5.png" alt="梯度上升计算公式"></p>
<p>这个梯度意味着要沿 x 的方向移动 <img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_6.png" alt="f(x, y)对x求偏导"> ，沿 y 的方向移动 <img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_7.png" alt="f(x, y)对y求偏导"> 。其中，函数f(x, y) 必须要在待计算的点上有定义并且可微。下图是一个具体的例子。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_8.png" alt="梯度上升"></p>
<p>上图展示的，梯度上升算法到达每个点后都会重新估计移动的方向。从 P0 开始，计算完该点的梯度，函数就根据梯度移动到下一点 P1。在 P1 点，梯度再次被重新计算，并沿着新的梯度方向移动到 P2 。如此循环迭代，直到满足停止条件。迭代过程中，梯度算子总是保证我们能选取到最佳的移动方向。</p>
<p>上图中的梯度上升算法沿梯度方向移动了一步。可以看到，梯度算子总是指向函数值增长最快的方向。这里所说的是移动方向，而未提到移动量的大小。该量值称为步长，记作 α 。用向量来表示的话，梯度上升算法的迭代公式如下: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_9.png" alt="梯度上升迭代公式"></p>
<p>该公式将一直被迭代执行，直至达到某个停止条件为止，比如迭代次数达到某个指定值或者算法达到某个可以允许的误差范围。</p>
<p>介绍一下几个相关的概念：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">例如：y = w0 + w1x1 + w2x2 + ... + wnxn</span><br><span class="line">梯度：参考上图的例子，二维图像，x方向代表第一个系数，也就是 w1，y方向代表第二个系数也就是 w2，这样的向量就是梯度。</span><br><span class="line">α：上面的梯度算法的迭代公式中的阿尔法，这个代表的是移动步长（step length）。移动步长会影响最终结果的拟合程度，最好的方法就是随着迭代次数更改移动步长。</span><br><span class="line">步长通俗的理解，<span class="number">100</span>米，如果我一步走<span class="number">10</span>米，我需要走<span class="number">10</span>步；如果一步走<span class="number">20</span>米，我只需要走<span class="number">5</span>步。这里的一步走多少米就是步长的意思。</span><br><span class="line">▽f(w)：代表沿着梯度变化的方向。</span><br></pre></td></tr></table></figure>


<p>   问：有人会好奇为什么有些书籍上说的是梯度下降法（Gradient Decent）?</p>
<p>   答： 其实这个两个方法在此情况下本质上是相同的。关键在于代价函数（cost function）或者叫目标函数（objective function）。如果目标函数是损失函数，那就是最小化损失函数来求函数的最小值，就用梯度下降。 如果目标函数是似然函数（Likelihood function），就是要最大化似然函数来求函数的最大值，那就用梯度上升。在逻辑回归中， 损失函数和似然函数无非就是互为正负关系。</p>
<p>   只需要在迭代公式中的加法变成减法。因此，对应的公式可以写成</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_10.png" alt="梯度下降迭代公式"></p>
<p><strong>局部最优现象 （Local Optima）</strong></p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_20.png" alt="梯度下降图_4"></p>
<p>上图表示参数 θ 与误差函数 J(θ) 的关系图 (这里的误差函数是损失函数，所以我们要最小化损失函数)，红色的部分是表示 J(θ) 有着比较高的取值，我们需要的是，能够让 J(θ) 的值尽量的低。也就是深蓝色的部分。θ0，θ1 表示 θ 向量的两个维度（此处的θ0，θ1是x0和x1的系数，也对应的是上文w0和w1）。</p>
<p>可能梯度下降的最终点并非是全局最小点，可能是一个局部最小点，如我们上图中的右边的梯度下降曲线，描述的是最终到达一个局部最小点，这是我们重新选择了一个初始点得到的。</p>
<p>看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。</p>
<h2 id="Logistic-回归-原理"><a href="#Logistic-回归-原理" class="headerlink" title="Logistic 回归 原理"></a>Logistic 回归 原理</h2><h3 id="Logistic-回归-工作原理"><a href="#Logistic-回归-工作原理" class="headerlink" title="Logistic 回归 工作原理"></a>Logistic 回归 工作原理</h3><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">每个回归系数初始化为 <span class="number">1</span></span><br><span class="line">重复 R 次:</span><br><span class="line">    计算整个数据集的梯度</span><br><span class="line">    使用 步长 x 梯度 更新回归系数的向量</span><br><span class="line">返回回归系数</span><br></pre></td></tr></table></figure>

<h3 id="Logistic-回归-开发流程"><a href="#Logistic-回归-开发流程" class="headerlink" title="Logistic 回归 开发流程"></a>Logistic 回归 开发流程</h3><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 采用任意方法收集数据</span></span><br><span class="line"><span class="section">准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。</span></span><br><span class="line"><span class="section">分析数据: 采用任意方法对数据进行分析。</span></span><br><span class="line"><span class="section">训练算法: 大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。</span></span><br><span class="line"><span class="section">测试算法: 一旦训练步骤完成，分类将会很快。</span></span><br><span class="line"><span class="section">使用算法: 首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。</span></span><br></pre></td></tr></table></figure>

<h3 id="Logistic-回归-算法特点"><a href="#Logistic-回归-算法特点" class="headerlink" title="Logistic 回归 算法特点"></a>Logistic 回归 算法特点</h3><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">优点: 计算代价不高，易于理解和实现。</span></span><br><span class="line"><span class="section">缺点: 容易欠拟合，分类精度可能不高。</span></span><br><span class="line"><span class="section">适用数据类型: 数值型和标称型数据。</span></span><br></pre></td></tr></table></figure>

<h3 id="附加-方向导数与梯度"><a href="#附加-方向导数与梯度" class="headerlink" title="附加 方向导数与梯度"></a>附加 方向导数与梯度</h3><p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6.png" alt="方向导数与梯度"></p>
<h2 id="Logistic-回归-项目案例"><a href="#Logistic-回归-项目案例" class="headerlink" title="Logistic 回归 项目案例"></a>Logistic 回归 项目案例</h2><h3 id="项目案例1-使用-Logistic-回归在简单数据集上的分类"><a href="#项目案例1-使用-Logistic-回归在简单数据集上的分类" class="headerlink" title="项目案例1: 使用 Logistic 回归在简单数据集上的分类"></a>项目案例1: 使用 Logistic 回归在简单数据集上的分类</h3><p><a href="/src/py2.x/ml/5.Logistic/logistic.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/5.Logistic/logistic.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/5.Logistic/logistic.py</a></p>
<h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p>在一个简单的数据集上，采用梯度上升法找到 Logistic 回归分类器在此数据集上的最佳回归系数</p>
<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 可以使用任何方法</span></span><br><span class="line"><span class="section">准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳</span></span><br><span class="line"><span class="section">分析数据: 画出决策边界</span></span><br><span class="line"><span class="section">训练算法: 使用梯度上升找到最佳参数</span></span><br><span class="line"><span class="section">测试算法: 使用 Logistic 回归进行分类</span></span><br><span class="line"><span class="section">使用算法: 对简单数据集中数据进行分类</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 可以使用任何方法</p>
</blockquote>
<p>我们采用存储在 TestSet.txt 文本文件中的数据，存储格式如下: </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">-0.017612</span>	<span class="number">14.053064</span>	<span class="number">0</span></span><br><span class="line"><span class="number">-1.395634</span>	<span class="number">4.662541</span>	<span class="number">1</span></span><br><span class="line"><span class="number">-0.752157</span>	<span class="number">6.538620</span>	<span class="number">0</span></span><br><span class="line"><span class="number">-1.322371</span>	<span class="number">7.152853</span>	<span class="number">0</span></span><br><span class="line"><span class="number">0.423363</span>	<span class="number">11.054677</span>	<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>绘制在图中，如下图所示: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_11.png" alt="简单数据集绘制在图上"></p>
<blockquote>
<p>准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 解析数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(file_name)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Desc: </span></span><br><span class="line"><span class="string">        加载并解析数据</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        file_name -- 要解析的文件路径</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dataMat -- 原始数据的特征</span></span><br><span class="line"><span class="string">        labelMat -- 原始数据的标签，也就是每条样本对应的类别。即目标向量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># dataMat为原始数据， labelMat为原始数据的标签</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = open(file_name)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = line.strip().split()</span><br><span class="line">        <span class="comment"># 为了方便计算，我们将 X0 的值设为 1.0 ，也就是在每一行的开头添加一个 1.0 作为 X0</span></span><br><span class="line">        dataMat.append([<span class="number">1.0</span>, float(lineArr[<span class="number">0</span>]), float(lineArr[<span class="number">1</span>])])</span><br><span class="line">        labelMat.append(int(lineArr[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据: 采用任意方法对数据进行分析，此处不需要</p>
</blockquote>
<blockquote>
<p>训练算法: 使用梯度上升找到最佳参数</p>
</blockquote>
<p>定义sigmoid阶跃函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sigmoid阶跃函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(inX)</span>:</span></span><br><span class="line">    <span class="comment"># return 1.0 / (1 + exp(-inX))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tanh是Sigmoid的变形，与 sigmoid 不同的是，tanh 是0均值的。因此，实际应用中，tanh 会比 sigmoid 更好。</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * <span class="number">1.0</span>/(<span class="number">1</span>+exp(<span class="number">-2</span>*inX)) - <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>Logistic 回归梯度上升优化算法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正常的处理方案</span></span><br><span class="line"><span class="comment"># 两个参数：第一个参数==&gt; dataMatIn 是一个2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本。</span></span><br><span class="line"><span class="comment"># 第二个参数==&gt; classLabels 是类别标签，它是一个 1*100 的行向量。为了便于矩阵计算，需要将该行向量转换为列向量，做法是将原向量转置，再将它赋值给labelMat。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn, classLabels)</span>:</span></span><br><span class="line">    <span class="comment"># 转化为矩阵[[1,1,2],[1,1,2]....]</span></span><br><span class="line">    dataMatrix = mat(dataMatIn)             <span class="comment"># 转换为 NumPy 矩阵</span></span><br><span class="line">    <span class="comment"># 转化为矩阵[[0,1,0,1,0,1.....]]，并转制[[0],[1],[0].....]</span></span><br><span class="line">    <span class="comment"># transpose() 行列转置函数</span></span><br><span class="line">    <span class="comment"># 将行向量转化为列向量   =&gt;  矩阵的转置</span></span><br><span class="line">    labelMat = mat(classLabels).transpose() <span class="comment"># 首先将数组转换为 NumPy 矩阵，然后再将行向量转置为列向量</span></span><br><span class="line">    <span class="comment"># m-&gt;数据量，样本数 n-&gt;特征数</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    <span class="comment"># print m, n, '__'*10, shape(dataMatrix.transpose()), '__'*100</span></span><br><span class="line">    <span class="comment"># alpha代表向目标移动的步长</span></span><br><span class="line">    alpha = <span class="number">0.001</span></span><br><span class="line">    <span class="comment"># 迭代次数</span></span><br><span class="line">    maxCycles = <span class="number">500</span></span><br><span class="line">    <span class="comment"># 生成一个长度和特征数相同的矩阵，此处n为3 -&gt; [[1],[1],[1]]</span></span><br><span class="line">    <span class="comment"># weights 代表回归系数， 此处的 ones((n,1)) 创建一个长度和特征数相同的矩阵，其中的数全部都是 1</span></span><br><span class="line">    weights = ones((n,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):              <span class="comment">#heavy on matrix operations</span></span><br><span class="line">        <span class="comment"># m*3 的矩阵 * 3*1 的矩阵 ＝ m*1的矩阵</span></span><br><span class="line">        <span class="comment"># 那么乘上矩阵的意义，就代表：通过公式得到的理论值</span></span><br><span class="line">        <span class="comment"># 参考地址： 矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/31050145</span></span><br><span class="line">        <span class="comment"># print 'dataMatrix====', dataMatrix </span></span><br><span class="line">        <span class="comment"># print 'weights====', weights</span></span><br><span class="line">        <span class="comment"># n*3   *  3*1  = n*1</span></span><br><span class="line">        h = sigmoid(dataMatrix*weights)     <span class="comment"># 矩阵乘法</span></span><br><span class="line">        <span class="comment"># print 'hhhhhhh====', h</span></span><br><span class="line">        <span class="comment"># labelMat是实际值</span></span><br><span class="line">        error = (labelMat - h)              <span class="comment"># 向量相减</span></span><br><span class="line">        <span class="comment"># 0.001* (3*m)*(m*1) 表示在每一个列上的一个误差情况，最后得出 x1,x2,xn的系数的偏移量</span></span><br><span class="line">        weights = weights + alpha * dataMatrix.transpose() * error <span class="comment"># 矩阵乘法，最后得到回归系数</span></span><br><span class="line">    <span class="keyword">return</span> array(weights)</span><br></pre></td></tr></table></figure>

<p>大家看到这儿可能会有一些疑惑，就是，我们在迭代中更新我们的回归系数，后边的部分是怎么计算出来的？为什么会是 alpha * dataMatrix.transpose() * error ?因为这就是我们所求的梯度，也就是对 f(w) 对 w 求一阶导数。具体推导如下:</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_21.png" alt="f(w)对w求一阶导数"><br>可参考<a href="http://blog.csdn.net/achuo/article/details/51160101" target="_blank" rel="noopener">http://blog.csdn.net/achuo/article/details/51160101</a></p>
<p>画出数据集和 Logistic 回归最佳拟合直线的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotBestFit</span><span class="params">(dataArr, labelMat, weights)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Desc:</span></span><br><span class="line"><span class="string">            将我们得到的数据可视化展示出来</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dataArr:样本数据的特征</span></span><br><span class="line"><span class="string">            labelMat:样本数据的类别标签，即目标变量</span></span><br><span class="line"><span class="string">            weights:回归系数</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    n = shape(dataArr)[<span class="number">0</span>]</span><br><span class="line">    xcord1 = []; ycord1 = []</span><br><span class="line">    xcord2 = []; ycord2 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> int(labelMat[i])== <span class="number">1</span>:</span><br><span class="line">            xcord1.append(dataArr[i,<span class="number">1</span>]); ycord1.append(dataArr[i,<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xcord2.append(dataArr[i,<span class="number">1</span>]); ycord2.append(dataArr[i,<span class="number">2</span>])</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.scatter(xcord1, ycord1, s=<span class="number">30</span>, c=<span class="string">'red'</span>, marker=<span class="string">'s'</span>)</span><br><span class="line">    ax.scatter(xcord2, ycord2, s=<span class="number">30</span>, c=<span class="string">'green'</span>)</span><br><span class="line">    x = arange(<span class="number">-3.0</span>, <span class="number">3.0</span>, <span class="number">0.1</span>)</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    y的由来，卧槽，是不是没看懂？</span></span><br><span class="line"><span class="string">    首先理论上是这个样子的。</span></span><br><span class="line"><span class="string">    dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])</span></span><br><span class="line"><span class="string">    w0*x0+w1*x1+w2*x2=f(x)</span></span><br><span class="line"><span class="string">    x0最开始就设置为1叻， x2就是我们画图的y值，而f(x)被我们磨合误差给算到w0,w1,w2身上去了</span></span><br><span class="line"><span class="string">    所以： w0+w1*x+w2*y=0 =&gt; y = (-w0-w1*x)/w2   </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    y = (-weights[<span class="number">0</span>]-weights[<span class="number">1</span>]*x)/weights[<span class="number">2</span>]</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    plt.xlabel(<span class="string">'X'</span>); plt.ylabel(<span class="string">'Y'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 使用 Logistic 回归进行分类</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testLR</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 1.收集并准备数据</span></span><br><span class="line">    dataMat, labelMat = loadDataSet(<span class="string">"data/5.Logistic/TestSet.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print dataMat, '---\n', labelMat</span></span><br><span class="line">    <span class="comment"># 2.训练模型，  f(x)=a1*x1+b2*x2+..+nn*xn中 (a1,b2, .., nn).T的矩阵值</span></span><br><span class="line">    <span class="comment"># 因为数组没有是复制n份， array的乘法就是乘法</span></span><br><span class="line">    dataArr = array(dataMat)</span><br><span class="line">    <span class="comment"># print dataArr</span></span><br><span class="line">    weights = gradAscent(dataArr, labelMat)</span><br><span class="line">    <span class="comment"># weights = stocGradAscent0(dataArr, labelMat)</span></span><br><span class="line">    <span class="comment"># weights = stocGradAscent1(dataArr, labelMat)</span></span><br><span class="line">    <span class="comment"># print '*'*30, weights</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据可视化</span></span><br><span class="line">    plotBestFit(dataArr, labelMat, weights)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法: 对简单数据集中数据进行分类</p>
</blockquote>
<h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p>梯度上升算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理 100 个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为 <code>随机梯度上升算法</code>。由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习(online learning)算法。与 “在线学习” 相对应，一次处理所有数据被称作是 “批处理” （batch） 。</p>
<p>随机梯度上升算法可以写成如下的伪代码: </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">所有回归系数初始化为 <span class="number">1</span></span><br><span class="line">对数据集中每个样本</span><br><span class="line">    计算该样本的梯度</span><br><span class="line">    使用 alpha x gradient 更新回归系数值</span><br><span class="line">返回回归系数值</span><br></pre></td></tr></table></figure>

<p>以下是随机梯度上升算法的实现代码: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机梯度上升</span></span><br><span class="line"><span class="comment"># 梯度上升优化算法在每次更新数据集时都需要遍历整个数据集，计算复杂都较高</span></span><br><span class="line"><span class="comment"># 随机梯度上升一次只用一个样本点来更新回归系数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span><span class="params">(dataMatrix, classLabels)</span>:</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># n*1的矩阵</span></span><br><span class="line">    <span class="comment"># 函数ones创建一个全1的数组</span></span><br><span class="line">    weights = ones(n)   <span class="comment"># 初始化长度为n的数组，元素全部为 1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,此处求出的 h 是一个具体的数值，而不是一个矩阵</span></span><br><span class="line">        h = sigmoid(sum(dataMatrix[i]*weights))</span><br><span class="line">        <span class="comment"># print 'dataMatrix[i]===', dataMatrix[i]</span></span><br><span class="line">        <span class="comment"># 计算真实类别与预测类别之间的差值，然后按照该差值调整回归系数</span></span><br><span class="line">        error = classLabels[i] - h</span><br><span class="line">        <span class="comment"># 0.01*(1*1)*(1*n)</span></span><br><span class="line">        <span class="keyword">print</span> weights, <span class="string">"*"</span>*<span class="number">10</span> , dataMatrix[i], <span class="string">"*"</span>*<span class="number">10</span> , error</span><br><span class="line">        weights = weights + alpha * error * dataMatrix[i]</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>

<p>可以看到，随机梯度上升算法与梯度上升算法在代码上很相似，但也有一些区别: 第一，后者的变量 h 和误差 error 都是向量，而前者则全是数值；第二，前者没有矩阵的转换过程，所有变量的数据类型都是 NumPy 数组。</p>
<p>判断优化算法优劣的可靠方法是看它是否收敛，也就是说参数是否达到了稳定值，是否还会不断地变化？下图展示了随机梯度上升算法在 200 次迭代过程中回归系数的变化情况。其中的系数2，也就是 X2 只经过了 50 次迭代就达到了稳定值，但系数 1 和 0 则需要更多次的迭代。如下图所示: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_12.png" alt="回归系数与迭代次数的关系图"></p>
<p>针对这个问题，我们改进了之前的随机梯度上升算法，如下: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机梯度上升算法（随机化）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataMatrix, classLabels, numIter=<span class="number">150</span>)</span>:</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    weights = ones(n)   <span class="comment"># 创建与列数相同的矩阵的系数矩阵，所有的元素都是1</span></span><br><span class="line">    <span class="comment"># 随机梯度, 循环150,观察是否收敛</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">        <span class="comment"># [0, 1, 2 .. m-1]</span></span><br><span class="line">        dataIndex = range(m)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># i和j的不断增大，导致alpha的值不断减少，但是不为0</span></span><br><span class="line">            alpha = <span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.0001</span>    <span class="comment"># alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001</span></span><br><span class="line">            <span class="comment"># 随机产生一个 0～len()之间的一个值</span></span><br><span class="line">            <span class="comment"># random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。</span></span><br><span class="line">            randIndex = int(random.uniform(<span class="number">0</span>,len(dataIndex)))</span><br><span class="line">            <span class="comment"># sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn</span></span><br><span class="line">            h = sigmoid(sum(dataMatrix[dataIndex[randIndex]]*weights))</span><br><span class="line">            error = classLabels[dataIndex[randIndex]] - h</span><br><span class="line">            <span class="comment"># print weights, '__h=%s' % h, '__'*20, alpha, '__'*20, error, '__'*20, dataMatrix[randIndex]</span></span><br><span class="line">            weights = weights + alpha * error * dataMatrix[dataIndex[randIndex]]</span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>

<p>上面的改进版随机梯度上升算法，我们修改了两处代码。</p>
<p>第一处改进为 alpha 的值。alpha 在每次迭代的时候都会调整，这回缓解上面波动图的数据波动或者高频波动。另外，虽然 alpha 会随着迭代次数不断减少，但永远不会减小到 0，因为我们在计算公式中添加了一个常数项。</p>
<p>第二处修改为 randIndex 更新，这里通过随机选取样本拉来更新回归系数。这种方法将减少周期性的波动。这种方法每次随机从列表中选出一个值，然后从列表中删掉该值（再进行下一次迭代）。</p>
<p>程序运行之后能看到类似于下图的结果图。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_13.png" alt="改进随机梯度下降结果图"></p>
<h3 id="项目案例2-从疝气病症预测病马的死亡率"><a href="#项目案例2-从疝气病症预测病马的死亡率" class="headerlink" title="项目案例2: 从疝气病症预测病马的死亡率"></a>项目案例2: 从疝气病症预测病马的死亡率</h3><p><a href="/src/py2.x/ml/5.Logistic/logistic.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/5.Logistic/logistic.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/5.Logistic/logistic.py</a></p>
<h4 id="项目概述-1"><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4><p>使用 Logistic 回归来预测患有疝病的马的存活问题。疝病是描述马胃肠痛的术语。然而，这种病不一定源自马的胃肠问题，其他问题也可能引发马疝病。这个数据集中包含了医院检测马疝病的一些指标，有的指标比较主观，有的指标难以测量，例如马的疼痛级别。</p>
<h4 id="开发流程-1"><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 给定数据文件</span></span><br><span class="line"><span class="section">准备数据: 用 Python 解析文本文件并填充缺失值</span></span><br><span class="line"><span class="section">分析数据: 可视化并观察数据</span></span><br><span class="line"><span class="section">训练算法: 使用优化算法，找到最佳的系数</span></span><br><span class="line"><span class="section">测试算法: 为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，</span></span><br><span class="line">         通过改变迭代的次数和步长的参数来得到更好的回归系数</span><br><span class="line"><span class="section">使用算法: 实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事，</span></span><br><span class="line">         这可以作为留给大家的一道习题</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 给定数据文件</p>
</blockquote>
<p>病马的训练数据已经给出来了，如下形式存储在文本文件中:</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1.000000</span>	<span class="number">1.000000</span>	<span class="number">39.200000</span>	<span class="number">88.000000</span>	<span class="number">20.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">4.000000</span>	<span class="number">1.000000</span>	<span class="number">3.000000</span>	<span class="number">4.000000</span>	<span class="number">2.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">4.000000</span>	<span class="number">2.000000</span>	<span class="number">50.000000</span>	<span class="number">85.000000</span>	<span class="number">2.000000</span>	<span class="number">2.000000</span>	<span class="number">0.000000</span></span><br><span class="line"><span class="number">2.000000</span>	<span class="number">1.000000</span>	<span class="number">38.300000</span>	<span class="number">40.000000</span>	<span class="number">24.000000</span>	<span class="number">1.000000</span>	<span class="number">1.000000</span>	<span class="number">3.000000</span>	<span class="number">1.000000</span>	<span class="number">3.000000</span>	<span class="number">3.000000</span>	<span class="number">1.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">1.000000</span>	<span class="number">1.000000</span>	<span class="number">33.000000</span>	<span class="number">6.700000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">1.000000</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>准备数据: 用 Python 解析文本文件并填充缺失值</p>
</blockquote>
<p>处理数据中的缺失值</p>
<p>假设有100个样本和20个特征，这些数据都是机器收集回来的。若机器上的某个传感器损坏导致一个特征无效时该怎么办？此时是否要扔掉整个数据？这种情况下，另外19个特征怎么办？<br>它们是否还可以用？答案是肯定的。因为有时候数据相当昂贵，扔掉和重新获取都是不可取的，所以必须采用一些方法来解决这个问题。</p>
<p>下面给出了一些可选的做法：</p>
<ul>
<li>使用可用特征的均值来填补缺失值；</li>
<li>使用特殊值来填补缺失值，如 -1；</li>
<li>忽略有缺失值的样本；</li>
<li>使用有相似样本的均值添补缺失值；</li>
<li>使用另外的机器学习算法预测缺失值。</li>
</ul>
<p>现在，我们对下一节要用的数据集进行预处理，使其可以顺利地使用分类算法。在预处理需要做两件事: </p>
<ul>
<li><p>所有的缺失值必须用一个实数值来替换，因为我们使用的 NumPy 数据类型不允许包含缺失值。我们这里选择实数 0 来替换所有缺失值，恰好能适用于 Logistic 回归。这样做的直觉在于，我们需要的是一个在更新时不会影响系数的值。回归系数的更新公式如下:</p>
<p>  weights = weights + alpha * error * dataMatrix[dataIndex[randIndex]]</p>
<p>  如果 dataMatrix 的某个特征对应值为 0，那么该特征的系数将不做更新，即:</p>
<p>  weights = weights</p>
<p>  另外，由于 Sigmoid(0) = 0.5 ，即它对结果的预测不具有任何倾向性，因此我们上述做法也不会对误差造成任何影响。基于上述原因，将缺失值用 0 代替既可以保留现有数据，也不需要对优化算法进行修改。此外，该数据集中的特征取值一般不为 0，因此在某种意义上说它也满足 “特殊值” 这个要求。</p>
</li>
<li><p>如果在测试数据集中发现了一条数据的类别标签已经缺失，那么我们的简单做法是将该条数据丢弃。这是因为类别标签与特征不同，很难确定采用某个合适的值来替换。采用 Logistic 回归进行分类时这种做法是合理的，而如果采用类似 kNN 的方法，则保留该条数据显得更加合理。</p>
</li>
</ul>
<p>原始的数据集经过预处理后，保存成两个文件: horseColicTest.txt 和 horseColicTraining.txt 。 </p>
<blockquote>
<p>分析数据: 可视化并观察数据</p>
</blockquote>
<p>将数据使用 MatPlotlib 打印出来，观察数据是否是我们想要的格式</p>
<blockquote>
<p>训练算法: 使用优化算法，找到最佳的系数</p>
</blockquote>
<p>下面给出 原始的梯度上升算法，随机梯度上升算法，改进版随机梯度上升算法 的代码: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正常的处理方案</span></span><br><span class="line"><span class="comment"># 两个参数：第一个参数==&gt; dataMatIn 是一个2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本。</span></span><br><span class="line"><span class="comment"># 第二个参数==&gt; classLabels 是类别标签，它是一个 1*100 的行向量。为了便于矩阵计算，需要将该行向量转换为列向量，做法是将原向量转置，再将它赋值给labelMat。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn, classLabels)</span>:</span></span><br><span class="line">    <span class="comment"># 转化为矩阵[[1,1,2],[1,1,2]....]</span></span><br><span class="line">    dataMatrix = mat(dataMatIn)             <span class="comment"># 转换为 NumPy 矩阵</span></span><br><span class="line">    <span class="comment"># 转化为矩阵[[0,1,0,1,0,1.....]]，并转制[[0],[1],[0].....]</span></span><br><span class="line">    <span class="comment"># transpose() 行列转置函数</span></span><br><span class="line">    <span class="comment"># 将行向量转化为列向量   =&gt;  矩阵的转置</span></span><br><span class="line">    labelMat = mat(classLabels).transpose() <span class="comment"># 首先将数组转换为 NumPy 矩阵，然后再将行向量转置为列向量</span></span><br><span class="line">    <span class="comment"># m-&gt;数据量，样本数 n-&gt;特征数</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    <span class="comment"># print m, n, '__'*10, shape(dataMatrix.transpose()), '__'*100</span></span><br><span class="line">    <span class="comment"># alpha代表向目标移动的步长</span></span><br><span class="line">    alpha = <span class="number">0.001</span></span><br><span class="line">    <span class="comment"># 迭代次数</span></span><br><span class="line">    maxCycles = <span class="number">500</span></span><br><span class="line">    <span class="comment"># 生成一个长度和特征数相同的矩阵，此处n为3 -&gt; [[1],[1],[1]]</span></span><br><span class="line">    <span class="comment"># weights 代表回归系数， 此处的 ones((n,1)) 创建一个长度和特征数相同的矩阵，其中的数全部都是 1</span></span><br><span class="line">    weights = ones((n,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):              <span class="comment">#heavy on matrix operations</span></span><br><span class="line">        <span class="comment"># m*3 的矩阵 * 3*1 的单位矩阵 ＝ m*1的矩阵</span></span><br><span class="line">        <span class="comment"># 那么乘上单位矩阵的意义，就代表：通过公式得到的理论值</span></span><br><span class="line">        <span class="comment"># 参考地址： 矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/31050145</span></span><br><span class="line">        <span class="comment"># print 'dataMatrix====', dataMatrix </span></span><br><span class="line">        <span class="comment"># print 'weights====', weights</span></span><br><span class="line">        <span class="comment"># n*3   *  3*1  = n*1</span></span><br><span class="line">        h = sigmoid(dataMatrix*weights)     <span class="comment"># 矩阵乘法</span></span><br><span class="line">        <span class="comment"># print 'hhhhhhh====', h</span></span><br><span class="line">        <span class="comment"># labelMat是实际值</span></span><br><span class="line">        error = (labelMat - h)              <span class="comment"># 向量相减</span></span><br><span class="line">        <span class="comment"># 0.001* (3*m)*(m*1) 表示在每一个列上的一个误差情况，最后得出 x1,x2,xn的系数的偏移量</span></span><br><span class="line">        weights = weights + alpha * dataMatrix.transpose() * error <span class="comment"># 矩阵乘法，最后得到回归系数</span></span><br><span class="line">    <span class="keyword">return</span> array(weights)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机梯度上升</span></span><br><span class="line"><span class="comment"># 梯度上升优化算法在每次更新数据集时都需要遍历整个数据集，计算复杂都较高</span></span><br><span class="line"><span class="comment"># 随机梯度上升一次只用一个样本点来更新回归系数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span><span class="params">(dataMatrix, classLabels)</span>:</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># n*1的矩阵</span></span><br><span class="line">    <span class="comment"># 函数ones创建一个全1的数组</span></span><br><span class="line">    weights = ones(n)   <span class="comment"># 初始化长度为n的数组，元素全部为 1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,此处求出的 h 是一个具体的数值，而不是一个矩阵</span></span><br><span class="line">        h = sigmoid(sum(dataMatrix[i]*weights))</span><br><span class="line">        <span class="comment"># print 'dataMatrix[i]===', dataMatrix[i]</span></span><br><span class="line">        <span class="comment"># 计算真实类别与预测类别之间的差值，然后按照该差值调整回归系数</span></span><br><span class="line">        error = classLabels[i] - h</span><br><span class="line">        <span class="comment"># 0.01*(1*1)*(1*n)</span></span><br><span class="line">        <span class="keyword">print</span> weights, <span class="string">"*"</span>*<span class="number">10</span> , dataMatrix[i], <span class="string">"*"</span>*<span class="number">10</span> , error</span><br><span class="line">        weights = weights + alpha * error * dataMatrix[i]</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机梯度上升算法（随机化）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataMatrix, classLabels, numIter=<span class="number">150</span>)</span>:</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    weights = ones(n)   <span class="comment"># 创建与列数相同的矩阵的系数矩阵，所有的元素都是1</span></span><br><span class="line">    <span class="comment"># 随机梯度, 循环150,观察是否收敛</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">        <span class="comment"># [0, 1, 2 .. m-1]</span></span><br><span class="line">        dataIndex = range(m)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># i和j的不断增大，导致alpha的值不断减少，但是不为0</span></span><br><span class="line">            alpha = <span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.0001</span>    <span class="comment"># alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001</span></span><br><span class="line">            <span class="comment"># 随机产生一个 0～len()之间的一个值</span></span><br><span class="line">            <span class="comment"># random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。</span></span><br><span class="line">            randIndex = int(random.uniform(<span class="number">0</span>,len(dataIndex)))</span><br><span class="line">            <span class="comment"># sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn</span></span><br><span class="line">            h = sigmoid(sum(dataMatrix[dataIndex[randIndex]]*weights))</span><br><span class="line">            error = classLabels[dataIndex[randIndex]] - h</span><br><span class="line">            <span class="comment"># print weights, '__h=%s' % h, '__'*20, alpha, '__'*20, error, '__'*20, dataMatrix[randIndex]</span></span><br><span class="line">            weights = weights + alpha * error * dataMatrix[dataIndex[randIndex]]</span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长的参数来得到更好的回归系数</p>
</blockquote>
<p>Logistic 回归分类函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分类函数，根据回归系数和特征向量来计算 Sigmoid的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyVector</span><span class="params">(inX, weights)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Desc: </span></span><br><span class="line"><span class="string">        最终的分类函数，根据回归系数和特征向量来计算 Sigmoid 的值，大于0.5函数返回1，否则返回0</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        inX -- 特征向量，features</span></span><br><span class="line"><span class="string">        weights -- 根据梯度下降/随机梯度下降 计算得到的回归系数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        如果 prob 计算大于 0.5 函数返回 1</span></span><br><span class="line"><span class="string">        否则返回 0</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    prob = sigmoid(sum(inX*weights))</span><br><span class="line">    <span class="keyword">if</span> prob &gt; <span class="number">0.5</span>: <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开测试集和训练集,并对数据进行格式化处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colicTest</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        打开测试集和训练集，并对数据进行格式化处理</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        errorRate -- 分类错误率</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    frTrain = open(<span class="string">'data/5.Logistic/horseColicTraining.txt'</span>)</span><br><span class="line">    frTest = open(<span class="string">'data/5.Logistic/horseColicTest.txt'</span>)</span><br><span class="line">    trainingSet = []</span><br><span class="line">    trainingLabels = []</span><br><span class="line">    <span class="comment"># 解析训练数据集中的数据特征和Labels</span></span><br><span class="line">    <span class="comment"># trainingSet 中存储训练数据集的特征，trainingLabels 存储训练数据集的样本对应的分类标签</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTrain.readlines():</span><br><span class="line">        currLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        lineArr = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        trainingSet.append(lineArr)</span><br><span class="line">        trainingLabels.append(float(currLine[<span class="number">21</span>]))</span><br><span class="line">    <span class="comment"># 使用 改进后的 随机梯度下降算法 求得在此数据集上的最佳回归系数 trainWeights</span></span><br><span class="line">    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, <span class="number">500</span>)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    numTestVec = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 读取 测试数据集 进行测试，计算分类错误的样本条数和最终的错误率</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTest.readlines():</span><br><span class="line">        numTestVec += <span class="number">1.0</span></span><br><span class="line">        currLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        lineArr = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        <span class="keyword">if</span> int(classifyVector(array(lineArr), trainWeights)) != int(currLine[<span class="number">21</span>]):</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    errorRate = (float(errorCount) / numTestVec)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"the error rate of this test is: %f"</span> % errorRate</span><br><span class="line">    <span class="keyword">return</span> errorRate</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 colicTest() 10次并求结果的平均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiTest</span><span class="params">()</span>:</span></span><br><span class="line">    numTests = <span class="number">10</span></span><br><span class="line">    errorSum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(numTests):</span><br><span class="line">        errorSum += colicTest()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"after %d iterations the average error rate is: %f"</span> % (numTests, errorSum/float(numTests))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法: 实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事，这可以作为留给大家的一道习题</p>
</blockquote>
<h1 id="额外内容-可选读"><a href="#额外内容-可选读" class="headerlink" title="额外内容(可选读)"></a>额外内容(可选读)</h1><p>在上文中，当Sigmoid函数大于 0.5 的数据被分入 1 类，小于 0.5 即被归入 0 类。其实0.5也是可以改动的。 比如大于 0.9 的数据被分入 1 类，小于 0.9 即被归入 0 类。</p>
<h2 id="Logistic回归-和-最大熵模型"><a href="#Logistic回归-和-最大熵模型" class="headerlink" title="Logistic回归 和 最大熵模型"></a>Logistic回归 和 最大熵模型</h2><p>Logistic回归和最大熵模型 都属于对数线性模型 （log linear model）。 当类标签（class label）只有两个的时候，最大熵模型就是 logistic 回归模型。 学习它们的模型一般采用极大似然估计或者正则化的极大似然估计。Logistic 回归和最大熵模型学习可以形式化为无约束最优化问题。（关于最大熵模型，可以阅读《统计学习方法》 第六章。）</p>
<h2 id="其他算法"><a href="#其他算法" class="headerlink" title="其他算法"></a>其他算法</h2><p>除了梯度下降，随机梯度下降，还有Conjugate Gradient，BFGS，L-BFGS，他们不需要指定alpha值（步长），而且比梯度下降更快，在现实中应用的也比较多。 当然这些算法相比随机梯度要复杂。</p>
<p>综上这些算法都有一个共通的缺点就是他们都是不断去逼近真实值，永远只是一个真实值的近似值而已。</p>
<h2 id="多标签分类"><a href="#多标签分类" class="headerlink" title="多标签分类"></a>多标签分类</h2><p>逻辑回归也可以用作于多标签分类。 思路如下：</p>
<p>假设我们标签A中有a0,a1,a2….an个标签，对于每个标签 ai (ai 是标签A之一)，我们训练一个逻辑回归分类器。</p>
<p>即，训练该标签的逻辑回归分类器的时候，将ai看作一类标签，非ai的所有标签看作一类标签。那么相当于整个数据集里面只有两类标签：ai 和其他。</p>
<p>剩下步骤就跟我们训练正常的逻辑回归分类器一样了。</p>
<p>测试数据的时候，将查询点套用在每个逻辑回归分类器中的Sigmoid 函数，取值最高的对应标签为查询点的标签。</p>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第4章 朴素贝叶斯</title>
    <url>/2020/06/01/4.%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
    <content><![CDATA[<h2 id="朴素贝叶斯-概述"><a href="#朴素贝叶斯-概述" class="headerlink" title="朴素贝叶斯 概述"></a>朴素贝叶斯 概述</h2><p><code>贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种: 朴素贝叶斯分类。</code></p>
<a id="more"></a>
<h2 id="贝叶斯理论-amp-条件概率"><a href="#贝叶斯理论-amp-条件概率" class="headerlink" title="贝叶斯理论 &amp; 条件概率"></a>贝叶斯理论 &amp; 条件概率</h2><h3 id="贝叶斯理论"><a href="#贝叶斯理论" class="headerlink" title="贝叶斯理论"></a>贝叶斯理论</h3><p>我们现在有一个数据集，它由两类数据组成，数据分布如下图所示：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83.png" alt="朴素贝叶斯示例数据分布" title="参数已知的概率分布"></p>
<p>我们现在用 p1(x,y) 表示数据点 (x,y) 属于类别 1（图中用圆点表示的类别）的概率，用 p2(x,y) 表示数据点 (x,y) 属于类别 2（图中三角形表示的类别）的概率，那么对于一个新数据点 (x,y)，可以用下面的规则来判断它的类别：</p>
<ul>
<li>如果 p1(x,y) &gt; p2(x,y) ，那么类别为1</li>
<li>如果 p2(x,y) &gt; p1(x,y) ，那么类别为2</li>
</ul>
<p>也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。</p>
<h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>如果你对 p(x,y|c1) 符号很熟悉，那么可以跳过本小节。</p>
<p>有一个装了 7 块石头的罐子，其中 3 块是白色的，4 块是黑色的。如果从罐子中随机取出一块石头，那么是白色石头的可能性是多少？由于取石头有 7 种可能，其中 3 种为白色，所以取出白色石头的概率为 3/7 。那么取到黑色石头的概率又是多少呢？很显然，是 4/7 。我们使用 P(white) 来表示取到白色石头的概率，其概率值可以通过白色石头数目除以总的石头数目来得到。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/NB_2.png" alt="包含 7 块石头的集合"></p>
<p>如果这 7 块石头如下图所示，放在两个桶中，那么上述概率应该如何计算？</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/NB_3.png" alt="7块石头放入两个桶中"></p>
<p>计算 P(white) 或者 P(black) ，如果事先我们知道石头所在桶的信息是会改变结果的。这就是所谓的条件概率（conditional probablity）。假定计算的是从 B 桶取到白色石头的概率，这个概率可以记作 P(white|bucketB) ，我们称之为“在已知石头出自 B 桶的条件下，取出白色石头的概率”。很容易得到，P(white|bucketA) 值为 2/4 ，P(white|bucketB) 的值为 1/3 。</p>
<p>条件概率的计算公式如下：</p>
<p>P(white|bucketB) = P(white and bucketB) / P(bucketB)</p>
<p>首先，我们用 B 桶中白色石头的个数除以两个桶中总的石头数，得到 P(white and bucketB) = 1/7 .其次，由于 B 桶中有 3 块石头，而总石头数为 7 ，于是 P(bucketB) 就等于 3/7 。于是又 P(white|bucketB) = P(white and bucketB) / P(bucketB) = (1/7) / (3/7) = 1/3 。</p>
<p>另外一种有效计算条件概率的方法称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知 P(x|c)，要求 P(c|x)，那么可以使用下面的计算方法：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/NB_4.png" alt="计算p(c|x)的方法"></p>
<h3 id="使用条件概率来分类"><a href="#使用条件概率来分类" class="headerlink" title="使用条件概率来分类"></a>使用条件概率来分类</h3><p>上面我们提到贝叶斯决策理论要求计算两个概率 p1(x, y) 和 p2(x, y):</p>
<ul>
<li>如果 p1(x, y) &gt; p2(x, y), 那么属于类别 1;</li>
<li>如果 p2(x, y) &gt; p1(X, y), 那么属于类别 2.</li>
</ul>
<p>这并不是贝叶斯决策理论的所有内容。使用 p1() 和 p2() 只是为了尽可能简化描述，而真正需要计算和比较的是 p(c1|x, y) 和 p(c2|x, y) .这些符号所代表的具体意义是: 给定某个由 x、y 表示的数据点，那么该数据点来自类别 c1 的概率是多少？数据点来自类别 c2 的概率又是多少？注意这些概率与概率 p(x, y|c1) 并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/NB_5.png" alt="应用贝叶斯准则"></p>
<p>使用上面这些定义，可以定义贝叶斯分类准则为:</p>
<ul>
<li>如果 P(c1|x, y) &gt; P(c2|x, y), 那么属于类别 c1;</li>
<li>如果 P(c2|x, y) &gt; P(c1|x, y), 那么属于类别 c2.</li>
</ul>
<p>在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。</p>
<p>我们假设特征之间  <strong>相互独立</strong> 。所谓 <b>独立(independence)</b> 指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，比如说，“我们”中的“我”和“们”出现的概率与这两个字相邻没有任何关系。这个假设正是朴素贝叶斯分类器中 朴素(naive) 一词的含义。朴素贝叶斯分类器中的另一个假设是，<b>每个特征同等重要</b>。</p>
<p><b>Note:</b> 朴素贝叶斯分类器通常有两种实现方式: 一种基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。</p>
<h2 id="朴素贝叶斯-场景"><a href="#朴素贝叶斯-场景" class="headerlink" title="朴素贝叶斯 场景"></a>朴素贝叶斯 场景</h2><p>机器学习的一个重要应用就是文档的自动分类。</p>
<p>在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。</p>
<p>朴素贝叶斯是上面介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。下面我们会进行一些朴素贝叶斯分类的实践项目。</p>
<h2 id="朴素贝叶斯-原理"><a href="#朴素贝叶斯-原理" class="headerlink" title="朴素贝叶斯 原理"></a>朴素贝叶斯 原理</h2><h3 id="朴素贝叶斯-工作原理"><a href="#朴素贝叶斯-工作原理" class="headerlink" title="朴素贝叶斯 工作原理"></a>朴素贝叶斯 工作原理</h3><figure class="highlight 1c"><table><tr><td class="code"><pre><span class="line">提取所有文档中的词条并进行去重</span><br><span class="line">获取文档的所有类别</span><br><span class="line">计算每个类别中的文档数目</span><br><span class="line">对每篇训练文档: </span><br><span class="line">    对每个类别: </span><br><span class="line">        如果词条出现在文档中--&gt;增加该词条的计数值（for循环或者矩阵相加）</span><br><span class="line">        增加所有词条的计数值（此类别下词条总数）</span><br><span class="line">对每个类别: </span><br><span class="line">    对每个词条: </span><br><span class="line">        将该词条的数目除以总词条数目得到的条件概率（P(词条<span class="string">|类别)）</span></span><br><span class="line">返回该文档属于每个类别的条件概率（P(类别<span class="string">|文档的所有词条)）</span></span><br></pre></td></tr></table></figure>

<h3 id="朴素贝叶斯-开发流程"><a href="#朴素贝叶斯-开发流程" class="headerlink" title="朴素贝叶斯 开发流程"></a>朴素贝叶斯 开发流程</h3><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 可以使用任何方法。</span></span><br><span class="line"><span class="section">准备数据: 需要数值型或者布尔型数据。</span></span><br><span class="line"><span class="section">分析数据: 有大量特征时，绘制特征作用不大，此时使用直方图效果更好。</span></span><br><span class="line"><span class="section">训练算法: 计算不同的独立特征的条件概率。</span></span><br><span class="line"><span class="section">测试算法: 计算错误率。</span></span><br><span class="line"><span class="section">使用算法: 一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。</span></span><br></pre></td></tr></table></figure>

<h3 id="朴素贝叶斯-算法特点"><a href="#朴素贝叶斯-算法特点" class="headerlink" title="朴素贝叶斯 算法特点"></a>朴素贝叶斯 算法特点</h3><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">优点: 在数据较少的情况下仍然有效，可以处理多类别问题。</span></span><br><span class="line"><span class="section">缺点: 对于输入数据的准备方式较为敏感。</span></span><br><span class="line"><span class="section">适用数据类型: 标称型数据。</span></span><br></pre></td></tr></table></figure>

<h2 id="朴素贝叶斯-项目案例"><a href="#朴素贝叶斯-项目案例" class="headerlink" title="朴素贝叶斯 项目案例"></a>朴素贝叶斯 项目案例</h2><h3 id="项目案例1-屏蔽社区留言板的侮辱性言论"><a href="#项目案例1-屏蔽社区留言板的侮辱性言论" class="headerlink" title="项目案例1: 屏蔽社区留言板的侮辱性言论"></a>项目案例1: 屏蔽社区留言板的侮辱性言论</h3><p><a href="/src/py2.x/ml/4.NaiveBayes/bayes.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py</a></p>
<h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p>构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此问题建立两个类别: 侮辱类和非侮辱类，使用 1 和 0 分别表示。</p>
<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 可以使用任何方法</span></span><br><span class="line"><span class="section">准备数据: 从文本中构建词向量</span></span><br><span class="line"><span class="section">分析数据: 检查词条确保解析的正确性</span></span><br><span class="line"><span class="section">训练算法: 从词向量计算概率</span></span><br><span class="line"><span class="section">测试算法: 根据现实情况修改分类器</span></span><br><span class="line"><span class="section">使用算法: 对社区留言板言论进行分类</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 可以使用任何方法</p>
</blockquote>
<p>本例是我们自己构造的词表:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    创建数据集</span></span><br><span class="line"><span class="string">    :return: 单词列表postingList, 所属类别classVec</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    postingList = [[<span class="string">'my'</span>, <span class="string">'dog'</span>, <span class="string">'has'</span>, <span class="string">'flea'</span>, <span class="string">'problems'</span>, <span class="string">'help'</span>, <span class="string">'please'</span>], <span class="comment">#[0,0,1,1,1......]</span></span><br><span class="line">                   [<span class="string">'maybe'</span>, <span class="string">'not'</span>, <span class="string">'take'</span>, <span class="string">'him'</span>, <span class="string">'to'</span>, <span class="string">'dog'</span>, <span class="string">'park'</span>, <span class="string">'stupid'</span>],</span><br><span class="line">                   [<span class="string">'my'</span>, <span class="string">'dalmation'</span>, <span class="string">'is'</span>, <span class="string">'so'</span>, <span class="string">'cute'</span>, <span class="string">'I'</span>, <span class="string">'love'</span>, <span class="string">'him'</span>],</span><br><span class="line">                   [<span class="string">'stop'</span>, <span class="string">'posting'</span>, <span class="string">'stupid'</span>, <span class="string">'worthless'</span>, <span class="string">'garbage'</span>],</span><br><span class="line">                   [<span class="string">'mr'</span>, <span class="string">'licks'</span>, <span class="string">'ate'</span>, <span class="string">'my'</span>, <span class="string">'steak'</span>, <span class="string">'how'</span>, <span class="string">'to'</span>, <span class="string">'stop'</span>, <span class="string">'him'</span>],</span><br><span class="line">                   [<span class="string">'quit'</span>, <span class="string">'buying'</span>, <span class="string">'worthless'</span>, <span class="string">'dog'</span>, <span class="string">'food'</span>, <span class="string">'stupid'</span>]]</span><br><span class="line">    classVec = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 1 is abusive, 0 not</span></span><br><span class="line">    <span class="keyword">return</span> postingList, classVec</span><br></pre></td></tr></table></figure>

<blockquote>
<p>准备数据: 从文本中构建词向量</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    获取所有单词的集合</span></span><br><span class="line"><span class="string">    :param dataSet: 数据集</span></span><br><span class="line"><span class="string">    :return: 所有单词的集合(即不含重复元素的单词列表)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    vocabSet = set([])  <span class="comment"># create empty set</span></span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 操作符 | 用于求两个集合的并集</span></span><br><span class="line">        vocabSet = vocabSet | set(document)  <span class="comment"># union of the two sets</span></span><br><span class="line">    <span class="keyword">return</span> list(vocabSet)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2Vec</span><span class="params">(vocabList, inputSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    遍历查看该单词是否出现，出现该单词则将该单词置1</span></span><br><span class="line"><span class="string">    :param vocabList: 所有单词集合列表</span></span><br><span class="line"><span class="string">    :param inputSet: 输入数据集</span></span><br><span class="line"><span class="string">    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 创建一个和词汇表等长的向量，并将其元素都设置为0</span></span><br><span class="line">    returnVec = [<span class="number">0</span>] * len(vocabList)<span class="comment"># [0,0......]</span></span><br><span class="line">    <span class="comment"># 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"the word: %s is not in my Vocabulary!"</span> % word</span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<p>检查函数执行情况，检查词表，不出现重复单词，需要的话，可以对其进行排序。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>listOPosts, listClasses = bayes.loadDataSet()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>myVocabList = bayes.createVocabList(listOPosts)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>myVocabList</span><br><span class="line">[<span class="string">'cute'</span>, <span class="string">'love'</span>, <span class="string">'help'</span>, <span class="string">'garbage'</span>, <span class="string">'quit'</span>, <span class="string">'I'</span>, <span class="string">'problems'</span>, <span class="string">'is'</span>, <span class="string">'park'</span>, </span><br><span class="line"><span class="string">'stop'</span>, <span class="string">'flea'</span>, <span class="string">'dalmation'</span>, <span class="string">'licks'</span>, <span class="string">'food'</span>, <span class="string">'not'</span>, <span class="string">'him'</span>, <span class="string">'buying'</span>, <span class="string">'posting'</span>, <span class="string">'has'</span>, <span class="string">'worthless'</span>, <span class="string">'ate'</span>, <span class="string">'to'</span>, <span class="string">'maybe'</span>, <span class="string">'please'</span>, <span class="string">'dog'</span>, <span class="string">'how'</span>, </span><br><span class="line"><span class="string">'stupid'</span>, <span class="string">'so'</span>, <span class="string">'take'</span>, <span class="string">'mr'</span>, <span class="string">'steak'</span>, <span class="string">'my'</span>]</span><br></pre></td></tr></table></figure>

<p>检查函数有效性。例如：myVocabList 中索引为 2 的元素是什么单词？应该是是 help 。该单词在第一篇文档中出现了，现在检查一下看看它是否出现在第四篇文档中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>bayes.setOfWords2Vec(myVocabList, listOPosts[<span class="number">0</span>])</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bayes.setOfWords2Vec(myVocabList, listOPosts[<span class="number">3</span>])</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>训练算法: 从词向量计算概率</p>
</blockquote>
<p>现在已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。接下来我们重写贝叶斯准则，将之前的 x, y 替换为 <b>w</b>. 粗体的 <b>w</b> 表示这是一个向量，即它由多个值组成。在这个例子中，数值个数与词汇表中的词个数相同。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/NB_6.png" alt="重写贝叶斯准则"></p>
<p>我们使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。</p>
<p>问: 上述代码实现中，为什么没有计算P(w)？</p>
<p>答：根据上述公式可知，我们右边的式子等同于左边的式子，由于对于每个ci，P(w)是固定的。并且我们只需要比较左边式子值的大小来决策分类，那么我们就可以简化为通过比较右边分子值得大小来做决策分类。</p>
<p>首先可以通过类别 i (侮辱性留言或者非侮辱性留言)中的文档数除以总的文档数来计算概率 p(ci) 。接下来计算 p(<b>w</b> | ci) ，这里就要用到朴素贝叶斯假设。如果将 w 展开为一个个独立特征，那么就可以将上述概率写作 p(w0, w1, w2…wn | ci) 。这里假设所有词都互相独立，该假设也称作条件独立性假设（例如 A 和 B 两个人抛骰子，概率是互不影响的，也就是相互独立的，A 抛 2点的同时 B 抛 3 点的概率就是 1/6 * 1/6），它意味着可以使用 p(w0 | ci)p(w1 | ci)p(w2 | ci)…p(wn | ci) 来计算上述概率，这样就极大地简化了计算的过程。</p>
<p>朴素贝叶斯分类器训练函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练数据原版</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵 [[1,0,1,1,1....],[],[]...]</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 文件数</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    <span class="comment"># 单词数</span></span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率，即trainCategory中所有的1的个数，</span></span><br><span class="line">    <span class="comment"># 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = sum(trainCategory) / float(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    p0Num = zeros(numWords) <span class="comment"># [0,0,0,.....]</span></span><br><span class="line">    p1Num = zeros(numWords) <span class="comment"># [0,0,0,.....]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数</span></span><br><span class="line">    p0Denom = <span class="number">0.0</span></span><br><span class="line">    p1Denom = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="comment"># 是否是侮辱性文件</span></span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果是侮辱性文件，对侮辱性文件的向量进行加和</span></span><br><span class="line">            p1Num += trainMatrix[i] <span class="comment">#[0,1,1,....] + [0,1,1,....]-&gt;[0,2,2,...]</span></span><br><span class="line">            <span class="comment"># 对向量中的所有元素进行求和，也就是计算所有侮辱性文件中出现的单词总数</span></span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表</span></span><br><span class="line">    <span class="comment"># 即 在1类别下，每个单词出现的概率</span></span><br><span class="line">    p1Vect = p1Num / p1Denom<span class="comment"># [1,2,3,5]/90-&gt;[1/90,...]</span></span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表</span></span><br><span class="line">    <span class="comment"># 即 在0类别下，每个单词出现的概率</span></span><br><span class="line">    p0Vect = p0Num / p0Denom</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 根据现实情况修改分类器</p>
</blockquote>
<p>在利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算 p(w0|1) * p(w1|1) * p(w2|1)。如果其中一个概率值为 0，那么最后的乘积也为 0。为降低这种影响，可以将所有词的出现数初始化为 1，并将分母初始化为 2 （取1 或 2 的目的主要是为了保证分子和分母不为0，大家可以根据业务需求进行更改）。</p>
<p>另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积 p(w0|ci) * p(w1|ci) * p(w2|ci)… p(wn|ci) 时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（用 Python 尝试相乘许多很小的数，最后四舍五入后会得到 0）。一种解决办法是对乘积取自然对数。在代数中有 ln(a * b) = ln(a) + ln(b), 于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。</p>
<p>下图给出了函数 f(x) 与 ln(f(x)) 的曲线。可以看出，它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/NB_7.png" alt="函数图像"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练数据优化版本</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 总文件数</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    <span class="comment"># 总单词数</span></span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = sum(trainCategory) / float(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    <span class="comment"># p0Num 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Num 侮辱的统计</span></span><br><span class="line">    p0Num = ones(numWords)<span class="comment">#[0,0......]-&gt;[1,1,1,1,1.....]</span></span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span></span><br><span class="line">    <span class="comment"># p0Denom 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Denom 侮辱的统计</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 累加辱骂词的频次</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 对每篇文章的辱骂的频次 进行统计汇总</span></span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line">    p1Vect = log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line">    p0Vect = log(p0Num / p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></figure>


<blockquote>
<p>使用算法: 对社区留言板言论进行分类</p>
</blockquote>
<p>朴素贝叶斯分类函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span><span class="params">(vec2Classify, p0Vec, p1Vec, pClass1)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    使用算法：</span></span><br><span class="line"><span class="string">        # 将乘法转换为加法</span></span><br><span class="line"><span class="string">        乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)</span></span><br><span class="line"><span class="string">        加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -&gt; log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))</span></span><br><span class="line"><span class="string">    :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量</span></span><br><span class="line"><span class="string">    :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line"><span class="string">    :param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line"><span class="string">    :param pClass1: 类别1，侮辱性文件的出现概率</span></span><br><span class="line"><span class="string">    :return: 类别1 or 0</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))</span></span><br><span class="line">    <span class="comment"># 大家可能会发现，上面的计算公式，没有除以贝叶斯准则的公式的分母，也就是 P(w) （P(w) 指的是此文档在所有的文档中出现的概率）就进行概率大小的比较了，</span></span><br><span class="line">    <span class="comment"># 因为 P(w) 针对的是包含侮辱和非侮辱的全部文档，所以 P(w) 是相同的。</span></span><br><span class="line">    <span class="comment"># 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。</span></span><br><span class="line">    <span class="comment"># 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来</span></span><br><span class="line">    p1 = sum(vec2Classify * p1Vec) + log(pClass1) <span class="comment"># P(w|c1) * P(c1) ，即贝叶斯准则的分子</span></span><br><span class="line">    p0 = sum(vec2Classify * p0Vec) + log(<span class="number">1.0</span> - pClass1) <span class="comment"># P(w|c0) * P(c0) ，即贝叶斯准则的分子·</span></span><br><span class="line">    <span class="keyword">if</span> p1 &gt; p0:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testingNB</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    测试朴素贝叶斯算法</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 1. 加载数据集</span></span><br><span class="line">    listOPosts, listClasses = loadDataSet()</span><br><span class="line">    <span class="comment"># 2. 创建单词集合</span></span><br><span class="line">    myVocabList = createVocabList(listOPosts)</span><br><span class="line">    <span class="comment"># 3. 计算单词是否出现并创建数据矩阵</span></span><br><span class="line">    trainMat = []</span><br><span class="line">    <span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOPosts:</span><br><span class="line">        <span class="comment"># 返回m*len(myVocabList)的矩阵， 记录的都是0，1信息</span></span><br><span class="line">        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))</span><br><span class="line">    <span class="comment"># 4. 训练数据</span></span><br><span class="line">    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))</span><br><span class="line">    <span class="comment"># 5. 测试数据</span></span><br><span class="line">    testEntry = [<span class="string">'love'</span>, <span class="string">'my'</span>, <span class="string">'dalmation'</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="keyword">print</span> testEntry, <span class="string">'classified as: '</span>, classifyNB(thisDoc, p0V, p1V, pAb)</span><br><span class="line">    testEntry = [<span class="string">'stupid'</span>, <span class="string">'garbage'</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="keyword">print</span> testEntry, <span class="string">'classified as: '</span>, classifyNB(thisDoc, p0V, p1V, pAb)</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 项目案例2: 使用朴素贝叶斯过滤垃圾邮件</span></span><br><span class="line"></span><br><span class="line">[完整代码地址](/src/py2.x/ml/<span class="number">4.</span>NaiveBayes/bayes.py): &lt;https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/<span class="number">4.</span>NaiveBayes/bayes.py&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 项目概述</span></span><br><span class="line"></span><br><span class="line">完成朴素贝叶斯的一个最著名的应用: 电子邮件垃圾过滤。</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 开发流程</span></span><br><span class="line"></span><br><span class="line">使用朴素贝叶斯对电子邮件进行分类</span><br></pre></td></tr></table></figure>
<p>收集数据: 提供文本文件<br>准备数据: 将文本文件解析成词条向量<br>分析数据: 检查词条确保解析的正确性<br>训练算法: 使用我们之前建立的 trainNB() 函数<br>测试算法: 使用朴素贝叶斯进行交叉验证<br>使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="quote">&gt; 收集数据: 提供文本文件</span></span><br><span class="line"></span><br><span class="line">文本文件内容如下:</span><br></pre></td></tr></table></figure>
<p>Hi Peter,</p>
<p>With Jose out of town, do you want to<br>meet once in a while to keep things<br>going and do some interesting stuff?</p>
<p>Let me know<br>Eugene</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 准备数据: 将文本文件解析成词条向量</span><br><span class="line"></span><br><span class="line">使用正则表达式来切分文本</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mySent = <span class="string">'This book is the best book on Python or M.L. I have ever laid eyes upon.'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>regEx = re.compile(<span class="string">'\\W*'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>listOfTokens = regEx.split(mySent)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>listOfTokens</span><br><span class="line">[<span class="string">'This'</span>, <span class="string">'book'</span>, <span class="string">'is'</span>, <span class="string">'the'</span>, <span class="string">'best'</span>, <span class="string">'book'</span>, <span class="string">'on'</span>, <span class="string">'Python'</span>, <span class="string">'or'</span>, <span class="string">'M.L.'</span>, <span class="string">'I'</span>, <span class="string">'have'</span>, <span class="string">'ever'</span>, <span class="string">'laid'</span>, <span class="string">'eyes'</span>, <span class="string">'upon'</span>, <span class="string">''</span>]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<blockquote>
<p>训练算法: 使用我们之前建立的 trainNB0() 函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练数据优化版本</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 总文件数</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    <span class="comment"># 总单词数</span></span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = sum(trainCategory) / float(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    <span class="comment"># p0Num 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Num 侮辱的统计</span></span><br><span class="line">    p0Num = ones(numWords)<span class="comment">#[0,0......]-&gt;[1,1,1,1,1.....]</span></span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span></span><br><span class="line">    <span class="comment"># p0Denom 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Denom 侮辱的统计</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 累加辱骂词的频次</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 对每篇文章的辱骂的频次 进行统计汇总</span></span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line">    p1Vect = log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line">    p0Vect = log(p0Num / p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 使用朴素贝叶斯进行交叉验证</p>
</blockquote>
<p>文件解析及完整的垃圾邮件测试函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 切分文本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span><span class="params">(bigString)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        接收一个大字符串并将其解析为字符串列表</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        bigString -- 大字符串</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        去掉少于 2 个字符的字符串，并将所有字符串转换为小写，返回字符串列表</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="comment"># 使用正则表达式来切分句子，其中分隔符是除单词、数字外的任意字符串</span></span><br><span class="line">    listOfTokens = re.split(<span class="string">r'\W*'</span>, bigString)</span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> listOfTokens <span class="keyword">if</span> len(tok) &gt; <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spamTest</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对贝叶斯垃圾邮件分类器进行自动化处理。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        none</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        对测试集中的每封邮件进行分类，若邮件分类错误，则错误数加 1，最后返回总的错误百分比。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    docList = []</span><br><span class="line">    classList = []</span><br><span class="line">    fullText = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">26</span>):</span><br><span class="line">        <span class="comment"># 切分，解析数据，并归类为 1 类别</span></span><br><span class="line">        wordList = textParse(open(<span class="string">'data/4.NaiveBayes/email/spam/%d.txt'</span> % i).read())</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        classList.append(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 切分，解析数据，并归类为 0 类别</span></span><br><span class="line">        wordList = textParse(open(<span class="string">'data/4.NaiveBayes/email/ham/%d.txt'</span> % i).read())</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 创建词汇表    </span></span><br><span class="line">    vocabList = createVocabList(docList)</span><br><span class="line">    trainingSet = range(<span class="number">50</span>)</span><br><span class="line">    testSet = []</span><br><span class="line">    <span class="comment"># 随机取 10 个邮件用来测试</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="comment"># random.uniform(x, y) 随机生成一个范围为 x ~ y 的实数</span></span><br><span class="line">        randIndex = int(random.uniform(<span class="number">0</span>, len(trainingSet)))</span><br><span class="line">        testSet.append(trainingSet[randIndex])</span><br><span class="line">        <span class="keyword">del</span>(trainingSet[randIndex])</span><br><span class="line">    trainMat = []</span><br><span class="line">    trainClasses = []</span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> trainingSet:</span><br><span class="line">        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))</span><br><span class="line">        trainClasses.append(classList[docIndex])</span><br><span class="line">    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testSet:</span><br><span class="line">        wordVector = setOfWords2Vec(vocabList, docList[docIndex])</span><br><span class="line">        <span class="keyword">if</span> classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'the errorCount is: '</span>, errorCount</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'the testSet length is :'</span>, len(testSet)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'the error rate is :'</span>, float(errorCount)/len(testSet)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上</p>
</blockquote>
<h3 id="项目案例3-使用朴素贝叶斯分类器从个人广告中获取区域倾向"><a href="#项目案例3-使用朴素贝叶斯分类器从个人广告中获取区域倾向" class="headerlink" title="项目案例3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向"></a>项目案例3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向</h3><p><a href="/src/py2.x/ml/4.NaiveBayes/bayes.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py</a></p>
<h4 id="项目概述-1"><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4><p>广告商往往想知道关于一个人的一些特定人口统计信息，以便能更好地定向推销广告。</p>
<p>我们将分别从美国的两个城市中选取一些人，通过分析这些人发布的信息，来比较这两个城市的人们在广告用词上是否不同。如果结论确实不同，那么他们各自常用的词是哪些，从人们的用词当中，我们能否对不同城市的人所关心的内容有所了解。</p>
<h4 id="开发流程-1"><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口</span></span><br><span class="line"><span class="section">准备数据: 将文本文件解析成词条向量</span></span><br><span class="line"><span class="section">分析数据: 检查词条确保解析的正确性</span></span><br><span class="line"><span class="section">训练算法: 使用我们之前建立的 trainNB0() 函数</span></span><br><span class="line"><span class="section">测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果</span></span><br><span class="line"><span class="section">使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口</p>
</blockquote>
<p>也就是导入 RSS 源，我们使用 python 下载文本，在<a href="http://code.google.com/p/feedparser/" target="_blank" rel="noopener">http://code.google.com/p/feedparser/</a> 下浏览相关文档，安装 feedparse，首先解压下载的包，并将当前目录切换到解压文件所在的文件夹，然后在 python 提示符下输入：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>python setup.py install</span><br></pre></td></tr></table></figure>

<blockquote>
<p>准备数据: 将文本文件解析成词条向量</p>
</blockquote>
<p>文档词袋模型</p>
<p>我们将每个词的出现与否作为一个特征，这可以被描述为 <b>词集模型(set-of-words model)</b>。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为 <b>词袋模型(bag-of-words model)</b>。在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数 setOfWords2Vec() 稍加修改，修改后的函数为 bagOfWords2Vec() 。</p>
<p>如下给出了基于词袋模型的朴素贝叶斯代码。它与函数 setOfWords2Vec() 几乎完全相同，唯一不同的是每当遇到一个单词时，它会增加词向量中的对应值，而不只是将对应的数值设为 1 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2VecMN</span><span class="params">(vocaList, inputSet)</span>:</span></span><br><span class="line">    returnVec = [<span class="number">0</span>] * len(vocabList)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocaList:</span><br><span class="line">            returnVec[vocabList.index(word)] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建一个包含在所有文档中出现的不重复词的列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    vocabSet=set([])    <span class="comment">#创建一个空集</span></span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</span><br><span class="line">        vocabSet=vocabSet|set(document)   <span class="comment">#创建两个集合的并集</span></span><br><span class="line">    <span class="keyword">return</span> list(vocabSet)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2VecMN</span><span class="params">(vocabList,inputSet)</span>:</span></span><br><span class="line">    returnVec=[<span class="number">0</span>]*len(vocabList)  <span class="comment">#创建一个其中所含元素都为0的向量</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">                returnVec[vocabList.index(word)]+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br><span class="line"></span><br><span class="line"><span class="comment">#文件解析</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span><span class="params">(bigString)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    listOfTokens=re.split(<span class="string">r'\W*'</span>,bigString)</span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> listOfTokens <span class="keyword">if</span> len(tok)&gt;<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<blockquote>
<p>训练算法: 使用我们之前建立的 trainNB0() 函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练数据优化版本</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 总文件数</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    <span class="comment"># 总单词数</span></span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = sum(trainCategory) / float(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    <span class="comment"># p0Num 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Num 侮辱的统计 </span></span><br><span class="line">    <span class="comment"># 避免单词列表中的任何一个单词为0，而导致最后的乘积为0，所以将每个单词的出现次数初始化为 1</span></span><br><span class="line">    p0Num = ones(numWords)<span class="comment">#[0,0......]-&gt;[1,1,1,1,1.....]</span></span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span></span><br><span class="line">    <span class="comment"># p0Denom 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Denom 侮辱的统计</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 累加辱骂词的频次</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 对每篇文章的辱骂的频次 进行统计汇总</span></span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line">    p1Vect = log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line">    p0Vect = log(p0Num / p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#RSS源分类器及高频词去除函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcMostFreq</span><span class="params">(vocabList,fullText)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> operator</span><br><span class="line">    freqDict=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> vocabList:  <span class="comment">#遍历词汇表中的每个词</span></span><br><span class="line">        freqDict[token]=fullText.count(token)  <span class="comment">#统计每个词在文本中出现的次数</span></span><br><span class="line">    sortedFreq=sorted(freqDict.iteritems(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)  <span class="comment">#根据每个词出现的次数从高到底对字典进行排序</span></span><br><span class="line">    <span class="keyword">return</span> sortedFreq[:<span class="number">30</span>]   <span class="comment">#返回出现次数最高的30个单词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">localWords</span><span class="params">(feed1,feed0)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> feedparser</span><br><span class="line">    docList=[];classList=[];fullText=[]</span><br><span class="line">    minLen=min(len(feed1[<span class="string">'entries'</span>]),len(feed0[<span class="string">'entries'</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(minLen):</span><br><span class="line">        wordList=textParse(feed1[<span class="string">'entries'</span>][i][<span class="string">'summary'</span>])   <span class="comment">#每次访问一条RSS源</span></span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">1</span>)</span><br><span class="line">        wordList=textParse(feed0[<span class="string">'entries'</span>][i][<span class="string">'summary'</span>])</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">0</span>)</span><br><span class="line">    vocabList=createVocabList(docList)</span><br><span class="line">    top30Words=calcMostFreq(vocabList,fullText)</span><br><span class="line">    <span class="keyword">for</span> pairW <span class="keyword">in</span> top30Words:</span><br><span class="line">        <span class="keyword">if</span> pairW[<span class="number">0</span>] <span class="keyword">in</span> vocabList:vocabList.remove(pairW[<span class="number">0</span>])    <span class="comment">#去掉出现次数最高的那些词</span></span><br><span class="line">    trainingSet=range(<span class="number">2</span>*minLen);testSet=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        randIndex=int(random.uniform(<span class="number">0</span>,len(trainingSet)))</span><br><span class="line">        testSet.append(trainingSet[randIndex])</span><br><span class="line">        <span class="keyword">del</span>(trainingSet[randIndex])</span><br><span class="line">    trainMat=[];trainClasses=[]</span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> trainingSet:</span><br><span class="line">        trainMat.append(bagOfWords2VecMN(vocabList,docList[docIndex]))</span><br><span class="line">        trainClasses.append(classList[docIndex])</span><br><span class="line">    p0V,p1V,pSpam=trainNBO(array(trainMat),array(trainClasses))</span><br><span class="line">    errorCount=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testSet:</span><br><span class="line">        wordVector=bagOfWords2VecMN(vocabList,docList[docIndex])</span><br><span class="line">        <span class="keyword">if</span> classifyNB(array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]:</span><br><span class="line">            errorCount+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'the error rate is:'</span>,float(errorCount)/len(testSet)</span><br><span class="line">    <span class="keyword">return</span> vocabList,p0V,p1V</span><br><span class="line"></span><br><span class="line"><span class="comment">#朴素贝叶斯分类函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span><span class="params">(vec2Classify,p0Vec,p1Vec,pClass1)</span>:</span></span><br><span class="line">    p1=sum(vec2Classify*p1Vec)+log(pClass1)</span><br><span class="line">    p0=sum(vec2Classify*p0Vec)+log(<span class="number">1.0</span>-pClass1)</span><br><span class="line">    <span class="keyword">if</span> p1&gt;p0:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词</p>
</blockquote>
<p>函数 localWords() 使用了两个 RSS 源作为参数，RSS 源要在函数外导入，这样做的原因是 RSS 源会随时间而改变，重新加载 RSS 源就会得到新的数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>reload(bayes)</span><br><span class="line">&lt;module <span class="string">'bayes'</span> <span class="keyword">from</span> <span class="string">'bayes.pyc'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> feedparser</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ny=feedparser.parse(<span class="string">'http://newyork.craigslist.org/stp/index.rss'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sy=feedparser.parse(<span class="string">'http://sfbay.craigslist.org/stp/index.rss'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabList,pSF,pNY=bayes.localWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabList,pSF,pNY=bayes.localWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabList,pSF,pNY=bayes.localWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.55</span></span><br></pre></td></tr></table></figure>
<p>为了得到错误率的精确估计，应该多次进行上述实验，然后取平均值</p>
<p>接下来，我们要分析一下数据，显示地域相关的用词</p>
<p>可以先对向量pSF与pNY进行排序，然后按照顺序打印出来，将下面的代码添加到文件中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#最具表征性的词汇显示函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTopWords</span><span class="params">(ny,sf)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> operator</span><br><span class="line">    vocabList,p0V,p1V=localWords(ny,sf)</span><br><span class="line">    topNY=[];topSF=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(p0V)):</span><br><span class="line">        <span class="keyword">if</span> p0V[i]&gt;<span class="number">-6.0</span>:topSF.append((vocabList[i],p0V[i]))</span><br><span class="line">        <span class="keyword">if</span> p1V[i]&gt;<span class="number">-6.0</span>:topNY.append((vocabList[i],p1V[i]))</span><br><span class="line">    sortedSF=sorted(topSF,key=<span class="keyword">lambda</span> pair:pair[<span class="number">1</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**"</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> sortedSF:</span><br><span class="line">        <span class="keyword">print</span> item[<span class="number">0</span>]</span><br><span class="line">    sortedNY=sorted(topNY,key=<span class="keyword">lambda</span> pair:pair[<span class="number">1</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**"</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> sortedNY:</span><br><span class="line">        <span class="keyword">print</span> item[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>函数 getTopWords() 使用两个 RSS 源作为输入，然后训练并测试朴素贝叶斯分类器，返回使用的概率值。然后创建两个列表用于元组的存储，与之前返回排名最高的 X 个单词不同，这里可以返回大于某个阈值的所有词，这些元组会按照它们的条件概率进行排序。</p>
<p>保存 bayes.py 文件，在python提示符下输入：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>reload(bayes)</span><br><span class="line">&lt;module <span class="string">'bayes'</span> <span class="keyword">from</span> <span class="string">'bayes.pyc'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bayes.getTopWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.55</span></span><br><span class="line">SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**</span><br><span class="line">how</span><br><span class="line">last</span><br><span class="line">man</span><br><span class="line">...</span><br><span class="line">veteran</span><br><span class="line">still</span><br><span class="line">ends</span><br><span class="line">late</span><br><span class="line">off</span><br><span class="line">own</span><br><span class="line">know</span><br><span class="line">NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**</span><br><span class="line">someone</span><br><span class="line">meet</span><br><span class="line">...</span><br><span class="line">apparel</span><br><span class="line">recalled</span><br><span class="line">starting</span><br><span class="line">strings</span><br></pre></td></tr></table></figure>

<p>当注释掉用于移除高频词的三行代码，然后比较注释前后的分类性能，去掉这几行代码之后，错误率为54%，，而保留这些代码得到的错误率为70%。这里观察到，这些留言中出现次数最多的前30个词涵盖了所有用词的30%，vocabList的大小约为3000个词，也就是说，词汇表中的一小部分单词却占据了所有文本用词的一大部分。产生这种现象的原因是因为语言中大部分都是冗余和结构辅助性内容。另一个常用的方法是不仅移除高频词，同时从某个预定高频词中移除结构上的辅助词，该词表称为停用词表。</p>
<p>从最后输出的单词，可以看出程序输出了大量的停用词，可以移除固定的停用词看看结果如何，这样做的话，分类错误率也会降低。</p>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第9章 树回归</title>
    <url>/2020/06/02/9.%E6%A0%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h2 id="树回归-概述"><a href="#树回归-概述" class="headerlink" title="树回归 概述"></a>树回归 概述</h2><p><code>我们本章介绍 CART(Classification And Regression Trees， 分类回归树) 的树构建算法。该算法既可以用于分类还可以用于回归。</code></p>
<h2 id="树回归-场景"><a href="#树回归-场景" class="headerlink" title="树回归 场景"></a>树回归 场景</h2><p>我们在第 8 章中介绍了线性回归的一些强大的方法，但这些方法创建的模型需要拟合所有的样本点（局部加权线性回归除外）。当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的想法就显得太难了，也略显笨拙。而且，实际生活中很多问题都是非线性的，不可能使用全局线性模型来拟合任何数据。</p>
<a id="more"></a>
<p>一种可行的方法是将数据集切分成很多份易建模的数据，然后利用我们的线性回归技术来建模。如果首次切分后仍然难以拟合线性模型就继续切分。在这种切分方式下，树回归和回归法就相当有用。</p>
<p>除了我们在 第3章 中介绍的 决策树算法，我们介绍一个新的叫做 CART(Classification And Regression Trees, 分类回归树) 的树构建算法。该算法既可以用于分类还可以用于回归。</p>
<h2 id="1、树回归-原理"><a href="#1、树回归-原理" class="headerlink" title="1、树回归 原理"></a>1、树回归 原理</h2><h3 id="1-1、树回归-原理概述"><a href="#1-1、树回归-原理概述" class="headerlink" title="1.1、树回归 原理概述"></a>1.1、树回归 原理概述</h3><p>为成功构建以分段常数为叶节点的树，需要度量出数据的一致性。第3章使用树进行分类，会在给定节点时计算数据的混乱度。那么如何计算连续型数值的混乱度呢？</p>
<p>在这里，计算连续型数值的混乱度是非常简单的。首先计算所有数据的均值，然后计算每条数据的值到均值的差值。为了对正负差值同等看待，一般使用绝对值或平方值来代替上述差值。</p>
<p>上述做法有点类似于前面介绍过的统计学中常用的方差计算。唯一不同就是，方差是平方误差的均值(均方差)，而这里需要的是平方误差的总值(总方差)。总方差可以通过均方差乘以数据集中样本点的个数来得到。</p>
<h3 id="1-2、树构建算法-比较"><a href="#1-2、树构建算法-比较" class="headerlink" title="1.2、树构建算法 比较"></a>1.2、树构建算法 比较</h3><p>我们在 第3章 中使用的树构建算法是 ID3 。ID3 的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有 4 种取值，那么数据将被切分成 4 份。一旦按照某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。另外一种方法是二元切分法，即每次把数据集切分成两份。如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。</p>
<p>除了切分过于迅速外， ID3 算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征转换成离散型，才能在 ID3 算法中使用。但这种转换过程会破坏连续型变量的内在性质。而使用二元切分法则易于对树构造过程进行调整以处理连续型特征。具体的处理方法是: 如果特征值大于给定值就走左子树，否则就走右子树。另外，二元切分法也节省了树的构建时间，但这点意义也不是特别大，因为这些树构建一般是离线完成，时间并非需要重点关注的因素。</p>
<p>CART 是十分著名且广泛记载的树构建算法，它使用二元切分来处理连续型变量。对 CART 稍作修改就可以处理回归问题。第 3 章中使用香农熵来度量集合的无组织程度。如果选用其他方法来代替香农熵，就可以使用树构建算法来完成回归。</p>
<p>回归树与分类树的思路类似，但是叶节点的数据类型不是离散型，而是连续型。</p>
<h4 id="1-2-1、附加-各常见树构造算法的划分分支方式"><a href="#1-2-1、附加-各常见树构造算法的划分分支方式" class="headerlink" title="1.2.1、附加 各常见树构造算法的划分分支方式"></a>1.2.1、附加 各常见树构造算法的划分分支方式</h4><p>还有一点要说明，构建决策树算法，常用到的是三个方法: ID3, C4.5, CART.<br>三种方法区别是划分树的分支的方式:</p>
<ol>
<li>ID3 是信息增益分支</li>
<li>C4.5 是信息增益率分支</li>
<li>CART 做分类工作时，采用 GINI 值作为节点分裂的依据；回归时，采用样本的最小方差作为节点的分裂依据。</li>
</ol>
<p>工程上总的来说: </p>
<p>CART 和 C4.5 之间主要差异在于分类结果上，CART 可以回归分析也可以分类，C4.5 只能做分类；C4.5 子节点是可以多分的，而 CART 是无数个二叉子节点；</p>
<p>以此拓展出以 CART 为基础的 “树群” Random forest ， 以 回归树 为基础的 “树群” GBDT 。</p>
<h3 id="1-3、树回归-工作原理"><a href="#1-3、树回归-工作原理" class="headerlink" title="1.3、树回归 工作原理"></a>1.3、树回归 工作原理</h3><p>1、找到数据集切分的最佳位置，函数 chooseBestSplit() 伪代码大致如下:</p>
<figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">对每个特征:</span></span><br><span class="line">    对每个特征值: </span><br><span class="line">        将数据集切分成两份（小于该特征值的数据样本放在左子树，否则放在右子树）</span><br><span class="line">        计算切分的误差</span><br><span class="line">        如果当前误差小于当前最小误差，那么将当前切分设定为最佳切分并更新最小误差</span><br><span class="line">返回最佳切分的特征和阈值</span><br></pre></td></tr></table></figure>
<p>2、树构建算法，函数 createTree() 伪代码大致如下:   </p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">找到最佳的待切分特征:</span><br><span class="line">    如果该节点不能再分，将该节点存为叶节点</span><br><span class="line">    执行二元切分</span><br><span class="line">    在右子树调用 createTree<span class="comment">()</span> 方法</span><br><span class="line">    在左子树调用 createTree<span class="comment">()</span> 方法</span><br></pre></td></tr></table></figure>

<h3 id="1-4、树回归-开发流程"><a href="#1-4、树回归-开发流程" class="headerlink" title="1.4、树回归 开发流程"></a>1.4、树回归 开发流程</h3><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">(<span class="number">1</span>) 收集数据：采用任意方法收集数据。</span><br><span class="line">(<span class="number">2</span>) 准备数据：需要数值型数据，标称型数据应该映射成二值型数据。</span><br><span class="line">(<span class="number">3</span>) 分析数据：绘出数据的二维可视化显示结果，以字典方式生成树。</span><br><span class="line">(<span class="number">4</span>) 训练算法：大部分时间都花费在叶节点树模型的构建上。</span><br><span class="line">(<span class="number">5</span>) 测试算法：使用测试数据上的R^<span class="number">2</span>值来分析模型的效果。</span><br><span class="line">(<span class="number">6</span>) 使用算法：使用训练处的树做预测，预测结果还可以用来做很多事情。</span><br></pre></td></tr></table></figure>

<h3 id="1-5、树回归-算法特点"><a href="#1-5、树回归-算法特点" class="headerlink" title="1.5、树回归 算法特点"></a>1.5、树回归 算法特点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">优点：可以对复杂和非线性的数据建模。</span><br><span class="line">缺点：结果不易理解。</span><br><span class="line">适用数据类型：数值型和标称型数据。</span><br></pre></td></tr></table></figure>

<h3 id="1-6、回归树-项目案例"><a href="#1-6、回归树-项目案例" class="headerlink" title="1.6、回归树 项目案例"></a>1.6、回归树 项目案例</h3><h4 id="1-6-1、项目概述"><a href="#1-6-1、项目概述" class="headerlink" title="1.6.1、项目概述"></a>1.6.1、项目概述</h4><p>在简单数据集上生成一棵回归树。</p>
<h4 id="1-6-2、开发流程"><a href="#1-6-2、开发流程" class="headerlink" title="1.6.2、开发流程"></a>1.6.2、开发流程</h4><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">收集数据：采用任意方法收集数据</span><br><span class="line">准备数据：需要数值型数据，标称型数据应该映射成二值型数据</span><br><span class="line">分析数据：绘出数据的二维可视化显示结果，以字典方式生成树</span><br><span class="line">训练算法：大部分时间都花费在叶节点树模型的构建上</span><br><span class="line">测试算法：使用测试数据上的R^<span class="number">2</span>值来分析模型的效果</span><br><span class="line">使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据：采用任意方法收集数据</p>
</blockquote>
<p>data1.txt 文件中存储的数据格式如下:</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">0.036098</span>	<span class="number">0.155096</span></span><br><span class="line"><span class="number">0.993349</span>	<span class="number">1.077553</span></span><br><span class="line"><span class="number">0.530897</span>	<span class="number">0.893462</span></span><br><span class="line"><span class="number">0.712386</span>	<span class="number">0.564858</span></span><br><span class="line"><span class="number">0.343554</span>	<span class="number">-0.371700</span></span><br><span class="line"><span class="number">0.098016</span>	<span class="number">-0.332760</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>准备数据：需要数值型数据，标称型数据应该映射成二值型数据</p>
</blockquote>
<blockquote>
<p>分析数据：绘出数据的二维可视化显示结果，以字典方式生成树</p>
</blockquote>
<p>基于 CART 算法构建回归树的简单数据集<br><img src="http://data.apachecn.org/img/AiLearning/ml/9.TreeRegression/RegTree_1.png" alt="基于 CART 算法构建回归树的简单数据集">  </p>
<p>用于测试回归树的分段常数数据集<br><img src="http://data.apachecn.org/img/AiLearning/ml/9.TreeRegression/RegTree_2.png" alt="用于测试回归树的分段常数数据集">  </p>
<blockquote>
<p>训练算法: 构造树的数据结构</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binSplitDataSet</span><span class="params">(dataSet, feature, value)</span>:</span></span><br><span class="line">    <span class="string">"""binSplitDataSet(将数据集，按照feature列的value进行 二元切分)</span></span><br><span class="line"><span class="string">        Description：在给定特征和特征值的情况下，该函数通过数组过滤方式将上述数据集合切分得到两个子集并返回。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMat 数据集</span></span><br><span class="line"><span class="string">        feature 待切分的特征列</span></span><br><span class="line"><span class="string">        value 特征列要比较的值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        mat0 小于等于 value 的数据集在左边</span></span><br><span class="line"><span class="string">        mat1 大于 value 的数据集在右边</span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># # 测试案例</span></span><br><span class="line">    <span class="comment"># print 'dataSet[:, feature]=', dataSet[:, feature]</span></span><br><span class="line">    <span class="comment"># print 'nonzero(dataSet[:, feature] &gt; value)[0]=', nonzero(dataSet[:, feature] &gt; value)[0]</span></span><br><span class="line">    <span class="comment"># print 'nonzero(dataSet[:, feature] &lt;= value)[0]=', nonzero(dataSet[:, feature] &lt;= value)[0]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dataSet[:, feature] 取去每一行中，第1列的值(从0开始算)</span></span><br><span class="line">    <span class="comment"># nonzero(dataSet[:, feature] &gt; value)  返回结果为true行的index下标</span></span><br><span class="line">    mat0 = dataSet[nonzero(dataSet[:, feature] &lt;= value)[<span class="number">0</span>], :]</span><br><span class="line">    mat1 = dataSet[nonzero(dataSet[:, feature] &gt; value)[<span class="number">0</span>], :]</span><br><span class="line">    <span class="keyword">return</span> mat0, mat1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.用最佳方式切分数据集</span></span><br><span class="line"><span class="comment"># 2.生成相应的叶节点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestSplit</span><span class="params">(dataSet, leafType=regLeaf, errType=regErr, ops=<span class="params">(<span class="number">1</span>, <span class="number">4</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""chooseBestSplit(用最佳方式切分数据集 和 生成相应的叶节点)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet   加载的原始数据集</span></span><br><span class="line"><span class="string">        leafType  建立叶子点的函数</span></span><br><span class="line"><span class="string">        errType   误差计算函数(求总方差)</span></span><br><span class="line"><span class="string">        ops       [容许误差下降值，切分的最少样本数]。</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bestIndex feature的index坐标</span></span><br><span class="line"><span class="string">        bestValue 切分的最优值</span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ops=(1,4)，非常重要，因为它决定了决策树划分停止的threshold值，被称为预剪枝（prepruning），其实也就是用于控制函数的停止时机。</span></span><br><span class="line">    <span class="comment"># 之所以这样说，是因为它防止决策树的过拟合，所以当误差的下降值小于tolS，或划分后的集合size小于tolN时，选择停止继续划分。</span></span><br><span class="line">    <span class="comment"># 最小误差下降值，划分后的误差减小小于这个差值，就不用继续划分</span></span><br><span class="line">    tolS = ops[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 划分最小 size 小于，就不继续划分了</span></span><br><span class="line">    tolN = ops[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 如果结果集(最后一列为1个变量)，就返回退出</span></span><br><span class="line">    <span class="comment"># .T 对数据集进行转置</span></span><br><span class="line">    <span class="comment"># .tolist()[0] 转化为数组并取第0列</span></span><br><span class="line">    <span class="keyword">if</span> len(set(dataSet[:, <span class="number">-1</span>].T.tolist()[<span class="number">0</span>])) == <span class="number">1</span>: <span class="comment"># 如果集合size为1，不用继续划分。</span></span><br><span class="line">        <span class="comment">#  exit cond 1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, leafType(dataSet)</span><br><span class="line">    <span class="comment"># 计算行列值</span></span><br><span class="line">    m, n = shape(dataSet)</span><br><span class="line">    <span class="comment"># 无分类误差的总方差和</span></span><br><span class="line">    <span class="comment"># the choice of the best feature is driven by Reduction in RSS error from mean</span></span><br><span class="line">    S = errType(dataSet)</span><br><span class="line">    <span class="comment"># inf 正无穷大</span></span><br><span class="line">    bestS, bestIndex, bestValue = inf, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="comment"># 循环处理每一列对应的feature值</span></span><br><span class="line">    <span class="keyword">for</span> featIndex <span class="keyword">in</span> range(n<span class="number">-1</span>): <span class="comment"># 对于每个特征</span></span><br><span class="line">        <span class="comment"># [0]表示这一列的[所有行]，不要[0]就是一个array[[所有行]]</span></span><br><span class="line">        <span class="keyword">for</span> splitVal <span class="keyword">in</span> set(dataSet[:, featIndex].T.tolist()[<span class="number">0</span>]):</span><br><span class="line">            <span class="comment"># 对该列进行分组，然后组内的成员的val值进行 二元切分</span></span><br><span class="line">            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)</span><br><span class="line">            <span class="comment"># 判断二元切分的方式的元素数量是否符合预期</span></span><br><span class="line">            <span class="keyword">if</span> (shape(mat0)[<span class="number">0</span>] &lt; tolN) <span class="keyword">or</span> (shape(mat1)[<span class="number">0</span>] &lt; tolN):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            newS = errType(mat0) + errType(mat1)</span><br><span class="line">            <span class="comment"># 如果二元切分，算出来的误差在可接受范围内，那么就记录切分点，并记录最小误差</span></span><br><span class="line">            <span class="comment"># 如果划分后误差小于 bestS，则说明找到了新的bestS</span></span><br><span class="line">            <span class="keyword">if</span> newS &lt; bestS:</span><br><span class="line">                bestIndex = featIndex</span><br><span class="line">                bestValue = splitVal</span><br><span class="line">                bestS = newS</span><br><span class="line">    <span class="comment"># 判断二元切分的方式的元素误差是否符合预期</span></span><br><span class="line">    <span class="comment"># if the decrease (S-bestS) is less than a threshold don't do the split</span></span><br><span class="line">    <span class="keyword">if</span> (S - bestS) &lt; tolS:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, leafType(dataSet)</span><br><span class="line">    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)</span><br><span class="line">    <span class="comment"># 对整体的成员进行判断，是否符合预期</span></span><br><span class="line">    <span class="comment"># 如果集合的 size 小于 tolN </span></span><br><span class="line">    <span class="keyword">if</span> (shape(mat0)[<span class="number">0</span>] &lt; tolN) <span class="keyword">or</span> (shape(mat1)[<span class="number">0</span>] &lt; tolN): <span class="comment"># 当最佳划分后，集合过小，也不划分，产生叶节点</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, leafType(dataSet)</span><br><span class="line">    <span class="keyword">return</span> bestIndex, bestValue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># assume dataSet is NumPy Mat so we can array filtering</span></span><br><span class="line"><span class="comment"># 假设 dataSet 是 NumPy Mat 类型的，那么我们可以进行 array 过滤</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, leafType=regLeaf, errType=regErr, ops=<span class="params">(<span class="number">1</span>, <span class="number">4</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""createTree(获取回归树)</span></span><br><span class="line"><span class="string">        Description：递归函数：如果构建的是回归树，该模型是一个常数，如果是模型树，其模型师一个线性方程。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet      加载的原始数据集</span></span><br><span class="line"><span class="string">        leafType     建立叶子点的函数</span></span><br><span class="line"><span class="string">        errType      误差计算函数</span></span><br><span class="line"><span class="string">        ops=(1, 4)   [容许误差下降值，切分的最少样本数]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        retTree    决策树最后的结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 选择最好的切分方式： feature索引值，最优切分值</span></span><br><span class="line">    <span class="comment"># choose the best split</span></span><br><span class="line">    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)</span><br><span class="line">    <span class="comment"># if the splitting hit a stop condition return val</span></span><br><span class="line">    <span class="comment"># 如果 splitting 达到一个停止条件，那么返回 val</span></span><br><span class="line">    <span class="keyword">if</span> feat <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> val</span><br><span class="line">    retTree = &#123;&#125;</span><br><span class="line">    retTree[<span class="string">'spInd'</span>] = feat</span><br><span class="line">    retTree[<span class="string">'spVal'</span>] = val</span><br><span class="line">    <span class="comment"># 大于在右边，小于在左边，分为2个数据集</span></span><br><span class="line">    lSet, rSet = binSplitDataSet(dataSet, feat, val)</span><br><span class="line">    <span class="comment"># 递归的进行调用，在左右子树中继续递归生成树</span></span><br><span class="line">    retTree[<span class="string">'left'</span>] = createTree(lSet, leafType, errType, ops)</span><br><span class="line">    retTree[<span class="string">'right'</span>] = createTree(rSet, leafType, errType, ops)</span><br><span class="line">    <span class="keyword">return</span> retTree</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" target="_blank" rel="noopener">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py</a></p>
<blockquote>
<p>测试算法：使用测试数据上的R^2值来分析模型的效果</p>
</blockquote>
<blockquote>
<p>使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情</p>
</blockquote>
<h2 id="2、树剪枝"><a href="#2、树剪枝" class="headerlink" title="2、树剪枝"></a>2、树剪枝</h2><p>一棵树如果节点过多，表明该模型可能对数据进行了 “过拟合”。</p>
<p>通过降低决策树的复杂度来避免过拟合的过程称为 <code>剪枝（pruning）</code>。在函数 chooseBestSplit() 中提前终止条件，实际上是在进行一种所谓的 <code>预剪枝（prepruning）</code>操作。另一个形式的剪枝需要使用测试集和训练集，称作 <code>后剪枝（postpruning）</code>。</p>
<h3 id="2-1、预剪枝-prepruning"><a href="#2-1、预剪枝-prepruning" class="headerlink" title="2.1、预剪枝(prepruning)"></a>2.1、预剪枝(prepruning)</h3><p>顾名思义，预剪枝就是及早的停止树增长，在构造决策树的同时进行剪枝。</p>
<p>所有决策树的构建方法，都是在无法进一步降低熵的情况下才会停止创建分支的过程，为了避免过拟合，可以设定一个阈值，熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。但是这种方法实际中的效果并不好。</p>
<h3 id="2-2、后剪枝-postpruning"><a href="#2-2、后剪枝-postpruning" class="headerlink" title="2.2、后剪枝(postpruning)"></a>2.2、后剪枝(postpruning)</h3><p>决策树构造完成后进行剪枝。剪枝的过程是对拥有同样父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否小于某一阈值。如果确实小，则这一组节点可以合并一个节点，其中包含了所有可能的结果。合并也被称作 <code>塌陷处理</code> ，在回归树中一般采用取需要合并的所有子树的平均值。后剪枝是目前最普遍的做法。</p>
<p>后剪枝 prune() 的伪代码如下:</p>
<figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">基于已有的树切分测试数据:</span></span><br><span class="line">    如果存在任一子集是一棵树，则在该子集递归剪枝过程</span><br><span class="line">    计算将当前两个叶节点合并后的误差</span><br><span class="line">    计算不合并的误差</span><br><span class="line">    如果合并会降低误差的话，就将叶节点合并</span><br></pre></td></tr></table></figure>

<h3 id="2-3、剪枝-代码"><a href="#2-3、剪枝-代码" class="headerlink" title="2.3、剪枝 代码"></a>2.3、剪枝 代码</h3><p>回归树剪枝函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 判断节点是否是一个字典</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isTree</span><span class="params">(obj)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        测试输入变量是否是一棵树,即是否是一个字典</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        obj -- 输入变量</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        返回布尔类型的结果。如果 obj 是一个字典，返回true，否则返回 false</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> (type(obj).__name__ == <span class="string">'dict'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算左右枝丫的均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMean</span><span class="params">(tree)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        从上往下遍历树直到叶节点为止，如果找到两个叶节点则计算它们的平均值。</span></span><br><span class="line"><span class="string">        对 tree 进行塌陷处理，即返回树平均值。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tree -- 输入的树</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        返回 tree 节点的平均值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">'right'</span>]):</span><br><span class="line">        tree[<span class="string">'right'</span>] = getMean(tree[<span class="string">'right'</span>])</span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">'left'</span>]):</span><br><span class="line">        tree[<span class="string">'left'</span>] = getMean(tree[<span class="string">'left'</span>])</span><br><span class="line">    <span class="keyword">return</span> (tree[<span class="string">'left'</span>]+tree[<span class="string">'right'</span>])/<span class="number">2.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否适合合并分枝</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune</span><span class="params">(tree, testData)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        从上而下找到叶节点，用测试数据集来判断将这些叶节点合并是否能降低测试误差</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tree -- 待剪枝的树</span></span><br><span class="line"><span class="string">        testData -- 剪枝所需要的测试数据 testData </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        tree -- 剪枝完成的树</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 判断是否测试数据集没有数据，如果没有，就直接返回tree本身的均值</span></span><br><span class="line">    <span class="keyword">if</span> shape(testData)[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> getMean(tree)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 判断分枝是否是dict字典，如果是就将测试数据集进行切分</span></span><br><span class="line">    <span class="keyword">if</span> (isTree(tree[<span class="string">'right'</span>]) <span class="keyword">or</span> isTree(tree[<span class="string">'left'</span>])):</span><br><span class="line">        lSet, rSet = binSplitDataSet(testData, tree[<span class="string">'spInd'</span>], tree[<span class="string">'spVal'</span>])</span><br><span class="line">    <span class="comment"># 如果是左边分枝是字典，就传入左边的数据集和左边的分枝，进行递归</span></span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">'left'</span>]):</span><br><span class="line">        tree[<span class="string">'left'</span>] = prune(tree[<span class="string">'left'</span>], lSet)</span><br><span class="line">    <span class="comment"># 如果是右边分枝是字典，就传入左边的数据集和左边的分枝，进行递归</span></span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">'right'</span>]):</span><br><span class="line">        tree[<span class="string">'right'</span>] = prune(tree[<span class="string">'right'</span>], rSet)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 上面的一系列操作本质上就是将测试数据集按照训练完成的树拆分好，对应的值放到对应的节点</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果左右两边同时都不是dict字典，也就是左右两边都是叶节点，而不是子树了，那么分割测试数据集。</span></span><br><span class="line">    <span class="comment"># 1. 如果正确 </span></span><br><span class="line">    <span class="comment">#   * 那么计算一下总方差 和 该结果集的本身不分枝的总方差比较</span></span><br><span class="line">    <span class="comment">#   * 如果 合并的总方差 &lt; 不合并的总方差，那么就进行合并</span></span><br><span class="line">    <span class="comment"># 注意返回的结果： 如果可以合并，原来的dict就变为了 数值</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isTree(tree[<span class="string">'left'</span>]) <span class="keyword">and</span> <span class="keyword">not</span> isTree(tree[<span class="string">'right'</span>]):</span><br><span class="line">        lSet, rSet = binSplitDataSet(testData, tree[<span class="string">'spInd'</span>], tree[<span class="string">'spVal'</span>])</span><br><span class="line">        <span class="comment"># power(x, y)表示x的y次方</span></span><br><span class="line">        errorNoMerge = sum(power(lSet[:, <span class="number">-1</span>] - tree[<span class="string">'left'</span>], <span class="number">2</span>)) + sum(power(rSet[:, <span class="number">-1</span>] - tree[<span class="string">'right'</span>], <span class="number">2</span>))</span><br><span class="line">        treeMean = (tree[<span class="string">'left'</span>] + tree[<span class="string">'right'</span>])/<span class="number">2.0</span></span><br><span class="line">        errorMerge = sum(power(testData[:, <span class="number">-1</span>] - treeMean, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 如果 合并的总方差 &lt; 不合并的总方差，那么就进行合并</span></span><br><span class="line">        <span class="keyword">if</span> errorMerge &lt; errorNoMerge:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"merging"</span></span><br><span class="line">            <span class="keyword">return</span> treeMean</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> tree</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> tree</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" target="_blank" rel="noopener">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py</a></p>
<h2 id="3、模型树"><a href="#3、模型树" class="headerlink" title="3、模型树"></a>3、模型树</h2><h3 id="3-1、模型树-简介"><a href="#3-1、模型树-简介" class="headerlink" title="3.1、模型树 简介"></a>3.1、模型树 简介</h3><p>用树来对数据建模，除了把叶节点简单地设定为常数值之外，还有一种方法是把叶节点设定为分段线性函数，这里所谓的 <code>分段线性（piecewise linear）</code> 是指模型由多个线性片段组成。</p>
<p>我们看一下图 9-4 中的数据，如果使用两条直线拟合是否比使用一组常数来建模好呢？答案显而易见。可以设计两条分别从 0.0<del>0.3、从 0.3</del>1.0 的直线，于是就可以得到两个线性模型。因为数据集里的一部分数据（0.0<del>0.3）以某个线性模型建模，而另一部分数据（0.3</del>1.0）则以另一个线性模型建模，因此我们说采用了所谓的分段线性模型。</p>
<p>决策树相比于其他机器学习算法的优势之一在于结果更易理解。很显然，两条直线比很多节点组成一棵大树更容易解释。模型树的可解释性是它优于回归树的特点之一。另外，模型树也具有更高的预测准确度。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/9.TreeRegression/RegTree_3.png" alt="分段线性数据"></p>
<p>将之前的回归树的代码稍作修改，就可以在叶节点生成线性模型而不是常数值。下面将利用树生成算法对数据进行划分，且每份切分数据都能很容易被线性模型所表示。这个算法的关键在于误差的计算。</p>
<p>那么为了找到最佳切分，应该怎样计算误差呢？前面用于回归树的误差计算方法这里不能再用。稍加变化，对于给定的数据集，应该先用模型来对它进行拟合，然后计算真实的目标值与模型预测值间的差值。最后将这些差值的平方求和就得到了所需的误差。</p>
<h3 id="3-2、模型树-代码"><a href="#3-2、模型树-代码" class="headerlink" title="3.2、模型树 代码"></a>3.2、模型树 代码</h3><p>模型树的叶节点生成函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 得到模型的ws系数：f(x) = x0 + x1*featrue1+ x3*featrue2 ...</span></span><br><span class="line"><span class="comment"># create linear model and return coeficients</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelLeaf</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        当数据不再需要切分的时候，生成叶节点的模型。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet -- 输入数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        调用 linearSolve 函数，返回得到的 回归系数ws</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ws, X, Y = linearSolve(dataSet)</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算线性模型的误差值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelErr</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        在给定数据集上计算误差。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet -- 输入数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        调用 linearSolve 函数，返回 yHat 和 Y 之间的平方误差。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ws, X, Y = linearSolve(dataSet)</span><br><span class="line">    yHat = X * ws</span><br><span class="line">    <span class="comment"># print corrcoef(yHat, Y, rowvar=0)</span></span><br><span class="line">    <span class="keyword">return</span> sum(power(Y - yHat, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="comment"># helper function used in two places</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linearSolve</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        将数据集格式化成目标变量Y和自变量X，执行简单的线性回归，得到ws</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet -- 输入数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        ws -- 执行线性回归的回归系数 </span></span><br><span class="line"><span class="string">        X -- 格式化自变量X</span></span><br><span class="line"><span class="string">        Y -- 格式化目标变量Y</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m, n = shape(dataSet)</span><br><span class="line">    <span class="comment"># 产生一个关于1的矩阵</span></span><br><span class="line">    X = mat(ones((m, n)))</span><br><span class="line">    Y = mat(ones((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># X的0列为1，常数项，用于计算平衡误差</span></span><br><span class="line">    X[:, <span class="number">1</span>: n] = dataSet[:, <span class="number">0</span>: n<span class="number">-1</span>]</span><br><span class="line">    Y = dataSet[:, <span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转置矩阵*矩阵</span></span><br><span class="line">    xTx = X.T * X</span><br><span class="line">    <span class="comment"># 如果矩阵的逆不存在，会造成程序异常</span></span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">raise</span> NameError(<span class="string">'This matrix is singular, cannot do inverse,\ntry increasing the second value of ops'</span>)</span><br><span class="line">    <span class="comment"># 最小二乘法求最优解:  w0*1+w1*x1=y</span></span><br><span class="line">    ws = xTx.I * (X.T * Y)</span><br><span class="line">    <span class="keyword">return</span> ws, X, Y</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" target="_blank" rel="noopener">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py</a></p>
<h3 id="3-3、模型树-运行结果"><a href="#3-3、模型树-运行结果" class="headerlink" title="3.3、模型树 运行结果"></a>3.3、模型树 运行结果</h3><p><img src="http://data.apachecn.org/img/AiLearning/ml/9.TreeRegression/RegTree_4.png" alt="模型树运行结果"></p>
<h2 id="4、树回归-项目案例"><a href="#4、树回归-项目案例" class="headerlink" title="4、树回归 项目案例"></a>4、树回归 项目案例</h2><h3 id="4-1、项目案例1-树回归与标准回归的比较"><a href="#4-1、项目案例1-树回归与标准回归的比较" class="headerlink" title="4.1、项目案例1: 树回归与标准回归的比较"></a>4.1、项目案例1: 树回归与标准回归的比较</h3><h4 id="4-1-1、项目概述"><a href="#4-1-1、项目概述" class="headerlink" title="4.1.1、项目概述"></a>4.1.1、项目概述</h4><p>前面介绍了模型树、回归树和一般的回归方法，下面测试一下哪个模型最好。</p>
<p>这些模型将在某个数据上进行测试，该数据涉及人的智力水平和自行车的速度的关系。当然，数据是假的。</p>
<h4 id="4-1-2、开发流程"><a href="#4-1-2、开发流程" class="headerlink" title="4.1.2、开发流程"></a>4.1.2、开发流程</h4><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">收集数据：采用任意方法收集数据</span><br><span class="line">准备数据：需要数值型数据，标称型数据应该映射成二值型数据</span><br><span class="line">分析数据：绘出数据的二维可视化显示结果，以字典方式生成树</span><br><span class="line">训练算法：模型树的构建</span><br><span class="line">测试算法：使用测试数据上的R^<span class="number">2</span>值来分析模型的效果</span><br><span class="line">使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 采用任意方法收集数据</p>
</blockquote>
<blockquote>
<p>准备数据：需要数值型数据，标称型数据应该映射成二值型数据</p>
</blockquote>
<p>数据存储格式:</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">3.000000</span>	<span class="number">46.852122</span></span><br><span class="line"><span class="number">23.000000</span>	<span class="number">178.676107</span></span><br><span class="line"><span class="number">0.000000</span>	<span class="number">86.154024</span></span><br><span class="line"><span class="number">6.000000</span>	<span class="number">68.707614</span></span><br><span class="line"><span class="number">15.000000</span>	<span class="number">139.737693</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据：绘出数据的二维可视化显示结果，以字典方式生成树</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/9.TreeRegression/RegTree_5.png" alt="骑自行车速度和智商之间的关系"></p>
<blockquote>
<p>训练算法：模型树的构建</p>
</blockquote>
<p>用树回归进行预测的代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 回归树测试案例</span></span><br><span class="line"><span class="comment"># 为了和 modelTreeEval() 保持一致，保留两个输入参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regTreeEval</span><span class="params">(model, inDat)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对 回归树 进行预测</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model -- 指定模型，可选值为 回归树模型 或者 模型树模型，这里为回归树</span></span><br><span class="line"><span class="string">        inDat -- 输入的测试数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        float(model) -- 将输入的模型数据转换为 浮点数 返回</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> float(model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型树测试案例</span></span><br><span class="line"><span class="comment"># 对输入数据进行格式化处理，在原数据矩阵上增加第0列，元素的值都是1，</span></span><br><span class="line"><span class="comment"># 也就是增加偏移值，和我们之前的简单线性回归是一个套路，增加一个偏移量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelTreeEval</span><span class="params">(model, inDat)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对 模型树 进行预测</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model -- 输入模型，可选值为 回归树模型 或者 模型树模型，这里为模型树模型</span></span><br><span class="line"><span class="string">        inDat -- 输入的测试数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        float(X * model) -- 将测试数据乘以 回归系数 得到一个预测值 ，转化为 浮点数 返回</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n = shape(inDat)[<span class="number">1</span>]</span><br><span class="line">    X = mat(ones((<span class="number">1</span>, n+<span class="number">1</span>)))</span><br><span class="line">    X[:, <span class="number">1</span>: n+<span class="number">1</span>] = inDat</span><br><span class="line">    <span class="comment"># print X, model</span></span><br><span class="line">    <span class="keyword">return</span> float(X * model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算预测的结果</span></span><br><span class="line"><span class="comment"># 在给定树结构的情况下，对于单个数据点，该函数会给出一个预测值。</span></span><br><span class="line"><span class="comment"># modelEval是对叶节点进行预测的函数引用，指定树的类型，以便在叶节点上调用合适的模型。</span></span><br><span class="line"><span class="comment"># 此函数自顶向下遍历整棵树，直到命中叶节点为止，一旦到达叶节点，它就会在输入数据上</span></span><br><span class="line"><span class="comment"># 调用modelEval()函数，该函数的默认值为regTreeEval()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">treeForeCast</span><span class="params">(tree, inData, modelEval=regTreeEval)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对特定模型的树进行预测，可以是 回归树 也可以是 模型树</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tree -- 已经训练好的树的模型</span></span><br><span class="line"><span class="string">        inData -- 输入的测试数据</span></span><br><span class="line"><span class="string">        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        返回预测值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isTree(tree):</span><br><span class="line">        <span class="keyword">return</span> modelEval(tree, inData)</span><br><span class="line">    <span class="keyword">if</span> inData[tree[<span class="string">'spInd'</span>]] &lt;= tree[<span class="string">'spVal'</span>]:</span><br><span class="line">        <span class="keyword">if</span> isTree(tree[<span class="string">'left'</span>]):</span><br><span class="line">            <span class="keyword">return</span> treeForeCast(tree[<span class="string">'left'</span>], inData, modelEval)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> modelEval(tree[<span class="string">'left'</span>], inData)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> isTree(tree[<span class="string">'right'</span>]):</span><br><span class="line">            <span class="keyword">return</span> treeForeCast(tree[<span class="string">'right'</span>], inData, modelEval)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> modelEval(tree[<span class="string">'right'</span>], inData)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createForeCast</span><span class="params">(tree, testData, modelEval=regTreeEval)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        调用 treeForeCast ，对特定模型的树进行预测，可以是 回归树 也可以是 模型树</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tree -- 已经训练好的树的模型</span></span><br><span class="line"><span class="string">        inData -- 输入的测试数据</span></span><br><span class="line"><span class="string">        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        返回预测值矩阵</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = len(testData)</span><br><span class="line">    yHat = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># print yHat</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        yHat[i, <span class="number">0</span>] = treeForeCast(tree, mat(testData[i]), modelEval)</span><br><span class="line">        <span class="comment"># print "yHat==&gt;", yHat[i, 0]</span></span><br><span class="line">    <span class="keyword">return</span> yHat</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" target="_blank" rel="noopener">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/regTrees.py</a></p>
<blockquote>
<p>测试算法：使用测试数据上的R^2值来分析模型的效果</p>
</blockquote>
<p>R^2 判定系数就是拟合优度判定系数，它体现了回归模型中自变量的变异在因变量的变异中所占的比例。如 R^2=0.99999 表示在因变量 y 的变异中有 99.999% 是由于变量 x 引起。当 R^2=1 时表示，所有观测点都落在拟合的直线或曲线上；当 R^2=0 时，表示自变量与因变量不存在直线或曲线关系。</p>
<p>所以我们看出， R^2 的值越接近 1.0 越好。</p>
<blockquote>
<p>使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情</p>
</blockquote>
<h2 id="5、附加-Python-中-GUI-的使用"><a href="#5、附加-Python-中-GUI-的使用" class="headerlink" title="5、附加 Python 中 GUI 的使用"></a>5、附加 Python 中 GUI 的使用</h2><h3 id="5-1、使用-Python-的-Tkinter-库创建-GUI"><a href="#5-1、使用-Python-的-Tkinter-库创建-GUI" class="headerlink" title="5.1、使用 Python 的 Tkinter 库创建 GUI"></a>5.1、使用 Python 的 Tkinter 库创建 GUI</h3><p>如果能让用户不需要任何指令就可以按照他们自己的方式来分析数据，就不需要对数据做出过多解释。其中一个能同时支持数据呈现和用户交互的方式就是构建一个图形用户界面(GUI，Graphical User Interface)，如图9-7所示。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/9.TreeRegression/GUI%E7%A4%BA%E4%BE%8B%E5%9B%BE.png" alt="GUI示例图" title="GUI示例图"></p>
<h3 id="5-2、用-Tkinter-创建-GUI"><a href="#5-2、用-Tkinter-创建-GUI" class="headerlink" title="5.2、用 Tkinter 创建 GUI"></a>5.2、用 Tkinter 创建 GUI</h3><p>Python 有很多 GUI 框架，其中一个易于使用的 Tkinter，是随 Python 的标准版编译版本发布的。Tkinter 可以在 Windows、Mac OS和大多数的 Linux 平台上使用。</p>
<h3 id="5-3、集成-Matplotlib-和-Tkinter"><a href="#5-3、集成-Matplotlib-和-Tkinter" class="headerlink" title="5.3、集成 Matplotlib 和 Tkinter"></a>5.3、集成 Matplotlib 和 Tkinter</h3><p>MatPlotlib 的构建程序包含一个前端，也就是面向用户的一些代码，如 plot() 和 scatter() 方法等。事实上，它同时创建了一个后端，用于实现绘图和不同应用之间接口。</p>
<p>通过改变后端可以将图像绘制在PNG、PDF、SVG等格式的文件上。下面将设置后端为 TkAgg (Agg 是一个 C++ 的库，可以从图像创建光栅图)。TkAgg可以在所选GUI框架上调用Agg，把 Agg 呈现在画布上。我们可以在Tk的GUI上放置一个画布，并用 .grid()来调整布局。</p>
<h3 id="5-4、用treeExplore-的GUI构建的模型树示例图"><a href="#5-4、用treeExplore-的GUI构建的模型树示例图" class="headerlink" title="5.4、用treeExplore 的GUI构建的模型树示例图"></a>5.4、用treeExplore 的GUI构建的模型树示例图</h3><p><img src="http://data.apachecn.org/img/AiLearning/ml/9.TreeRegression/GUI%E6%9B%B4%E5%A5%BD%E7%9A%84%E7%A4%BA%E4%BE%8B%E5%9B%BE.png" alt="取得更好预测效果的GUI示例图" title="取得更好预测效果的GUI示例图"></p>
<p><a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/treeExplore.py" target="_blank" rel="noopener">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/treeExplore.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/9.RegTrees/treeExplore.py</a></p>
<h2 id="6、树回归-小结"><a href="#6、树回归-小结" class="headerlink" title="6、树回归 小结"></a>6、树回归 小结</h2><p>数据集中经常包含一些复杂的相关关系，使得输入数据和目标变量之间呈现非线性关系。对这些复杂的关系建模，一种可行的方式是使用树来对预测值分段，包括分段常数或分段直线。一般采用树结构来对这种数据建模。相应地，若叶节点使用的模型是分段常数则称为回归树，若叶节点使用的模型师线性回归方程则称为模型树。</p>
<p>CART 算法可以用于构建二元树并处理离散型或连续型数据的切分。若使用不同的误差准则，就可以通过CART 算法构建模型树和回归树。该算法构建出的树会倾向于对数据过拟合。一棵过拟合的树常常十分复杂，剪枝技术的出现就是为了解决这个问题。两种剪枝方法分别是预剪枝（在树的构建过程中就进行剪枝）和后剪枝（当树构建完毕再进行剪枝），预剪枝更有效但需要用户定义一些参数。</p>
<p>Tkinter 是 Python 的一个 GUI 工具包。虽然并不是唯一的包，但它最常用。利用 Tkinter ，我们可以轻轻松松绘制各种部件并安排它们的位置。另外，可以为 Tkinter 构造一个特殊的部件来显示 Matplotlib 绘出的图。所以，Matplotlib 和 Tkinter 的集成可以构建出更强大的 GUI ，用户可以以更自然的方式来探索机器学习算法的奥妙。</p>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a>      </li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第七章 集成方法-随机森林和AdaBoost</title>
    <url>/2020/06/02/7.%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8CAdaBoost/</url>
    <content><![CDATA[<h2 id="集成方法-ensemble-method（元算法-meta-algorithm）-概述"><a href="#集成方法-ensemble-method（元算法-meta-algorithm）-概述" class="headerlink" title="集成方法: ensemble method（元算法: meta algorithm） 概述"></a>集成方法: ensemble method（元算法: meta algorithm） 概述</h2><ul>
<li><p>概念：是对其他算法进行组合的一种形式。</p>
</li>
<li><p>通俗来说： 当做重要决定时，大家可能都会考虑吸取多个专家而不只是一个人的意见。<br>  机器学习处理问题时又何尝不是如此？ 这就是集成方法背后的思想。</p>
</li>
<li><p>集成方法：  </p>
<ol>
<li>投票选举(bagging: 自举汇聚法 bootstrap aggregating): 是基于数据随机重抽样分类器构造的方法</li>
<li>再学习(boosting): 是基于所有分类器的加权求和的方法<a id="more"></a>

</li>
</ol>
</li>
</ul>
<h2 id="集成方法-场景"><a href="#集成方法-场景" class="headerlink" title="集成方法 场景"></a>集成方法 场景</h2><p>目前 bagging 方法最流行的版本是: 随机森林(random forest)<br/><br>选男友：美女选择择偶对象的时候，会问几个闺蜜的建议，最后选择一个综合得分最高的一个作为男朋友</p>
<p>目前 boosting 方法最流行的版本是: AdaBoost<br/><br>追女友：3个帅哥追同一个美女，第1个帅哥失败-&gt;(传授经验：姓名、家庭情况) 第2个帅哥失败-&gt;(传授经验：兴趣爱好、性格特点) 第3个帅哥成功</p>
<blockquote>
<p>bagging 和 boosting 区别是什么？</p>
</blockquote>
<ol>
<li>bagging 是一种与 boosting 很类似的技术, 所使用的多个分类器的类型（数据量和特征量）都是一致的。</li>
<li>bagging 是由不同的分类器（1.数据随机化 2.特征随机化）经过训练，综合得出的出现最多分类结果；boosting 是通过调整已有分类器错分的那些数据来获得新的分类器，得出目前最优的结果。</li>
<li>bagging 中的分类器权重是相等的；而 boosting 中的分类器加权求和，所以权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。</li>
</ol>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><h3 id="随机森林-概述"><a href="#随机森林-概述" class="headerlink" title="随机森林 概述"></a>随机森林 概述</h3><ul>
<li>随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。</li>
<li>决策树相当于一个大师，通过自己在数据集中学到的知识用于新数据的分类。但是俗话说得好，一个诸葛亮，玩不过三个臭皮匠。随机森林就是希望构建多个臭皮匠，希望最终的分类效果能够超过单个大师的一种算法。</li>
</ul>
<h3 id="随机森林-原理"><a href="#随机森林-原理" class="headerlink" title="随机森林 原理"></a>随机森林 原理</h3><p>那随机森林具体如何构建呢？<br/><br>有两个方面：<br/></p>
<ol>
<li>数据的随机性化<br/> </li>
<li>待选特征的随机化<br/></li>
</ol>
<p>使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。</p>
<blockquote>
<p>数据的随机化：使得随机森林中的决策树更普遍化一点，适合更多的场景。</p>
</blockquote>
<p>（有放回的准确率在：70% 以上， 无放回的准确率在：60% 以上）</p>
<ol>
<li>采取有放回的抽样方式 构造子数据集，保证不同子集之间的数量级一样（不同子集／同一子集 之间的元素可以重复）</li>
<li>利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。</li>
<li>然后统计子决策树的投票结果，得到最终的分类 就是 随机森林的输出结果。</li>
<li>如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。</li>
</ol>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/7.RandomForest/%E6%95%B0%E6%8D%AE%E9%87%8D%E6%8A%BD%E6%A0%B7.jpg" alt="数据重抽样"></p>
<blockquote>
<p>待选特征的随机化</p>
</blockquote>
<ol>
<li>子树从所有的待选特征中随机选取一定的特征。</li>
<li>在选取的特征中选取最优的特征。</li>
</ol>
<p>下图中，蓝色的方块代表所有可以被选择的特征，也就是目前的待选特征；黄色的方块是分裂特征。<br/><br>左边是一棵决策树的特征选取过程，通过在待选特征中选取最优的分裂特征（别忘了前文提到的ID3算法，C4.5算法，CART算法等等），完成分裂。<br/><br>右边是一个随机森林中的子树的特征选取过程。<br/></p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/7.RandomForest/%E7%89%B9%E5%BE%81%E9%87%8D%E6%8A%BD%E6%A0%B7.jpg" alt="特征重抽样"></p>
<blockquote>
<p>随机森林 开发流程</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">收集数据：任何方法</span><br><span class="line">准备数据：转换样本集</span><br><span class="line">分析数据：任何方法</span><br><span class="line">训练算法：通过数据随机化和特征随机化，进行多实例的分类评估</span><br><span class="line">测试算法：计算错误率</span><br><span class="line">使用算法：输入样本数据，然后运行 随机森林 算法判断输入数据分类属于哪个分类，最后对计算出的分类执行后续处理</span><br></pre></td></tr></table></figure>

<blockquote>
<p>随机森林 算法特点</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">优点：几乎不需要输入准备、可实现隐式特征选择、训练速度非常快、其他模型很难超越、很难建立一个糟糕的随机森林模型、大量优秀、免费以及开源的实现。</span><br><span class="line">缺点：劣势在于模型大小、是个很难去解释的黑盒子。</span><br><span class="line">适用数据范围：数值型和标称型</span><br></pre></td></tr></table></figure>

<h3 id="项目案例-声纳信号分类"><a href="#项目案例-声纳信号分类" class="headerlink" title="项目案例: 声纳信号分类"></a>项目案例: 声纳信号分类</h3><h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p>这是 Gorman 和 Sejnowski 在研究使用神经网络的声纳信号分类中使用的数据集。任务是训练一个模型来区分声纳信号。</p>
<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight isbl"><table><tr><td class="code"><pre><span class="line">收集数据：提供的文本文件</span><br><span class="line">准备数据：转换样本集</span><br><span class="line">分析数据：手工检查数据</span><br><span class="line">训练算法：在数据上，利用 <span class="function"><span class="title">random_forest</span>() 函数进行优化评估，返回模型的综合分类结果</span></span><br><span class="line"><span class="function">测试算法：在采用自定义 <span class="variable">n_folds</span> 份随机重抽样 进行测试评估，得出综合的预测评分</span></span><br><span class="line"><span class="function">使用算法：若你感兴趣可以构建完整的应用程序，从案例进行封装，也可以参考我们的代码</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据：提供的文本文件</p>
</blockquote>
<p>样本数据：sonar-all-data.txt</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">0.02</span>,<span class="number">0.0371</span>,<span class="number">0.0428</span>,<span class="number">0.0207</span>,<span class="number">0.0954</span>,<span class="number">0.0986</span>,<span class="number">0.1539</span>,<span class="number">0.1601</span>,<span class="number">0.3109</span>,<span class="number">0.2111</span>,<span class="number">0.1609</span>,<span class="number">0.1582</span>,<span class="number">0.2238</span>,<span class="number">0.0645</span>,<span class="number">0.066</span>,<span class="number">0.2273</span>,<span class="number">0.31</span>,<span class="number">0.2999</span>,<span class="number">0.5078</span>,<span class="number">0.4797</span>,<span class="number">0.5783</span>,<span class="number">0.5071</span>,<span class="number">0.4328</span>,<span class="number">0.555</span>,<span class="number">0.6711</span>,<span class="number">0.6415</span>,<span class="number">0.7104</span>,<span class="number">0.808</span>,<span class="number">0.6791</span>,<span class="number">0.3857</span>,<span class="number">0.1307</span>,<span class="number">0.2604</span>,<span class="number">0.5121</span>,<span class="number">0.7547</span>,<span class="number">0.8537</span>,<span class="number">0.8507</span>,<span class="number">0.6692</span>,<span class="number">0.6097</span>,<span class="number">0.4943</span>,<span class="number">0.2744</span>,<span class="number">0.051</span>,<span class="number">0.2834</span>,<span class="number">0.2825</span>,<span class="number">0.4256</span>,<span class="number">0.2641</span>,<span class="number">0.1386</span>,<span class="number">0.1051</span>,<span class="number">0.1343</span>,<span class="number">0.0383</span>,<span class="number">0.0324</span>,<span class="number">0.0232</span>,<span class="number">0.0027</span>,<span class="number">0.0065</span>,<span class="number">0.0159</span>,<span class="number">0.0072</span>,<span class="number">0.0167</span>,<span class="number">0.018</span>,<span class="number">0.0084</span>,<span class="number">0.009</span>,<span class="number">0.0032</span>,R</span><br><span class="line"><span class="number">0.0453</span>,<span class="number">0.0523</span>,<span class="number">0.0843</span>,<span class="number">0.0689</span>,<span class="number">0.1183</span>,<span class="number">0.2583</span>,<span class="number">0.2156</span>,<span class="number">0.3481</span>,<span class="number">0.3337</span>,<span class="number">0.2872</span>,<span class="number">0.4918</span>,<span class="number">0.6552</span>,<span class="number">0.6919</span>,<span class="number">0.7797</span>,<span class="number">0.7464</span>,<span class="number">0.9444</span>,<span class="number">1</span>,<span class="number">0.8874</span>,<span class="number">0.8024</span>,<span class="number">0.7818</span>,<span class="number">0.5212</span>,<span class="number">0.4052</span>,<span class="number">0.3957</span>,<span class="number">0.3914</span>,<span class="number">0.325</span>,<span class="number">0.32</span>,<span class="number">0.3271</span>,<span class="number">0.2767</span>,<span class="number">0.4423</span>,<span class="number">0.2028</span>,<span class="number">0.3788</span>,<span class="number">0.2947</span>,<span class="number">0.1984</span>,<span class="number">0.2341</span>,<span class="number">0.1306</span>,<span class="number">0.4182</span>,<span class="number">0.3835</span>,<span class="number">0.1057</span>,<span class="number">0.184</span>,<span class="number">0.197</span>,<span class="number">0.1674</span>,<span class="number">0.0583</span>,<span class="number">0.1401</span>,<span class="number">0.1628</span>,<span class="number">0.0621</span>,<span class="number">0.0203</span>,<span class="number">0.053</span>,<span class="number">0.0742</span>,<span class="number">0.0409</span>,<span class="number">0.0061</span>,<span class="number">0.0125</span>,<span class="number">0.0084</span>,<span class="number">0.0089</span>,<span class="number">0.0048</span>,<span class="number">0.0094</span>,<span class="number">0.0191</span>,<span class="number">0.014</span>,<span class="number">0.0049</span>,<span class="number">0.0052</span>,<span class="number">0.0044</span>,R</span><br><span class="line"><span class="number">0.0262</span>,<span class="number">0.0582</span>,<span class="number">0.1099</span>,<span class="number">0.1083</span>,<span class="number">0.0974</span>,<span class="number">0.228</span>,<span class="number">0.2431</span>,<span class="number">0.3771</span>,<span class="number">0.5598</span>,<span class="number">0.6194</span>,<span class="number">0.6333</span>,<span class="number">0.706</span>,<span class="number">0.5544</span>,<span class="number">0.532</span>,<span class="number">0.6479</span>,<span class="number">0.6931</span>,<span class="number">0.6759</span>,<span class="number">0.7551</span>,<span class="number">0.8929</span>,<span class="number">0.8619</span>,<span class="number">0.7974</span>,<span class="number">0.6737</span>,<span class="number">0.4293</span>,<span class="number">0.3648</span>,<span class="number">0.5331</span>,<span class="number">0.2413</span>,<span class="number">0.507</span>,<span class="number">0.8533</span>,<span class="number">0.6036</span>,<span class="number">0.8514</span>,<span class="number">0.8512</span>,<span class="number">0.5045</span>,<span class="number">0.1862</span>,<span class="number">0.2709</span>,<span class="number">0.4232</span>,<span class="number">0.3043</span>,<span class="number">0.6116</span>,<span class="number">0.6756</span>,<span class="number">0.5375</span>,<span class="number">0.4719</span>,<span class="number">0.4647</span>,<span class="number">0.2587</span>,<span class="number">0.2129</span>,<span class="number">0.2222</span>,<span class="number">0.2111</span>,<span class="number">0.0176</span>,<span class="number">0.1348</span>,<span class="number">0.0744</span>,<span class="number">0.013</span>,<span class="number">0.0106</span>,<span class="number">0.0033</span>,<span class="number">0.0232</span>,<span class="number">0.0166</span>,<span class="number">0.0095</span>,<span class="number">0.018</span>,<span class="number">0.0244</span>,<span class="number">0.0316</span>,<span class="number">0.0164</span>,<span class="number">0.0095</span>,<span class="number">0.0078</span>,R</span><br></pre></td></tr></table></figure>

<blockquote>
<p>准备数据：转换样本集</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入csv文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(filename)</span>:</span></span><br><span class="line">    dataset = []</span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            lineArr = []</span><br><span class="line">            <span class="keyword">for</span> featrue <span class="keyword">in</span> line.split(<span class="string">','</span>):</span><br><span class="line">                <span class="comment"># strip()返回移除字符串头尾指定的字符生成的新字符串</span></span><br><span class="line">                str_f = featrue.strip()</span><br><span class="line">                <span class="keyword">if</span> str_f.isdigit(): <span class="comment"># 判断是否是数字</span></span><br><span class="line">                    <span class="comment"># 将数据集的第column列转换成float形式</span></span><br><span class="line">                    lineArr.append(float(str_f))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 添加分类标签</span></span><br><span class="line">                    lineArr.append(str_f)</span><br><span class="line">            dataset.append(lineArr)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据：手工检查数据</p>
</blockquote>
<blockquote>
<p>训练算法：在数据上，利用 random_forest() 函数进行优化评估，返回模型的综合分类结果</p>
</blockquote>
<ul>
<li>样本数据随机无放回抽样-用于交叉验证</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_validation_split</span><span class="params">(dataset, n_folds)</span>:</span></span><br><span class="line">    <span class="string">"""cross_validation_split(将数据集进行抽重抽样 n_folds 份，数据可以重复抽取)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataset     原始数据集</span></span><br><span class="line"><span class="string">        n_folds     数据集dataset分成n_flods份</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dataset_split    list集合，存放的是：将数据集进行抽重抽样 n_folds 份，数据可以重复抽取</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dataset_split = list()</span><br><span class="line">    dataset_copy = list(dataset)       <span class="comment"># 复制一份 dataset,防止 dataset 的内容改变</span></span><br><span class="line">    fold_size = len(dataset) / n_folds</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_folds):</span><br><span class="line">        fold = list()                  <span class="comment"># 每次循环 fold 清零，防止重复导入 dataset_split</span></span><br><span class="line">        <span class="keyword">while</span> len(fold) &lt; fold_size:   <span class="comment"># 这里不能用 if，if 只是在第一次判断时起作用，while 执行循环，直到条件不成立</span></span><br><span class="line">            <span class="comment"># 有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此为自助采样法。从而保证每棵决策树训练集的差异性            </span></span><br><span class="line">            index = randrange(len(dataset_copy))</span><br><span class="line">            <span class="comment"># 将对应索引 index 的内容从 dataset_copy 中导出，并将该内容从 dataset_copy 中删除。</span></span><br><span class="line">            <span class="comment"># pop() 函数用于移除列表中的一个元素（默认最后一个元素），并且返回该元素的值。</span></span><br><span class="line">            fold.append(dataset_copy.pop(index))  <span class="comment"># 无放回的方式</span></span><br><span class="line">            <span class="comment"># fold.append(dataset_copy[index])  # 有放回的方式</span></span><br><span class="line">        dataset_split.append(fold)</span><br><span class="line">    <span class="comment"># 由dataset分割出的n_folds个数据构成的列表，为了用于交叉验证</span></span><br><span class="line">    <span class="keyword">return</span> dataset_split</span><br></pre></td></tr></table></figure>

<ul>
<li>训练数据集随机化</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create a random subsample from the dataset with replacement</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsample</span><span class="params">(dataset, ratio)</span>:</span>   <span class="comment"># 创建数据集的随机子样本</span></span><br><span class="line">    <span class="string">"""random_forest(评估算法性能，返回模型得分)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataset         训练数据集</span></span><br><span class="line"><span class="string">        ratio           训练数据集的样本比例</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        sample          随机抽样的训练样本</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    sample = list()</span><br><span class="line">    <span class="comment"># 训练样本的按比例抽样。</span></span><br><span class="line">    <span class="comment"># round() 方法返回浮点数x的四舍五入值。</span></span><br><span class="line">    n_sample = round(len(dataset) * ratio)</span><br><span class="line">    <span class="keyword">while</span> len(sample) &lt; n_sample:</span><br><span class="line">        <span class="comment"># 有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此为自助采样法。从而保证每棵决策树训练集的差异性</span></span><br><span class="line">        index = randrange(len(dataset))</span><br><span class="line">        sample.append(dataset[index])</span><br><span class="line">    <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure>

<ul>
<li>特征随机化</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 找出分割数据集的最优特征，得到最优的特征 index，特征值 row[index]，以及分割完的数据 groups（left, right）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_split</span><span class="params">(dataset, n_features)</span>:</span></span><br><span class="line">    class_values = list(set(row[<span class="number">-1</span>] <span class="keyword">for</span> row <span class="keyword">in</span> dataset))  <span class="comment"># class_values =[0, 1]</span></span><br><span class="line">    b_index, b_value, b_score, b_groups = <span class="number">999</span>, <span class="number">999</span>, <span class="number">999</span>, <span class="literal">None</span></span><br><span class="line">    features = list()</span><br><span class="line">    <span class="keyword">while</span> len(features) &lt; n_features:</span><br><span class="line">        index = randrange(len(dataset[<span class="number">0</span>])<span class="number">-1</span>)  <span class="comment"># 往 features 添加 n_features 个特征（ n_feature 等于特征数的个数），特征索引从 dataset 中随机取</span></span><br><span class="line">        <span class="keyword">if</span> index <span class="keyword">not</span> <span class="keyword">in</span> features:</span><br><span class="line">            features.append(index)</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> features:                    <span class="comment"># 在 n_features 个特征中选出最优的特征索引，并没有遍历所有特征，从而保证了每课决策树的差异性</span></span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> dataset:</span><br><span class="line">            groups = test_split(index, row[index], dataset)  <span class="comment"># groups=(left, right), row[index] 遍历每一行 index 索引下的特征值作为分类值 value, 找出最优的分类特征和特征值</span></span><br><span class="line">            gini = gini_index(groups, class_values)</span><br><span class="line">            <span class="comment"># 左右两边的数量越一样，说明数据区分度不高，gini系数越大</span></span><br><span class="line">            <span class="keyword">if</span> gini &lt; b_score:</span><br><span class="line">                b_index, b_value, b_score, b_groups = index, row[index], gini, groups  <span class="comment"># 最后得到最优的分类特征 b_index,分类特征值 b_value,分类结果 b_groups。b_value 为分错的代价成本</span></span><br><span class="line">    <span class="comment"># print b_score</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'index'</span>: b_index, <span class="string">'value'</span>: b_value, <span class="string">'groups'</span>: b_groups&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>随机森林</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Random Forest Algorithm</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_forest</span><span class="params">(train, test, max_depth, min_size, sample_size, n_trees, n_features)</span>:</span></span><br><span class="line">    <span class="string">"""random_forest(评估算法性能，返回模型得分)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        train           训练数据集</span></span><br><span class="line"><span class="string">        test            测试数据集</span></span><br><span class="line"><span class="string">        max_depth       决策树深度不能太深，不然容易导致过拟合</span></span><br><span class="line"><span class="string">        min_size        叶子节点的大小</span></span><br><span class="line"><span class="string">        sample_size     训练数据集的样本比例</span></span><br><span class="line"><span class="string">        n_trees         决策树的个数</span></span><br><span class="line"><span class="string">        n_features      选取的特征的个数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        predictions     每一行的预测结果，bagging 预测最后的分类结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    trees = list()</span><br><span class="line">    <span class="comment"># n_trees 表示决策树的数量</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_trees):</span><br><span class="line">        <span class="comment"># 随机抽样的训练样本， 随机采样保证了每棵决策树训练集的差异性</span></span><br><span class="line">        sample = subsample(train, sample_size)</span><br><span class="line">        <span class="comment"># 创建一个决策树</span></span><br><span class="line">        tree = build_tree(sample, max_depth, min_size, n_features)</span><br><span class="line">        trees.append(tree)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每一行的预测结果，bagging 预测最后的分类结果</span></span><br><span class="line">    predictions = [bagging_predict(trees, row) <span class="keyword">for</span> row <span class="keyword">in</span> test]</span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法：在采用自定义 n_folds 份随机重抽样 进行测试评估，得出综合的预测评分。</p>
</blockquote>
<ul>
<li>计算随机森林的预测结果的正确率</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 评估算法性能，返回模型得分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_algorithm</span><span class="params">(dataset, algorithm, n_folds, *args)</span>:</span></span><br><span class="line">    <span class="string">"""evaluate_algorithm(评估算法性能，返回模型得分)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataset     原始数据集</span></span><br><span class="line"><span class="string">        algorithm   使用的算法</span></span><br><span class="line"><span class="string">        n_folds     数据的份数</span></span><br><span class="line"><span class="string">        *args       其他的参数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        scores      模型得分</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据集进行随机抽样，分成 n_folds 份，数据无重复的抽取</span></span><br><span class="line">    folds = cross_validation_split(dataset, n_folds)</span><br><span class="line">    scores = list()</span><br><span class="line">    <span class="comment"># 每次循环从 folds 从取出一个 fold 作为测试集，其余作为训练集，遍历整个 folds ，实现交叉验证</span></span><br><span class="line">    <span class="keyword">for</span> fold <span class="keyword">in</span> folds:</span><br><span class="line">        train_set = list(folds)</span><br><span class="line">        train_set.remove(fold)</span><br><span class="line">        <span class="comment"># 将多个 fold 列表组合成一个 train_set 列表, 类似 union all</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In [20]: l1=[[1, 2, 'a'], [11, 22, 'b']]</span></span><br><span class="line"><span class="string">        In [21]: l2=[[3, 4, 'c'], [33, 44, 'd']]</span></span><br><span class="line"><span class="string">        In [22]: l=[]</span></span><br><span class="line"><span class="string">        In [23]: l.append(l1)</span></span><br><span class="line"><span class="string">        In [24]: l.append(l2)</span></span><br><span class="line"><span class="string">        In [25]: l</span></span><br><span class="line"><span class="string">        Out[25]: [[[1, 2, 'a'], [11, 22, 'b']], [[3, 4, 'c'], [33, 44, 'd']]]</span></span><br><span class="line"><span class="string">        In [26]: sum(l, [])</span></span><br><span class="line"><span class="string">        Out[26]: [[1, 2, 'a'], [11, 22, 'b'], [3, 4, 'c'], [33, 44, 'd']]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        train_set = sum(train_set, [])</span><br><span class="line">        test_set = list()</span><br><span class="line">        <span class="comment"># fold 表示从原始数据集 dataset 提取出来的测试集</span></span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> fold:</span><br><span class="line">            row_copy = list(row)</span><br><span class="line">            row_copy[<span class="number">-1</span>] = <span class="literal">None</span> </span><br><span class="line">            test_set.append(row_copy)</span><br><span class="line">        predicted = algorithm(train_set, test_set, *args)</span><br><span class="line">        actual = [row[<span class="number">-1</span>] <span class="keyword">for</span> row <span class="keyword">in</span> fold]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算随机森林的预测结果的正确率</span></span><br><span class="line">        accuracy = accuracy_metric(actual, predicted)</span><br><span class="line">        scores.append(accuracy)</span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法：若你感兴趣可以构建完整的应用程序，从案例进行封装，也可以参考我们的代码</p>
</blockquote>
<p><a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/7.RandomForest/randomForest.py" target="_blank" rel="noopener">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/7.RandomForest/randomForest.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/7.RandomForest/randomForest.py</a></p>
<h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><h3 id="AdaBoost-adaptive-boosting-自适应-boosting-概述"><a href="#AdaBoost-adaptive-boosting-自适应-boosting-概述" class="headerlink" title="AdaBoost (adaptive boosting: 自适应 boosting) 概述"></a>AdaBoost (adaptive boosting: 自适应 boosting) 概述</h3><p><code>能否使用弱分类器和多个实例来构建一个强分类器？ 这是一个非常有趣的理论问题。</code></p>
<h3 id="AdaBoost-原理"><a href="#AdaBoost-原理" class="headerlink" title="AdaBoost 原理"></a>AdaBoost 原理</h3><blockquote>
<p>AdaBoost 工作原理</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/7.AdaBoost/adaboost_illustration.png" alt="AdaBoost 工作原理" title="AdaBoost 工作原理"></p>
<blockquote>
<p>AdaBoost 开发流程</p>
</blockquote>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">收集数据：可以使用任意方法</span><br><span class="line">准备数据：依赖于所使用的弱分类器类型，本章使用的是单层决策树，这种分类器可以处理任何数据类型。</span><br><span class="line">    当然也可以使用任意分类器作为弱分类器，第<span class="number">2</span>章到第<span class="number">6</span>章中的任一分类器都可以充当弱分类器。</span><br><span class="line">    作为弱分类器，简单分类器的效果更好。</span><br><span class="line">分析数据：可以使用任意方法。</span><br><span class="line">训练算法：AdaBoost 的大部分时间都用在训练上，分类器将多次在同一数据集上训练弱分类器。</span><br><span class="line">测试算法：计算分类的错误率。</span><br><span class="line">使用算法：通SVM一样，AdaBoost 预测两个类别中的一个。如果想把它应用到多个类别的场景，那么就要像多类 SVM 中的做法一样对 AdaBoost 进行修改。</span><br></pre></td></tr></table></figure>

<blockquote>
<p>AdaBoost 算法特点</p>
</blockquote>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="bullet">* </span>优点：泛化（由具体的、个别的扩大为一般的）错误率低，易编码，可以应用在大部分分类器上，无参数调节。</span><br><span class="line"><span class="bullet">* </span>缺点：对离群点敏感。</span><br><span class="line"><span class="bullet">* </span>适用数据类型：数值型和标称型数据。</span><br></pre></td></tr></table></figure>

<h3 id="项目案例-马疝病的预测"><a href="#项目案例-马疝病的预测" class="headerlink" title="项目案例: 马疝病的预测"></a>项目案例: 马疝病的预测</h3><blockquote>
<p>项目流程图</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/7.AdaBoost/adaboost_code-flow-chart.jpg" alt="AdaBoost代码流程图" title="AdaBoost代码流程图"></p>
<p>基于单层决策树构建弱分类器</p>
<ul>
<li>单层决策树(decision stump, 也称决策树桩)是一种简单的决策树。</li>
</ul>
<h4 id="项目概述-1"><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4><p>预测患有疝气病的马的存活问题，这里的数据包括368个样本和28个特征，疝气病是描述马胃肠痛的术语，然而，这种病并不一定源自马的胃肠问题，其他问题也可能引发疝气病，该数据集中包含了医院检测马疝气病的一些指标，有的指标比较主观，有的指标难以测量，例如马的疼痛级别。另外，除了部分指标主观和难以测量之外，该数据还存在一个问题，数据集中有30%的值是缺失的。</p>
<h4 id="开发流程-1"><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">收集数据：提供的文本文件</span><br><span class="line">准备数据：确保类别标签是+<span class="number">1</span>和<span class="number">-1</span>，而非<span class="number">1</span>和<span class="number">0</span></span><br><span class="line">分析数据：统计分析</span><br><span class="line">训练算法：在数据上，利用 adaBoostTrainDS() 函数训练出一系列的分类器</span><br><span class="line">测试算法：我们拥有两个数据集。在不采用随机抽样的方法下，我们就会对 AdaBoost 和 Logistic 回归的结果进行完全对等的比较</span><br><span class="line">使用算法：观察该例子上的错误率。不过，也可以构建一个 Web 网站，让驯马师输入马的症状然后预测马是否会死去</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据：提供的文本文件</p>
</blockquote>
<p>训练数据：horseColicTraining.txt<br/><br>测试数据：horseColicTest.txt</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">2.000000</span>	<span class="number">1.000000</span>	<span class="number">38.500000</span>	<span class="number">66.000000</span>	<span class="number">28.000000</span>	<span class="number">3.000000</span>	<span class="number">3.000000</span>	<span class="number">0.000000</span>	<span class="number">2.000000</span>	<span class="number">5.000000</span>	<span class="number">4.000000</span>	<span class="number">4.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">3.000000</span>	<span class="number">5.000000</span>	<span class="number">45.000000</span>	<span class="number">8.400000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">-1.000000</span></span><br><span class="line"><span class="number">1.000000</span>	<span class="number">1.000000</span>	<span class="number">39.200000</span>	<span class="number">88.000000</span>	<span class="number">20.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">4.000000</span>	<span class="number">1.000000</span>	<span class="number">3.000000</span>	<span class="number">4.000000</span>	<span class="number">2.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">4.000000</span>	<span class="number">2.000000</span>	<span class="number">50.000000</span>	<span class="number">85.000000</span>	<span class="number">2.000000</span>	<span class="number">2.000000</span>	<span class="number">-1.000000</span></span><br><span class="line"><span class="number">2.000000</span>	<span class="number">1.000000</span>	<span class="number">38.300000</span>	<span class="number">40.000000</span>	<span class="number">24.000000</span>	<span class="number">1.000000</span>	<span class="number">1.000000</span>	<span class="number">3.000000</span>	<span class="number">1.000000</span>	<span class="number">3.000000</span>	<span class="number">3.000000</span>	<span class="number">1.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">1.000000</span>	<span class="number">1.000000</span>	<span class="number">33.000000</span>	<span class="number">6.700000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">1.000000</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>准备数据：确保类别标签是+1和-1，而非1和0</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="comment"># 获取 feature 的数量, 便于获取</span></span><br><span class="line">    numFeat = len(open(fileName).readline().split(<span class="string">'\t'</span>))</span><br><span class="line">    dataArr = []</span><br><span class="line">    labelArr = []</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = []</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeat<span class="number">-1</span>):</span><br><span class="line">            lineArr.append(float(curLine[i]))</span><br><span class="line">        dataArr.append(lineArr)</span><br><span class="line">        labelArr.append(float(curLine[<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataArr, labelArr</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据：统计分析</p>
</blockquote>
<p>过拟合(overfitting, 也称为过学习)</p>
<ul>
<li>发现测试错误率在达到一个最小值之后有开始上升，这种现象称为过拟合。</li>
</ul>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/7.AdaBoost/%E8%BF%87%E6%8B%9F%E5%90%88.png" alt="过拟合"></p>
<ul>
<li>通俗来说：就是把一些噪音数据也拟合进去的，如下图。</li>
</ul>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/7.AdaBoost/%E8%BF%87%E6%8B%9F%E5%90%88%E5%9B%BE%E8%A7%A3.png" alt="过拟合"></p>
<blockquote>
<p>训练算法：在数据上，利用 adaBoostTrainDS() 函数训练出一系列的分类器</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaBoostTrainDS</span><span class="params">(dataArr, labelArr, numIt=<span class="number">40</span>)</span>:</span></span><br><span class="line">    <span class="string">"""adaBoostTrainDS(adaBoost训练过程放大)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataArr   特征标签集合</span></span><br><span class="line"><span class="string">        labelArr  分类标签集合</span></span><br><span class="line"><span class="string">        numIt     实例数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        weakClassArr  弱分类器的集合</span></span><br><span class="line"><span class="string">        aggClassEst   预测的分类结果值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    weakClassArr = []</span><br><span class="line">    m = shape(dataArr)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 初始化 D，设置每个样本的权重值，平均分为m份</span></span><br><span class="line">    D = mat(ones((m, <span class="number">1</span>))/m)</span><br><span class="line">    aggClassEst = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numIt):</span><br><span class="line">        <span class="comment"># 得到决策树的模型</span></span><br><span class="line">        bestStump, error, classEst = buildStump(dataArr, labelArr, D)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># alpha目的主要是计算每一个分类器实例的权重(组合就是分类结果)</span></span><br><span class="line">        <span class="comment"># 计算每个分类器的alpha权重值</span></span><br><span class="line">        alpha = float(<span class="number">0.5</span>*log((<span class="number">1.0</span>-error)/max(error, <span class="number">1e-16</span>)))</span><br><span class="line">        bestStump[<span class="string">'alpha'</span>] = alpha</span><br><span class="line">        <span class="comment"># store Stump Params in Array</span></span><br><span class="line">        weakClassArr.append(bestStump)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">print</span> <span class="string">"alpha=%s, classEst=%s, bestStump=%s, error=%s "</span> % (alpha, classEst.T, bestStump, error)</span><br><span class="line">        <span class="comment"># 分类正确：乘积为1，不会影响结果，-1主要是下面求e的-alpha次方</span></span><br><span class="line">        <span class="comment"># 分类错误：乘积为 -1，结果会受影响，所以也乘以 -1</span></span><br><span class="line">        expon = multiply(<span class="number">-1</span>*alpha*mat(labelArr).T, classEst)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'(-1取反)预测值expon='</span>, expon.T</span><br><span class="line">        <span class="comment"># 计算e的expon次方，然后计算得到一个综合的概率的值</span></span><br><span class="line">        <span class="comment"># 结果发现： 判断错误的样本，D中相对应的样本权重值会变大。</span></span><br><span class="line">        D = multiply(D, exp(expon))</span><br><span class="line">        D = D/D.sum()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 预测的分类结果值，在上一轮结果的基础上，进行加和操作</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'当前的分类结果：'</span>, alpha*classEst.T</span><br><span class="line">        aggClassEst += alpha*classEst</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"叠加后的分类结果aggClassEst: "</span>, aggClassEst.T</span><br><span class="line">        <span class="comment"># sign 判断正为1， 0为0， 负为-1，通过最终加和的权重值，判断符号。</span></span><br><span class="line">        <span class="comment"># 结果为：错误的样本标签集合，因为是 !=,那么结果就是0 正, 1 负</span></span><br><span class="line">        aggErrors = multiply(sign(aggClassEst) != mat(labelArr).T, ones((m, <span class="number">1</span>)))</span><br><span class="line">        errorRate = aggErrors.sum()/m</span><br><span class="line">        <span class="comment"># print "total error=%s " % (errorRate)</span></span><br><span class="line">        <span class="keyword">if</span> errorRate == <span class="number">0.0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> weakClassArr, aggClassEst</span><br></pre></td></tr></table></figure>

<figure class="highlight processing"><table><tr><td class="code"><pre><span class="line">发现：</span><br><span class="line"><span class="built_in">alpha</span> （模型权重）目的主要是计算每一个分类器实例的权重(加和就是分类结果)</span><br><span class="line">  分类的权重值：最大的值= <span class="built_in">alpha</span> 的加和，最小值=-最大值</span><br><span class="line">D （样本权重）的目的是为了计算错误概率： weightedError = D.T*errArr，求最佳分类器</span><br><span class="line">  样本的权重值：如果一个值误判的几率越小，那么 D 的样本权重越小</span><br></pre></td></tr></table></figure>

<p><img src="http://data.apachecn.org/img/AiLearning/ml/7.AdaBoost/adaboost_alpha.png" alt="AdaBoost算法权重计算公式" title="AdaBoost算法权重计算公式"></p>
<blockquote>
<p>测试算法：我们拥有两个数据集。在不采用随机抽样的方法下，我们就会对 AdaBoost 和 Logistic 回归的结果进行完全对等的比较。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaClassify</span><span class="params">(datToClass, classifierArr)</span>:</span></span><br><span class="line">    <span class="string">"""adaClassify(ada分类测试)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        datToClass     多个待分类的样例</span></span><br><span class="line"><span class="string">        classifierArr  弱分类器的集合</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        sign(aggClassEst) 分类结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># do stuff similar to last aggClassEst in adaBoostTrainDS</span></span><br><span class="line">    dataMat = mat(datToClass)</span><br><span class="line">    m = shape(dataMat)[<span class="number">0</span>]</span><br><span class="line">    aggClassEst = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 循环 多个分类器</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classifierArr)):</span><br><span class="line">        <span class="comment"># 前提： 我们已经知道了最佳的分类器的实例</span></span><br><span class="line">        <span class="comment"># 通过分类器来核算每一次的分类结果，然后通过alpha*每一次的结果 得到最后的权重加和的值。</span></span><br><span class="line">        classEst = stumpClassify(dataMat, classifierArr[i][<span class="string">'dim'</span>], classifierArr[i][<span class="string">'thresh'</span>], classifierArr[i][<span class="string">'ineq'</span>])</span><br><span class="line">        aggClassEst += classifierArr[i][<span class="string">'alpha'</span>]*classEst</span><br><span class="line">    <span class="keyword">return</span> sign(aggClassEst)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法：观察该例子上的错误率。不过，也可以构建一个 Web 网站，让驯马师输入马的症状然后预测马是否会死去。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 马疝病数据集</span></span><br><span class="line"><span class="comment"># 训练集合</span></span><br><span class="line">dataArr, labelArr = loadDataSet(<span class="string">"data/7.AdaBoost/horseColicTraining2.txt"</span>)</span><br><span class="line">weakClassArr, aggClassEst = adaBoostTrainDS(dataArr, labelArr, <span class="number">40</span>)</span><br><span class="line"><span class="keyword">print</span> weakClassArr, <span class="string">'\n-----\n'</span>, aggClassEst.T</span><br><span class="line"><span class="comment"># 计算ROC下面的AUC的面积大小</span></span><br><span class="line">plotROC(aggClassEst.T, labelArr)</span><br><span class="line"><span class="comment"># 测试集合</span></span><br><span class="line">dataArrTest, labelArrTest = loadDataSet(<span class="string">"data/7.AdaBoost/horseColicTest2.txt"</span>)</span><br><span class="line">m = shape(dataArrTest)[<span class="number">0</span>]</span><br><span class="line">predicting10 = adaClassify(dataArrTest, weakClassArr)</span><br><span class="line">errArr = mat(ones((m, <span class="number">1</span>)))</span><br><span class="line"><span class="comment"># 测试：计算总样本数，错误样本数，错误率</span></span><br><span class="line"><span class="keyword">print</span> m, errArr[predicting10 != mat(labelArrTest).T].sum(), errArr[predicting10 != mat(labelArrTest).T].sum()/m</span><br></pre></td></tr></table></figure>

<p><a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/7.AdaBoost/adaboost.py" target="_blank" rel="noopener">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/7.AdaBoost/adaboost.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/7.AdaBoost/adaboost.py</a></p>
<h4 id="要点补充"><a href="#要点补充" class="headerlink" title="要点补充"></a>要点补充</h4><blockquote>
<p>非均衡现象：</p>
</blockquote>
<p><code>在分类器训练时，正例数目和反例数目不相等（相差很大）。或者发生在正负例分类错误的成本不同的时候。</code></p>
<ul>
<li>判断马是否能继续生存(不可误杀)</li>
<li>过滤垃圾邮件(不可漏判)</li>
<li>不能放过传染病的人</li>
<li>不能随便认为别人犯罪</li>
</ul>
<p>我们有多种方法来处理这个问题： 具体可参考<a href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/" target="_blank" rel="noopener">此链接</a></p>
<p>再结合书中的方法，可以归为八大类：</p>
<h5 id="1-能否收集到更多的数据？"><a href="#1-能否收集到更多的数据？" class="headerlink" title="1.能否收集到更多的数据？"></a>1.能否收集到更多的数据？</h5><p>这个措施往往被人们所忽略，被认为很蠢。但是更大的数据集更能体现样本的分布，多样性。</p>
<h5 id="2-尝试使用其他的评价指标"><a href="#2-尝试使用其他的评价指标" class="headerlink" title="2.尝试使用其他的评价指标"></a>2.尝试使用其他的评价指标</h5><p>Accuracy 或者error rate 不能用于非均衡的数据集。这会误导人。这时候可以尝试其他的评价指标。</p>
<p>Confusion Matrix 混淆矩阵：使用一个表格对分类器所预测的类别与其真实的类别的样本统计，分别为：TP、FN、FP与TN。</p>
<p>Precision：精确度</p>
<p>Recall： 召回率</p>
<p>F1 Score (or F-Score)： 精确度和召回率的加权平均</p>
<p>或者使用</p>
<p>Kappa (Cohen’s kappa)</p>
<p>ROC Curves</p>
<blockquote>
<p>ROC 评估方法</p>
</blockquote>
<ul>
<li>ROC 曲线: 最佳的分类器应该尽可能地处于左上角</li>
</ul>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/7.AdaBoost/ROC%E6%9B%B2%E7%BA%BF.png" alt="ROC曲线"></p>
<ul>
<li><p>对不同的 ROC 曲线进行比较的一个指标是曲线下的面积(Area Unser the Curve, AUC). </p>
</li>
<li><p>AUC 给出的是分类器的平均性能值，当然它并不能完全代替对整条曲线的观察。</p>
</li>
<li><p>一个完美分类器的 AUC 为1，而随机猜测的 AUC 则为0.5。</p>
</li>
</ul>
<h5 id="3-尝试对样本重抽样"><a href="#3-尝试对样本重抽样" class="headerlink" title="3.尝试对样本重抽样"></a>3.尝试对样本重抽样</h5><p>欠抽样(undersampling)或者过抽样(oversampling)</p>
<pre><code>- 欠抽样: 意味着删除样例
- 过抽样: 意味着复制样例(重复使用)</code></pre><p>对大类进行欠抽样</p>
<p>对小类进行过抽样</p>
<p>或者结合上述两种方法进行抽样</p>
<p>一些经验法则：</p>
<ul>
<li><p>考虑样本（超过1万、十万甚至更多）进行欠采样，即删除部分样本；</p>
</li>
<li><p>考虑样本（不足1为甚至更少）进行过采样，即添加部分样本的副本；</p>
</li>
<li><p>考虑尝试随机采样与非随机采样两种采样方法；</p>
</li>
<li><p>考虑对各类别尝试不同的采样比例，不一定是1:1</p>
</li>
<li><p>考虑同时使用过采样与欠采样</p>
</li>
</ul>
<h5 id="4-尝试产生人工生成的样本"><a href="#4-尝试产生人工生成的样本" class="headerlink" title="4.尝试产生人工生成的样本"></a>4.尝试产生人工生成的样本</h5><p>一种简单的方法就是随机抽样小类样本的属性（特征）来组成新的样本即属性值随机采样。你可以根据经验进行抽样，可以使用其他方式比如朴素贝叶斯方法假设各属性之间互相独立进行采样，这样便可得到更多的数据，但是无法保证属性之间的非线性关系。</p>
<p>当然也有系统性的算法。最常用的SMOTE(Synthetic Minority Over-Sampling Technique)。 顾名思义，这是一种over sampling（过抽样）的方式。它是产生人为的样本而不是制造样本副本。这个算法是选取2个或者2个以上相似的样本（根据距离度量 distance measure），然后每次选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声(每次只处理一个属性)。这样就构造了更多的新生数据。具体可以参见<a href="http://www.jair.org/papers/paper953.html" target="_blank" rel="noopener">原始论文</a>。</p>
<p>python实现可以查阅<a href="https://github.com/scikit-learn-contrib/imbalanced-learn" target="_blank" rel="noopener">UnbalancedDataset</a></p>
<h5 id="5-尝试不同的算法"><a href="#5-尝试不同的算法" class="headerlink" title="5.尝试不同的算法"></a>5.尝试不同的算法</h5><p>强烈建议不要在每个问题上使用你最喜欢的算法。虽然这个算法带来较好的效果，但是它也会蒙蔽你观察数据内蕴含的其他的信息。至少你得在同一个问题上试试各种算法。具体可以参阅<a href="https://machinelearningmastery.com/why-you-should-be-spot-checking-algorithms-on-your-machine-learning-problems/" target="_blank" rel="noopener">Why you should be Spot-Checking Algorithms on your Machine Learning Problems</a></p>
<p>比如说，决策树经常在非均衡数据集上表现良好。创建分类树时候使用基于类变量的划分规则强制使类别表达出来。如果有疑惑，可以尝试一些流行的决策树，比如, C4.5, C5.0, CART 和 Random Forrest。</p>
<h5 id="6-尝试使用惩罚的模型"><a href="#6-尝试使用惩罚的模型" class="headerlink" title="6.尝试使用惩罚的模型"></a>6.尝试使用惩罚的模型</h5><p>你可以使用同种算法但是以不同的角度对待这个问题。</p>
<p>惩罚的模型就是对于不同的分类错误给予不同的代价（惩罚）。比如对于错分的小类给予更高的代价。这种方式会使模型偏差，更加关注小类。</p>
<p>通常来说这种代价/惩罚或者比重在学习中算法是特定的。比如使用代价函数来实现：</p>
<blockquote>
<p>代价函数</p>
</blockquote>
<ul>
<li>基于代价函数的分类器决策控制：<code>TP*(-5)+FN*1+FP*50+TN*0</code></li>
</ul>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/7.AdaBoost/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.png" alt="代价函数"></p>
<p>这种方式叫做 cost sensitive learning，Weka 中相应的框架可以实现叫<a href="http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/CostSensitiveClassifier.html" target="_blank" rel="noopener">CostSensitiveClassifier</a></p>
<p>如果当你只能使用特定算法而且无法重抽样，或者模型效果不行，这时候使用惩罚（penalization）是可行的方法。这提供另外一种方式来“平衡”类别。但是设定惩罚函数/代价函数是比较复杂的。最好还是尝试不同的代价函数组合来得到最优效果。</p>
<h5 id="7-尝试使用不同的角度"><a href="#7-尝试使用不同的角度" class="headerlink" title="7.尝试使用不同的角度"></a>7.尝试使用不同的角度</h5><p>其实有很多研究关于非均衡数据。他们有自己的算法，度量，术语。</p>
<p>从它们的角度看看你的问题，思考你的问题，说不定会有新的想法。</p>
<p>两个领域您可以考虑： anomaly detection(异常值检测) 和 change detection（变化趋势检测）。</p>
<p>Anomaly dectection 就是检测稀有事件。 比如通过机器震动来识别机器谷中或者根据一系列系统的调用来检测恶意操作。与常规操作相比，这些事件是罕见的。</p>
<p>把小类想成异常类这种转变可能会帮助你想到新办法来分类数据样本。</p>
<p>change detection 变化趋势检测类似于异常值检测。但是他不是寻找异常值而是寻找变化或区别。比如通过使用模式或者银行交易记录来观察用户行为转变。</p>
<p>这些两种转变可能会给你新的方式去思考你的问题和新的技术去尝试。</p>
<h5 id="8-尝试去创新"><a href="#8-尝试去创新" class="headerlink" title="8.尝试去创新"></a>8.尝试去创新</h5><p>仔细思考你的问题然后想想看如何将这问题细分为几个更切实际的小问题。</p>
<p>比如：</p>
<p>将你的大类分解成多个较小的类；</p>
<p>使用One Class分类器（看待成异常点检测）；</p>
<p>对数据集进行抽样成多个数据集，使用集成方式，训练多个分类器，然后联合这些分类器进行分类；</p>
<p>这只是一个例子。更多的可以参阅<a href="http://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set" target="_blank" rel="noopener">In classification, how do you handle an unbalanced training set?</a> 和<a href="https://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/" target="_blank" rel="noopener">Classification when 80% of my training set is of one class</a></p>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a> </li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第8章 预测数值型数据：回归</title>
    <url>/2020/06/02/8.%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h2 id="回归（Regression）-概述"><a href="#回归（Regression）-概述" class="headerlink" title="回归（Regression） 概述"></a>回归（Regression） 概述</h2><p><code>我们前边提到的分类的目标变量是标称型数据，而回归则是对连续型的数据做出处理，回归的目的是预测数值型数据的目标值。</code></p>
<h2 id="回归-场景"><a href="#回归-场景" class="headerlink" title="回归 场景"></a>回归 场景</h2><p>回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。</p>
<a id="more"></a>
<p>假如你想要预测兰博基尼跑车的功率大小，可能会这样计算:</p>
<p>HorsePower = 0.0015 * annualSalary - 0.99 * hoursListeningToPublicRadio</p>
<p>这就是所谓的 <code>回归方程(regression equation)</code>，其中的 0.0015 和 -0.99 称作 <code>回归系数（regression weights）</code>，求这些回归系数的过程就是回归。一旦有了这些回归系数，再给定输入，做预测就非常容易了。具体的做法是用回归系数乘以输入值，再将结果全部加在一起，就得到了预测值。我们这里所说的，回归系数是一个向量，输入也是向量，这些运算也就是求出二者的内积。</p>
<p>说到回归，一般都是指 <code>线性回归(linear regression)</code>。线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出。</p>
<p>补充：<br>线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。这样就可以表达特征与结果之间的非线性关系。</p>
<h2 id="回归-原理"><a href="#回归-原理" class="headerlink" title="回归 原理"></a>回归 原理</h2><h3 id="1、线性回归"><a href="#1、线性回归" class="headerlink" title="1、线性回归"></a>1、线性回归</h3><p>我们应该怎样从一大堆数据里求出回归方程呢？ 假定输入数据存放在矩阵 x 中，而回归系数存放在向量 w 中。那么对于给定的数据 X1，预测结果将会通过 Y = X1^T w 给出。现在的问题是，手里有一些 X 和对应的 y，怎样才能找到 w 呢？一个常用的方法就是找出使误差最小的 w 。这里的误差是指预测 y 值和真实 y 值之间的差值，使用该误差的简单累加将使得正差值和负差值相互抵消，所以我们采用平方误差（实际上就是我们通常所说的最小二乘法）。</p>
<p>平方误差可以写做（其实我们是使用这个函数作为 loss function）: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_18.png" alt="平方误差"></p>
<p>用矩阵表示还可以写做 <img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_19.png" alt="平方误差_2"> 。如果对 w 求导，得到 <img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_20.png" alt="平方误差_3"> ，令其等于零，解出 w 如下（具体求导过程为: <a href="http://blog.csdn.net/nomadlx53/article/details/50849941" target="_blank" rel="noopener">http://blog.csdn.net/nomadlx53/article/details/50849941</a> ）:</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_1.png" alt="回归系数的最佳估计计算公式"></p>
<h4 id="1-1、线性回归-须知概念"><a href="#1-1、线性回归-须知概念" class="headerlink" title="1.1、线性回归 须知概念"></a>1.1、线性回归 须知概念</h4><h5 id="1-1-1、矩阵求逆"><a href="#1-1-1、矩阵求逆" class="headerlink" title="1.1.1、矩阵求逆"></a>1.1.1、矩阵求逆</h5><p>因为我们在计算回归方程的回归系数时，用到的计算公式如下: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_1.png" alt="回归系数的最佳估计计算公式"></p>
<p>需要对矩阵求逆，因此这个方程只在逆矩阵存在的时候适用，我们在程序代码中对此作出判断。<br>判断矩阵是否可逆的一个可选方案是: </p>
<p>判断矩阵的行列式是否为 0，若为 0 ，矩阵就不存在逆矩阵，不为 0 的话，矩阵才存在逆矩阵。</p>
<h5 id="1-1-2、最小二乘法"><a href="#1-1-2、最小二乘法" class="headerlink" title="1.1.2、最小二乘法"></a>1.1.2、最小二乘法</h5><p>最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。</p>
<h4 id="1-2、线性回归-工作原理"><a href="#1-2、线性回归-工作原理" class="headerlink" title="1.2、线性回归 工作原理"></a>1.2、线性回归 工作原理</h4><figure class="highlight gml"><table><tr><td class="code"><pre><span class="line">读入数据，将数据特征<span class="symbol">x</span>、特征标签<span class="symbol">y</span>存储在矩阵<span class="symbol">x</span>、<span class="symbol">y</span>中</span><br><span class="line">验证 <span class="symbol">x</span>^Tx 矩阵是否可逆</span><br><span class="line">使用最小二乘法求得 回归系数 w 的最佳估计</span><br></pre></td></tr></table></figure>

<h4 id="1-3、线性回归-开发流程"><a href="#1-3、线性回归-开发流程" class="headerlink" title="1.3、线性回归 开发流程"></a>1.3、线性回归 开发流程</h4><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 采用任意方法收集数据</span></span><br><span class="line"><span class="section">准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据</span></span><br><span class="line"><span class="section">分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比</span></span><br><span class="line"><span class="section">训练算法: 找到回归系数</span></span><br><span class="line"><span class="section">测试算法: 使用 R^2 或者预测值和数据的拟合度，来分析模型的效果</span></span><br><span class="line"><span class="section">使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签</span></span><br></pre></td></tr></table></figure>

<h4 id="1-4、线性回归-算法特点"><a href="#1-4、线性回归-算法特点" class="headerlink" title="1.4、线性回归 算法特点"></a>1.4、线性回归 算法特点</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">优点：结果易于理解，计算上不复杂。</span><br><span class="line">缺点：对非线性的数据拟合不好。</span><br><span class="line">适用于数据类型：数值型和标称型数据。</span><br></pre></td></tr></table></figure>

<h4 id="1-5、线性回归-项目案例"><a href="#1-5、线性回归-项目案例" class="headerlink" title="1.5、线性回归 项目案例"></a>1.5、线性回归 项目案例</h4><p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a></p>
<h5 id="1-5-1、线性回归-项目概述"><a href="#1-5-1、线性回归-项目概述" class="headerlink" title="1.5.1、线性回归 项目概述"></a>1.5.1、线性回归 项目概述</h5><p>根据下图中的点，找出该数据的最佳拟合直线。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_2.png" alt="线性回归数据示例图" title="线性回归数据示例图"></p>
<p>数据格式为: </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">x0          x1          y </span><br><span class="line"><span class="number">1.000000</span>	<span class="number">0.067732</span>	<span class="number">3.176513</span></span><br><span class="line"><span class="number">1.000000</span>	<span class="number">0.427810</span>	<span class="number">3.816464</span></span><br><span class="line"><span class="number">1.000000</span>	<span class="number">0.995731</span>	<span class="number">4.550095</span></span><br><span class="line"><span class="number">1.000000</span>	<span class="number">0.738336</span>	<span class="number">4.256571</span></span><br></pre></td></tr></table></figure>

<h5 id="1-5-2、线性回归-编写代码"><a href="#1-5-2、线性回归-编写代码" class="headerlink" title="1.5.2、线性回归 编写代码"></a>1.5.2、线性回归 编写代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span>                 </span><br><span class="line">    <span class="string">""" 加载数据</span></span><br><span class="line"><span class="string">        解析以tab键分隔的文件中的浮点数</span></span><br><span class="line"><span class="string">    Returns：</span></span><br><span class="line"><span class="string">        dataMat ：  feature 对应的数据集</span></span><br><span class="line"><span class="string">        labelMat ： feature 对应的分类标签，即类别标签</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 获取样本特征的总数，不算最后的目标变量 </span></span><br><span class="line">    numFeat = len(open(fileName).readline().split(<span class="string">'\t'</span>)) - <span class="number">1</span> </span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment"># 读取每一行</span></span><br><span class="line">        lineArr =[]</span><br><span class="line">        <span class="comment"># 删除一行中以tab分隔的数据前后的空白符号</span></span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="comment"># i 从0到2，不包括2 </span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeat):</span><br><span class="line">            <span class="comment"># 将数据添加到lineArr List中，每一行数据测试数据组成一个行向量           </span></span><br><span class="line">            lineArr.append(float(curLine[i]))</span><br><span class="line">            <span class="comment"># 将测试数据的输入数据部分存储到dataMat 的List中</span></span><br><span class="line">        dataMat.append(lineArr)</span><br><span class="line">        <span class="comment"># 将每一行的最后一个数据，即类别，或者叫目标变量存储到labelMat List中</span></span><br><span class="line">        labelMat.append(float(curLine[<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standRegres</span><span class="params">(xArr,yArr)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Description：</span></span><br><span class="line"><span class="string">        线性回归</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        xArr ：输入的样本数据，包含每个样本数据的 feature</span></span><br><span class="line"><span class="string">        yArr ：对应于输入数据的类别标签，也就是每个样本对应的目标变量</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        ws：回归系数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># mat()函数将xArr，yArr转换为矩阵 mat().T 代表的是对矩阵进行转置操作</span></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    <span class="comment"># 矩阵乘法的条件是左矩阵的列数等于右矩阵的行数</span></span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    <span class="comment"># 因为要用到xTx的逆矩阵，所以事先需要确定计算得到的xTx是否可逆，条件是矩阵的行列式不为0</span></span><br><span class="line">    <span class="comment"># linalg.det() 函数是用来求得矩阵的行列式的，如果矩阵的行列式为0，则这个矩阵是不可逆的，就无法进行接下来的运算                   </span></span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"This matrix is singular, cannot do inverse"</span> </span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 最小二乘法</span></span><br><span class="line">    <span class="comment"># http://cwiki.apachecn.org/pages/viewpage.action?pageId=5505133</span></span><br><span class="line">    <span class="comment"># 书中的公式，求得w的最优解</span></span><br><span class="line">    ws = xTx.I * (xMat.T*yMat)            </span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression1</span><span class="params">()</span>:</span></span><br><span class="line">    xArr, yArr = loadDataSet(<span class="string">"data/8.Regression/data.txt"</span>)</span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr)</span><br><span class="line">    ws = standRegres(xArr, yArr)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)               <span class="comment">#add_subplot(349)函数的参数的意思是，将画布分成3行4列图像画在从左到右从上到下第9块</span></span><br><span class="line">    ax.scatter(xMat[:, <span class="number">1</span>].flatten(), yMat.T[:, <span class="number">0</span>].flatten().A[<span class="number">0</span>]) <span class="comment">#scatter 的x是xMat中的第二列，y是yMat的第一列</span></span><br><span class="line">    xCopy = xMat.copy() </span><br><span class="line">    xCopy.sort(<span class="number">0</span>)</span><br><span class="line">    yHat = xCopy * ws</span><br><span class="line">    ax.plot(xCopy[:, <span class="number">1</span>], yHat)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<h5 id="1-5-3、线性回归-拟合效果"><a href="#1-5-3、线性回归-拟合效果" class="headerlink" title="1.5.3、线性回归 拟合效果"></a>1.5.3、线性回归 拟合效果</h5><p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_3.png" alt="线性回归数据效果图" title="线性回归数据效果图"></p>
<h3 id="2、局部加权线性回归"><a href="#2、局部加权线性回归" class="headerlink" title="2、局部加权线性回归"></a>2、局部加权线性回归</h3><p>线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有最小均方差的无偏估计。显而易见，如果模型欠拟合将不能取得最好的预测效果。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。</p>
<p>一个方法是局部加权线性回归（Locally Weighted Linear Regression，LWLR）。在这个算法中，我们给预测点附近的每个点赋予一定的权重，然后与 线性回归 类似，在这个子集上基于最小均方误差来进行普通的回归。我们需要最小化的目标函数大致为:</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_21.png" alt="局部加权线性回归回归系数公式"></p>
<p>目标函数中 w 为权重，不是回归系数。与 kNN 一样，这种算法每次预测均需要事先选取出对应的数据子集。该算法解出回归系数 w 的形式如下: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_4.png" alt="局部加权线性回归回归系数公式"></p>
<p>其中 W 是一个矩阵，用来给每个数据点赋予权重。$\hat{w}$ 则为回归系数。 这两个是不同的概念，请勿混用。</p>
<p>LWLR 使用 “核”（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_23.png" alt="局部加权线性回归高斯核"></p>
<p>这样就构建了一个只含对角元素的权重矩阵 <strong>w</strong>，并且点 x 与 x(i) 越近，w(i) 将会越大。上述公式中包含一个需要用户指定的参数 k ，它决定了对附近的点赋予多大的权重，这也是使用 LWLR 时唯一需要考虑的参数，下面的图给出了参数 k 与权重的关系。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_6.png" alt="参数k与权重的关系"></p>
<p>上面的图是 每个点的权重图（假定我们正预测的点是 x = 0.5），最上面的图是原始数据集，第二个图显示了当 k = 0.5 时，大部分的数据都用于训练回归模型；而最下面的图显示当 k=0.01 时，仅有很少的局部点被用于训练回归模型。</p>
<h4 id="2-1、局部加权线性回归-工作原理"><a href="#2-1、局部加权线性回归-工作原理" class="headerlink" title="2.1、局部加权线性回归 工作原理"></a>2.1、局部加权线性回归 工作原理</h4><figure class="highlight gml"><table><tr><td class="code"><pre><span class="line">读入数据，将数据特征<span class="symbol">x</span>、特征标签<span class="symbol">y</span>存储在矩阵<span class="symbol">x</span>、<span class="symbol">y</span>中</span><br><span class="line">利用高斯核构造一个权重矩阵 W，对预测点附近的点施加权重</span><br><span class="line">验证 X^TWX 矩阵是否可逆</span><br><span class="line">使用最小二乘法求得 回归系数 w 的最佳估计</span><br></pre></td></tr></table></figure>

<h4 id="2-2、局部加权线性回归-项目案例"><a href="#2-2、局部加权线性回归-项目案例" class="headerlink" title="2.2、局部加权线性回归 项目案例"></a>2.2、局部加权线性回归 项目案例</h4><p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a></p>
<h5 id="2-2-1、局部加权线性回归-项目概述"><a href="#2-2-1、局部加权线性回归-项目概述" class="headerlink" title="2.2.1、局部加权线性回归 项目概述"></a>2.2.1、局部加权线性回归 项目概述</h5><p>我们仍然使用上面 线性回归 的数据集，对这些点进行一个 局部加权线性回归 的拟合。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_2.png" alt="局部加权线性回归数据示例图"></p>
<p>数据格式为: </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1.000000</span>	<span class="number">0.067732</span>	<span class="number">3.176513</span></span><br><span class="line"><span class="number">1.000000</span>	<span class="number">0.427810</span>	<span class="number">3.816464</span></span><br><span class="line"><span class="number">1.000000</span>	<span class="number">0.995731</span>	<span class="number">4.550095</span></span><br><span class="line"><span class="number">1.000000</span>	<span class="number">0.738336</span>	<span class="number">4.256571</span></span><br></pre></td></tr></table></figure>

<h5 id="2-2-2、局部加权线性回归-编写代码"><a href="#2-2-2、局部加权线性回归-编写代码" class="headerlink" title="2.2.2、局部加权线性回归 编写代码"></a>2.2.2、局部加权线性回归 编写代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    <span class="comment"># 局部加权线性回归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlr</span><span class="params">(testPoint,xArr,yArr,k=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Description：</span></span><br><span class="line"><span class="string">            局部加权线性回归，在待预测点附近的每个点赋予一定的权重，在子集上基于最小均方差来进行普通的回归。</span></span><br><span class="line"><span class="string">        Args：</span></span><br><span class="line"><span class="string">            testPoint：样本点</span></span><br><span class="line"><span class="string">            xArr：样本的特征数据，即 feature</span></span><br><span class="line"><span class="string">            yArr：每个样本对应的类别标签，即目标变量</span></span><br><span class="line"><span class="string">            k:关于赋予权重矩阵的核的一个参数，与权重的衰减速率有关</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            testPoint * ws：数据点与具有权重的系数相乘得到的预测点</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            这其中会用到计算权重的公式，w = e^((x^((i))-x) / -2k^2)</span></span><br><span class="line"><span class="string">            理解：x为某个预测点，x^((i))为样本点，样本点距离预测点越近，贡献的误差越大（权值越大），越远则贡献的误差越小（权值越小）。</span></span><br><span class="line"><span class="string">            关于预测点的选取，在我的代码中取的是样本点。其中k是带宽参数，控制w（钟形函数）的宽窄程度，类似于高斯函数的标准差。</span></span><br><span class="line"><span class="string">            算法思路：假设预测点取样本点中的第i个样本点（共m个样本点），遍历1到m个样本点（含第i个），算出每一个样本点与预测点的距离，</span></span><br><span class="line"><span class="string">            也就可以计算出每个样本贡献误差的权值，可以看出w是一个有m个元素的向量（写成对角阵形式）。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># mat() 函数是将array转换为矩阵的函数， mat().T 是转换为矩阵之后，再进行转置操作</span></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    <span class="comment"># 获得xMat矩阵的行数</span></span><br><span class="line">    m = shape(xMat)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># eye()返回一个对角线元素为1，其他元素为0的二维数组，创建权重矩阵weights，该矩阵为每个样本点初始化了一个权重                   </span></span><br><span class="line">    weights = mat(eye((m)))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># testPoint 的形式是 一个行向量的形式</span></span><br><span class="line">        <span class="comment"># 计算 testPoint 与输入样本点之间的距离，然后下面计算出每个样本贡献误差的权值</span></span><br><span class="line">        diffMat = testPoint - xMat[j,:]</span><br><span class="line">        <span class="comment"># k控制衰减的速度</span></span><br><span class="line">        weights[j,j] = exp(diffMat*diffMat.T/(<span class="number">-2.0</span>*k**<span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 根据矩阵乘法计算 xTx ，其中的 weights 矩阵是样本点对应的权重矩阵</span></span><br><span class="line">    xTx = xMat.T * (weights * xMat)</span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"This matrix is singular, cannot do inverse"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 计算出回归系数的一个估计</span></span><br><span class="line">    ws = xTx.I * (xMat.T * (weights * yMat))</span><br><span class="line">    <span class="keyword">return</span> testPoint * ws</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlrTest</span><span class="params">(testArr,xArr,yArr,k=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Description：</span></span><br><span class="line"><span class="string">            测试局部加权线性回归，对数据集中每个点调用 lwlr() 函数</span></span><br><span class="line"><span class="string">        Args：</span></span><br><span class="line"><span class="string">            testArr：测试所用的所有样本点</span></span><br><span class="line"><span class="string">            xArr：样本的特征数据，即 feature</span></span><br><span class="line"><span class="string">            yArr：每个样本对应的类别标签，即目标变量</span></span><br><span class="line"><span class="string">            k：控制核函数的衰减速率</span></span><br><span class="line"><span class="string">        Returns：</span></span><br><span class="line"><span class="string">            yHat：预测点的估计值</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 得到样本点的总数</span></span><br><span class="line">    m = shape(testArr)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 构建一个全部都是 0 的 1 * m 的矩阵</span></span><br><span class="line">    yHat = zeros(m)</span><br><span class="line">    <span class="comment"># 循环所有的数据点，并将lwlr运用于所有的数据点 </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        yHat[i] = lwlr(testArr[i],xArr,yArr,k)</span><br><span class="line">    <span class="comment"># 返回估计值</span></span><br><span class="line">    <span class="keyword">return</span> yHat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlrTestPlot</span><span class="params">(xArr,yArr,k=<span class="number">1.0</span>)</span>:</span>  </span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Description:</span></span><br><span class="line"><span class="string">            首先将 X 排序，其余的都与lwlrTest相同，这样更容易绘图</span></span><br><span class="line"><span class="string">        Args：</span></span><br><span class="line"><span class="string">            xArr：样本的特征数据，即 feature</span></span><br><span class="line"><span class="string">            yArr：每个样本对应的类别标签，即目标变量，实际值</span></span><br><span class="line"><span class="string">            k：控制核函数的衰减速率的有关参数，这里设定的是常量值 1</span></span><br><span class="line"><span class="string">        Return：</span></span><br><span class="line"><span class="string">            yHat：样本点的估计值</span></span><br><span class="line"><span class="string">            xCopy：xArr的复制</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 生成一个与目标变量数目相同的 0 向量</span></span><br><span class="line">    yHat = zeros(shape(yArr))</span><br><span class="line">    <span class="comment"># 将 xArr 转换为 矩阵形式</span></span><br><span class="line">    xCopy = mat(xArr)</span><br><span class="line">    <span class="comment"># 排序</span></span><br><span class="line">    xCopy.sort(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 开始循环，为每个样本点进行局部加权线性回归，得到最终的目标变量估计值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(shape(xArr)[<span class="number">0</span>]):</span><br><span class="line">        yHat[i] = lwlr(xCopy[i],xArr,yArr,k)</span><br><span class="line">    <span class="keyword">return</span> yHat,xCopy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#test for LWLR</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression2</span><span class="params">()</span>:</span></span><br><span class="line">    xArr, yArr = loadDataSet(<span class="string">"data/8.Regression/data.txt"</span>)</span><br><span class="line">    yHat = lwlrTest(xArr, xArr, yArr, <span class="number">0.003</span>)</span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    srtInd = xMat[:,<span class="number">1</span>].argsort(<span class="number">0</span>)           <span class="comment"># argsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)，然后输出</span></span><br><span class="line">    xSort=xMat[srtInd][:,<span class="number">0</span>,:]</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.plot(xSort[:,<span class="number">1</span>], yHat[srtInd])</span><br><span class="line">    ax.scatter(xMat[:,<span class="number">1</span>].flatten().A[<span class="number">0</span>], mat(yArr).T.flatten().A[<span class="number">0</span>] , s=<span class="number">2</span>, c=<span class="string">'red'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<h5 id="2-2-3、局部加权线性回归-拟合效果"><a href="#2-2-3、局部加权线性回归-拟合效果" class="headerlink" title="2.2.3、局部加权线性回归 拟合效果"></a>2.2.3、局部加权线性回归 拟合效果</h5><p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_7.png" alt="局部加权线性回归数据效果图"></p>
<p>上图使用了 3 种不同平滑值绘出的局部加权线性回归的结果。上图中的平滑系数 k =1.0，中图 k = 0.01，下图 k = 0.003 。可以看到，k = 1.0 时的使所有数据等比重，其模型效果与基本的线性回归相同，k=0.01时该模型可以挖出数据的潜在规律，而 k=0.003时则考虑了太多的噪声，进而导致了过拟合现象。</p>
<h4 id="2-3、局部加权线性回归-注意事项"><a href="#2-3、局部加权线性回归-注意事项" class="headerlink" title="2.3、局部加权线性回归 注意事项"></a>2.3、局部加权线性回归 注意事项</h4><p>局部加权线性回归也存在一个问题，即增加了计算量，因为它对每个点做预测时都必须使用整个数据集。</p>
<h3 id="3、线性回归-amp-局部加权线性回归-项目案例"><a href="#3、线性回归-amp-局部加权线性回归-项目案例" class="headerlink" title="3、线性回归 &amp; 局部加权线性回归 项目案例"></a>3、线性回归 &amp; 局部加权线性回归 项目案例</h3><p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a></p>
<p>到此为止，我们已经介绍了找出最佳拟合直线的两种方法，下面我们用这些技术来预测鲍鱼的年龄。</p>
<h4 id="3-1、项目概述"><a href="#3-1、项目概述" class="headerlink" title="3.1、项目概述"></a>3.1、项目概述</h4><p>我们有一份来自 UCI 的数据集合的数据，记录了鲍鱼（一种介壳类水生动物）的年龄。鲍鱼年龄可以从鲍鱼壳的层数推算得到。</p>
<h4 id="3-2、开发流程"><a href="#3-2、开发流程" class="headerlink" title="3.2、开发流程"></a>3.2、开发流程</h4><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 采用任意方法收集数据</span></span><br><span class="line"><span class="section">准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据</span></span><br><span class="line"><span class="section">分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比</span></span><br><span class="line"><span class="section">训练算法: 找到回归系数</span></span><br><span class="line"><span class="section">测试算法: 使用 rssError()函数 计算预测误差的大小，来分析模型的效果</span></span><br><span class="line"><span class="section">使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 采用任意方法收集数据</p>
</blockquote>
<blockquote>
<p>准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据</p>
</blockquote>
<p>数据存储格式:</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>	<span class="number">0.455</span>	<span class="number">0.365</span>	<span class="number">0.095</span>	<span class="number">0.514</span>	<span class="number">0.2245</span>	<span class="number">0.101</span>	<span class="number">0.15</span>	<span class="number">15</span></span><br><span class="line"><span class="number">1</span>	<span class="number">0.35</span>	<span class="number">0.265</span>	<span class="number">0.09</span>	<span class="number">0.2255</span>	<span class="number">0.0995</span>	<span class="number">0.0485</span>	<span class="number">0.07</span>	<span class="number">7</span></span><br><span class="line"><span class="number">-1</span>	<span class="number">0.53</span>	<span class="number">0.42</span>	<span class="number">0.135</span>	<span class="number">0.677</span>	<span class="number">0.2565</span>	<span class="number">0.1415</span>	<span class="number">0.21</span>	<span class="number">9</span></span><br><span class="line"><span class="number">1</span>	<span class="number">0.44</span>	<span class="number">0.365</span>	<span class="number">0.125</span>	<span class="number">0.516</span>	<span class="number">0.2155</span>	<span class="number">0.114</span>	<span class="number">0.155</span>	<span class="number">10</span></span><br><span class="line"><span class="number">0</span>	<span class="number">0.33</span>	<span class="number">0.255</span>	<span class="number">0.08</span>	<span class="number">0.205</span>	<span class="number">0.0895</span>	<span class="number">0.0395</span>	<span class="number">0.055</span>	<span class="number">7</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比</p>
</blockquote>
<blockquote>
<p>训练算法: 找到回归系数</p>
</blockquote>
<p>使用上面我们讲到的 局部加权线性回归 训练算法，求出回归系数</p>
<blockquote>
<p>测试算法: 使用 rssError()函数 计算预测误差的大小，来分析模型的效果</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rssError</span><span class="params">(yArr,yHatArr)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        返回真实值与预测值误差大小</span></span><br><span class="line"><span class="string">    Args：</span></span><br><span class="line"><span class="string">        yArr：样本的真实值</span></span><br><span class="line"><span class="string">        yHatArr：样本的预测值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        一个数字，代表误差</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">   <span class="keyword">return</span> ((yArr-yHatArr)**<span class="number">2</span>).sum()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test for abloneDataSet</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abaloneTest</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        预测鲍鱼的年龄</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    abX, abY = loadDataSet(<span class="string">"data/8.Regression/abalone.txt"</span>)</span><br><span class="line">    <span class="comment"># 使用不同的核进行预测</span></span><br><span class="line">    oldyHat01 = lwlrTest(abX[<span class="number">0</span>:<span class="number">99</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">0.1</span>)</span><br><span class="line">    oldyHat1 = lwlrTest(abX[<span class="number">0</span>:<span class="number">99</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">1</span>)</span><br><span class="line">    oldyHat10 = lwlrTest(abX[<span class="number">0</span>:<span class="number">99</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">10</span>)   </span><br><span class="line">    <span class="comment"># 打印出不同的核预测值与训练数据集上的真实值之间的误差大小</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"old yHat01 error Size is :"</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], oldyHat01.T)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"old yHat1 error Size is :"</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], oldyHat1.T)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"old yHat10 error Size is :"</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], oldyHat10.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印出 不同的核预测值 与 新数据集（测试数据集）上的真实值之间的误差大小</span></span><br><span class="line">    newyHat01 = lwlrTest(abX[<span class="number">100</span>:<span class="number">199</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"new yHat01 error Size is :"</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], newyHat01.T)</span><br><span class="line">    newyHat1 = lwlrTest(abX[<span class="number">100</span>:<span class="number">199</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"new yHat1 error Size is :"</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], newyHat1.T)</span><br><span class="line">    newyHat10 = lwlrTest(abX[<span class="number">100</span>:<span class="number">199</span>], abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>], <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"new yHat10 error Size is :"</span> , rssError(abY[<span class="number">0</span>:<span class="number">99</span>], newyHat10.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用简单的 线性回归 进行预测，与上面的计算进行比较</span></span><br><span class="line">    standWs = standRegres(abX[<span class="number">0</span>:<span class="number">99</span>], abY[<span class="number">0</span>:<span class="number">99</span>])</span><br><span class="line">    standyHat = mat(abX[<span class="number">100</span>:<span class="number">199</span>]) * standWs</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"standRegress error Size is:"</span>, rssError(abY[<span class="number">100</span>:<span class="number">199</span>], standyHat.T.A)</span><br></pre></td></tr></table></figure>


<p>根据我们上边的测试，可以看出: </p>
<p>简单线性回归达到了与局部加权现行回归类似的效果。这也说明了一点，必须在未知数据上比较效果才能选取到最佳模型。那么最佳的核大小是 10 吗？或许是，但如果想得到更好的效果，可以尝试用 10 个不同的样本集做 10 次测试来比较结果。</p>
<blockquote>
<p>使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签</p>
</blockquote>
<h3 id="4、缩减系数来-“理解”-数据"><a href="#4、缩减系数来-“理解”-数据" class="headerlink" title="4、缩减系数来 “理解” 数据"></a>4、缩减系数来 “理解” 数据</h3><p>如果数据的特征比样本点还多应该怎么办？是否还可以使用线性回归和之前的方法来做预测？答案是否定的，即我们不能再使用前面介绍的方法。这是因为在计算 <img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_8.png" alt="矩阵求逆"> 的时候会出错。</p>
<p>如果特征比样本点还多(n &gt; m)，也就是说输入数据的矩阵 x 不是满秩矩阵。非满秩矩阵求逆时会出现问题。</p>
<p>为了解决这个问题，我们引入了 <code>岭回归（ridge regression）</code> 这种缩减方法。接着是 <code>lasso法</code>，最后介绍 <code>前向逐步回归</code>。</p>
<h4 id="4-1、岭回归"><a href="#4-1、岭回归" class="headerlink" title="4.1、岭回归"></a>4.1、岭回归</h4><p>简单来说，岭回归就是在矩阵 <img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_9.png" alt="矩阵_1"> 上加一个 λI 从而使得矩阵非奇异，进而能对 <img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_10.png" alt="矩阵_2"> 求逆。其中矩阵I是一个 n * n （等于列数） 的单位矩阵，<br>对角线上元素全为1，其他元素全为0。而λ是一个用户定义的数值，后面会做介绍。在这种情况下，回归系数的计算公式将变成：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_11.png" alt="岭回归的回归系数计算"></p>
<p>岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入 λ 来限制了所有 w 之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也叫作 <code>缩减(shrinkage)</code>。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_22.png" alt="岭回归"></p>
<p>缩减方法可以去掉不重要的参数，因此能更好地理解数据。此外，与简单的线性回归相比，缩减法能取得更好的预测效果。</p>
<p>这里通过预测误差最小化得到 λ: 数据获取之后，首先抽一部分数据用于测试，剩余的作为训练集用于训练参数 w。训练完毕后在测试集上测试预测性能。通过选取不同的 λ 来重复上述测试过程，最终得到一个使预测误差最小的 λ 。</p>
<h5 id="4-1-1、岭回归-原始代码"><a href="#4-1-1、岭回归-原始代码" class="headerlink" title="4.1.1、岭回归 原始代码"></a>4.1.1、岭回归 原始代码</h5><p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeRegres</span><span class="params">(xMat,yMat,lam=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Desc：</span></span><br><span class="line"><span class="string">            这个函数实现了给定 lambda 下的岭回归求解。</span></span><br><span class="line"><span class="string">            如果数据的特征比样本点还多，就不能再使用上面介绍的的线性回归和局部线性回归了，因为计算 (xTx)^(-1)会出现错误。</span></span><br><span class="line"><span class="string">            如果特征比样本点还多（n &gt; m），也就是说，输入数据的矩阵x不是满秩矩阵。非满秩矩阵在求逆时会出现问题。</span></span><br><span class="line"><span class="string">            为了解决这个问题，我们下边讲一下：岭回归，这是我们要讲的第一种缩减方法。</span></span><br><span class="line"><span class="string">        Args：</span></span><br><span class="line"><span class="string">            xMat：样本的特征数据，即 feature</span></span><br><span class="line"><span class="string">            yMat：每个样本对应的类别标签，即目标变量，实际值</span></span><br><span class="line"><span class="string">            lam：引入的一个λ值，使得矩阵非奇异</span></span><br><span class="line"><span class="string">        Returns：</span></span><br><span class="line"><span class="string">            经过岭回归公式计算得到的回归系数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    <span class="comment"># 岭回归就是在矩阵 xTx 上加一个 λI 从而使得矩阵非奇异，进而能对 xTx + λI 求逆</span></span><br><span class="line">    denom = xTx + eye(shape(xMat)[<span class="number">1</span>])*lam</span><br><span class="line">    <span class="comment"># 检查行列式是否为零，即矩阵是否可逆，行列式为0的话就不可逆，不为0的话就是可逆。</span></span><br><span class="line">    <span class="keyword">if</span> linalg.det(denom) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"This matrix is singular, cannot do inverse"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = denom.I * (xMat.T*yMat)</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeTest</span><span class="params">(xArr,yArr)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Desc：</span></span><br><span class="line"><span class="string">            函数 ridgeTest() 用于在一组 λ 上测试结果</span></span><br><span class="line"><span class="string">        Args：</span></span><br><span class="line"><span class="string">            xArr：样本数据的特征，即 feature</span></span><br><span class="line"><span class="string">            yArr：样本数据的类别标签，即真实数据</span></span><br><span class="line"><span class="string">        Returns：</span></span><br><span class="line"><span class="string">            wMat：将所有的回归系数输出到一个矩阵并返回</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat=mat(yArr).T</span><br><span class="line">    <span class="comment"># 计算Y的均值</span></span><br><span class="line">    yMean = mean(yMat,<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># Y的所有的特征减去均值</span></span><br><span class="line">    yMat = yMat - yMean</span><br><span class="line">    <span class="comment"># 标准化 x，计算 xMat 平均值</span></span><br><span class="line">    xMeans = mean(xMat,<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 然后计算 X的方差</span></span><br><span class="line">    xVar = var(xMat,<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 所有特征都减去各自的均值并除以方差</span></span><br><span class="line">    xMat = (xMat - xMeans)/xVar</span><br><span class="line">    <span class="comment"># 可以在 30 个不同的 lambda 下调用 ridgeRegres() 函数。</span></span><br><span class="line">    numTestPts = <span class="number">30</span></span><br><span class="line">    <span class="comment"># 创建30 * m 的全部数据为0 的矩阵</span></span><br><span class="line">    wMat = zeros((numTestPts,shape(xMat)[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTestPts):</span><br><span class="line">        <span class="comment"># exp() 返回 e^x </span></span><br><span class="line">        ws = ridgeRegres(xMat,yMat,exp(i<span class="number">-10</span>))</span><br><span class="line">        wMat[i,:]=ws.T</span><br><span class="line">    <span class="keyword">return</span> wMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#test for ridgeRegression</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression3</span><span class="params">()</span>:</span></span><br><span class="line">    abX,abY = loadDataSet(<span class="string">"data/8.Regression/abalone.txt"</span>)</span><br><span class="line">    ridgeWeights = ridgeTest(abX, abY)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.plot(ridgeWeights)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<h5 id="4-1-2、岭回归在鲍鱼数据集上的运行效果"><a href="#4-1-2、岭回归在鲍鱼数据集上的运行效果" class="headerlink" title="4.1.2、岭回归在鲍鱼数据集上的运行效果"></a>4.1.2、岭回归在鲍鱼数据集上的运行效果</h5><p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_12.png" alt="岭回归的运行效果"></p>
<p>上图绘制出了回归系数与 log(λ) 的关系。在最左边，即 λ 最小时，可以得到所有系数的原始值（与线性回归一致）；而在右边，系数全部缩减为0；在中间部分的某值将可以取得最好的预测效果。为了定量地找到最佳参数值，还需要进行交叉验证。另外，要判断哪些变量对结果预测最具有影响力，在上图中观察它们对应的系数大小就可以了。</p>
<h4 id="4-2、套索方法-Lasso，The-Least-Absolute-Shrinkage-and-Selection-Operator"><a href="#4-2、套索方法-Lasso，The-Least-Absolute-Shrinkage-and-Selection-Operator" class="headerlink" title="4.2、套索方法(Lasso，The Least Absolute Shrinkage and Selection Operator)"></a>4.2、套索方法(Lasso，The Least Absolute Shrinkage and Selection Operator)</h4><p>在增加如下约束时，普通的最小二乘法回归会得到与岭回归一样的公式: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_13.png" alt="lasso_1"></p>
<p>上式限定了所有回归系数的平方和不能大于 λ 。使用普通的最小二乘法回归在当两个或更多的特征相关时，可能会得到一个很大的正系数和一个很大的负系数。正是因为上述限制条件的存在，使用岭回归可以避免这个问题。</p>
<p>与岭回归类似，另一个缩减方法lasso也对回归系数做了限定，对应的约束条件如下: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_14.png" alt="lasso_2"></p>
<p>唯一的不同点在于，这个约束条件使用绝对值取代了平方和。虽然约束形式只是稍作变化，结果却大相径庭: 在 λ 足够小的时候，一些系数会因此被迫缩减到 0.这个特性可以帮助我们更好地理解数据。</p>
<h4 id="4-3、前向逐步回归"><a href="#4-3、前向逐步回归" class="headerlink" title="4.3、前向逐步回归"></a>4.3、前向逐步回归</h4><p>前向逐步回归算法可以得到与 lasso 差不多的效果，但更加简单。它属于一种贪心算法，即每一步都尽可能减少误差。一开始，所有权重都设置为 0，然后每一步所做的决策是对某个权重增加或减少一个很小的值。</p>
<p>伪代码如下:</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">数据标准化，使其分布满足 0 均值 和单位方差</span><br><span class="line">在每轮迭代过程中: </span><br><span class="line">    设置当前最小误差 lowestError 为正无穷</span><br><span class="line">    对每个特征:</span><br><span class="line">        增大或缩小:</span><br><span class="line">            改变一个系数得到一个新的 w</span><br><span class="line">            计算新 w 下的误差</span><br><span class="line">            如果误差 <span class="builtin-name">Error</span> 小于当前最小误差 lowestError: 设置 Wbest 等于当前的 W</span><br><span class="line">        将 W 设置为新的 Wbest</span><br></pre></td></tr></table></figure>

<h5 id="4-3-1、前向逐步回归-原始代码"><a href="#4-3-1、前向逐步回归-原始代码" class="headerlink" title="4.3.1、前向逐步回归 原始代码"></a>4.3.1、前向逐步回归 原始代码</h5><p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stageWise</span><span class="params">(xArr,yArr,eps=<span class="number">0.01</span>,numIt=<span class="number">100</span>)</span>:</span></span><br><span class="line">    xMat = mat(xArr); yMat=mat(yArr).T</span><br><span class="line">    yMean = mean(yMat,<span class="number">0</span>)</span><br><span class="line">    yMat = yMat - yMean     <span class="comment"># 也可以规则化ys但会得到更小的coef</span></span><br><span class="line">    xMat = regularize(xMat)</span><br><span class="line">    m,n=shape(xMat)</span><br><span class="line">    <span class="comment">#returnMat = zeros((numIt,n)) # 测试代码删除</span></span><br><span class="line">    ws = zeros((n,<span class="number">1</span>)); wsTest = ws.copy(); wsMax = ws.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numIt):</span><br><span class="line">        <span class="keyword">print</span> (ws.T)</span><br><span class="line">        lowestError = inf; </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">for</span> sign <span class="keyword">in</span> [<span class="number">-1</span>,<span class="number">1</span>]:</span><br><span class="line">                wsTest = ws.copy()</span><br><span class="line">                wsTest[j] += eps*sign</span><br><span class="line">                yTest = xMat*wsTest</span><br><span class="line">                rssE = rssError(yMat.A,yTest.A)</span><br><span class="line">                <span class="keyword">if</span> rssE &lt; lowestError:</span><br><span class="line">                    lowestError = rssE</span><br><span class="line">                    wsMax = wsTest</span><br><span class="line">        ws = wsMax.copy()</span><br><span class="line">        returnMat[i,:]=ws.T</span><br><span class="line">    <span class="keyword">return</span> returnMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#test for stageWise</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression4</span><span class="params">()</span>:</span></span><br><span class="line">    xArr,yArr=loadDataSet(<span class="string">"data/8.Regression/abalone.txt"</span>)</span><br><span class="line">    print(stageWise(xArr,yArr,<span class="number">0.01</span>,<span class="number">200</span>))</span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    xMat = regularize(xMat)</span><br><span class="line">    yM = mean(yMat,<span class="number">0</span>)</span><br><span class="line">    yMat = yMat - yM</span><br><span class="line">    weights = standRegres(xMat, yMat.T)</span><br><span class="line">    <span class="keyword">print</span> (weights.T)</span><br></pre></td></tr></table></figure>


<h5 id="4-3-2、逐步线性回归在鲍鱼数据集上的运行效果"><a href="#4-3-2、逐步线性回归在鲍鱼数据集上的运行效果" class="headerlink" title="4.3.2、逐步线性回归在鲍鱼数据集上的运行效果"></a>4.3.2、逐步线性回归在鲍鱼数据集上的运行效果</h5><p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_15.png" alt="逐步线性回归运行效果"></p>
<p>逐步线性回归算法的主要优点在于它可以帮助人们理解现有的模型并作出改进。当构建了一个模型后，可以运行该算法找出重要的特征，这样就有可能及时停止对那些不重要特征的收集。最后，如果用于测试，该算法每100次迭代后就可以构建出一个模型，可以使用类似于10折交叉验证的方法比较这些模型，最终选择使误差最小的模型。</p>
<h4 id="4-4、小结"><a href="#4-4、小结" class="headerlink" title="4.4、小结"></a>4.4、小结</h4><p>当应用缩减方法（如逐步线性回归或岭回归）时，模型也就增加了偏差（bias），与此同时却减小了模型的方差。</p>
<h3 id="5、权衡偏差和方差"><a href="#5、权衡偏差和方差" class="headerlink" title="5、权衡偏差和方差"></a>5、权衡偏差和方差</h3><p>任何时候，一旦发现模型和测量值之间存在差异，就说出现了误差。当考虑模型中的 “噪声” 或者说误差时，必须考虑其来源。你可能会对复杂的过程进行简化，这将导致在模型和测量值之间出现 “噪声” 或误差，若无法理解数据的真实生成过程，也会导致差异的产生。另外，测量过程本身也可能产生 “噪声” 或者问题。下面我们举一个例子，我们使用 <code>线性回归</code> 和 <code>局部加权线性回归</code> 处理过一个从文件导入的二维数据。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_16.png" alt="生成公式"></p>
<p>其中的 N(0, 1) 是一个均值为 0、方差为 1 的正态分布。我们尝试过仅用一条直线来拟合上述数据。不难想到，直线所能得到的最佳拟合应该是 3.0+1.7x 这一部分。这样的话，误差部分就是 0.1sin(30x)+0.06N(0, 1) 。在上面，我们使用了局部加权线性回归来试图捕捉数据背后的结构。该结构拟合起来有一定的难度，因此我们测试了多组不同的局部权重来找到具有最小测试误差的解。</p>
<p>下图给出了训练误差和测试误差的曲线图，上面的曲面就是测试误差，下面的曲线是训练误差。我们根据 预测鲍鱼年龄 的实验知道: 如果降低核的大小，那么训练误差将变小。从下图开看，从左到右就表示了核逐渐减小的过程。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/8.Regression/LinearR_17.png" alt="偏差方差图"></p>
<p>一般认为，上述两种误差由三个部分组成: 偏差、测量误差和随机噪声。局部加权线性回归 和 预测鲍鱼年龄 中，我们通过引入了三个越来越小的核来不断增大模型的方差。</p>
<p>在缩减系数来“理解”数据这一节中，我们介绍了缩减法，可以将一些系数缩减成很小的值或直接缩减为 0 ，这是一个增大模型偏差的例子。通过把一些特征的回归系数缩减到 0 ，同时也就减小了模型的复杂度。例子中有 8 个特征，消除其中两个后不仅使模型更易理解，同时还降低了预测误差。对照上图，左侧是参数缩减过于严厉的结果，而右侧是无缩减的效果。</p>
<p>方差是可以度量的。如果从鲍鱼数据中取一个随机样本集（例如取其中 100 个数据）并用线性模型拟合，将会得到一组回归系数。同理，再取出另一组随机样本集并拟合，将会得到另一组回归系数。这些系数间的差异大小也就是模型方差的反映。</p>
<h3 id="6、回归-项目案例"><a href="#6、回归-项目案例" class="headerlink" title="6、回归 项目案例"></a>6、回归 项目案例</h3><h4 id="项目案例1-预测乐高玩具套装的价格"><a href="#项目案例1-预测乐高玩具套装的价格" class="headerlink" title="项目案例1: 预测乐高玩具套装的价格"></a>项目案例1: 预测乐高玩具套装的价格</h4><p><a href="/src/py2.x/ml/8.Regression/regression.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/8.Regression/regression.py</a></p>
<h5 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h5><p>Dangler 喜欢为乐高套装估价，我们用回归技术来帮助他建立一个预测模型。</p>
<h5 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h5><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">(<span class="number">1</span>) 收集数据：用 Google Shopping 的API收集数据。</span><br><span class="line">(<span class="number">2</span>) 准备数据：从返回的JSON数据中抽取价格。</span><br><span class="line">(<span class="number">3</span>) 分析数据：可视化并观察数据。</span><br><span class="line">(<span class="number">4</span>) 训练算法：构建不同的模型，采用逐步线性回归和直接的线性回归模型。</span><br><span class="line">(<span class="number">5</span>) 测试算法：使用交叉验证来测试不同的模型，分析哪个效果最好。</span><br><span class="line">(<span class="number">6</span>) 使用算法：这次练习的目标就是生成数据模型。</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 使用 Google 购物的 API </p>
</blockquote>
<p>由于 Google 提供的 api 失效，我们只能自己下载咯，将数据存储在了 input 文件夹下的 setHtml 文件夹下</p>
<blockquote>
<p>准备数据: 从返回的 JSON 数据中抽取价格</p>
</blockquote>
<p>因为我们这里不是在线的，就不再是 JSON 了，我们直接解析线下的网页，得到我们想要的数据。</p>
<blockquote>
<p>分析数据: 可视化并观察数据</p>
</blockquote>
<p>这里我们将解析得到的数据打印出来，然后观察数据。</p>
<blockquote>
<p>训练算法: 构建不同的模型</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从页面读取数据，生成retX和retY列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrapePage</span><span class="params">(retX, retY, inFile, yr, numPce, origPrc)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打开并读取HTML文件</span></span><br><span class="line">    fr = open(inFile)</span><br><span class="line">    soup = BeautifulSoup(fr.read())</span><br><span class="line">    i=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据HTML页面结构进行解析</span></span><br><span class="line">    currentRow = soup.findAll(<span class="string">'table'</span>, r=<span class="string">"%d"</span> % i)</span><br><span class="line">    <span class="keyword">while</span>(len(currentRow)!=<span class="number">0</span>):</span><br><span class="line">        currentRow = soup.findAll(<span class="string">'table'</span>, r=<span class="string">"%d"</span> % i)</span><br><span class="line">        title = currentRow[<span class="number">0</span>].findAll(<span class="string">'a'</span>)[<span class="number">1</span>].text</span><br><span class="line">        lwrTitle = title.lower()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 查找是否有全新标签</span></span><br><span class="line">        <span class="keyword">if</span> (lwrTitle.find(<span class="string">'new'</span>) &gt; <span class="number">-1</span>) <span class="keyword">or</span> (lwrTitle.find(<span class="string">'nisb'</span>) &gt; <span class="number">-1</span>):</span><br><span class="line">            newFlag = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            newFlag = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 查找是否已经标志出售，我们只收集已出售的数据</span></span><br><span class="line">        soldUnicde = currentRow[<span class="number">0</span>].findAll(<span class="string">'td'</span>)[<span class="number">3</span>].findAll(<span class="string">'span'</span>)</span><br><span class="line">        <span class="keyword">if</span> len(soldUnicde)==<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"item #%d did not sell"</span> % i</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 解析页面获取当前价格</span></span><br><span class="line">            soldPrice = currentRow[<span class="number">0</span>].findAll(<span class="string">'td'</span>)[<span class="number">4</span>]</span><br><span class="line">            priceStr = soldPrice.text</span><br><span class="line">            priceStr = priceStr.replace(<span class="string">'$'</span>,<span class="string">''</span>) <span class="comment">#strips out $</span></span><br><span class="line">            priceStr = priceStr.replace(<span class="string">','</span>,<span class="string">''</span>) <span class="comment">#strips out ,</span></span><br><span class="line">            <span class="keyword">if</span> len(soldPrice)&gt;<span class="number">1</span>:</span><br><span class="line">                priceStr = priceStr.replace(<span class="string">'Free shipping'</span>, <span class="string">''</span>)</span><br><span class="line">            sellingPrice = float(priceStr)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 去掉不完整的套装价格</span></span><br><span class="line">            <span class="keyword">if</span>  sellingPrice &gt; origPrc * <span class="number">0.5</span>:</span><br><span class="line">                    <span class="keyword">print</span> <span class="string">"%d\t%d\t%d\t%f\t%f"</span> % (yr,numPce,newFlag,origPrc, sellingPrice)</span><br><span class="line">                    retX.append([yr, numPce, newFlag, origPrc])</span><br><span class="line">                    retY.append(sellingPrice)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        currentRow = soup.findAll(<span class="string">'table'</span>, r=<span class="string">"%d"</span> % i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 依次读取六种乐高套装的数据，并生成数据矩阵        </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setDataCollect</span><span class="params">(retX, retY)</span>:</span></span><br><span class="line">    scrapePage(retX, retY, <span class="string">'data/8.Regression/setHtml/lego8288.html'</span>, <span class="number">2006</span>, <span class="number">800</span>, <span class="number">49.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">'data/8.Regression/setHtml/lego10030.html'</span>, <span class="number">2002</span>, <span class="number">3096</span>, <span class="number">269.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">'data/8.Regression/setHtml/lego10179.html'</span>, <span class="number">2007</span>, <span class="number">5195</span>, <span class="number">499.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">'data/8.Regression/setHtml/lego10181.html'</span>, <span class="number">2007</span>, <span class="number">3428</span>, <span class="number">199.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">'data/8.Regression/setHtml/lego10189.html'</span>, <span class="number">2008</span>, <span class="number">5922</span>, <span class="number">299.99</span>)</span><br><span class="line">    scrapePage(retX, retY, <span class="string">'data/8.Regression/setHtml/lego10196.html'</span>, <span class="number">2009</span>, <span class="number">3263</span>, <span class="number">249.99</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法：使用交叉验证来测试不同的模型，分析哪个效果最好</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 交叉验证测试岭回归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crossValidation</span><span class="params">(xArr,yArr,numVal=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 获得数据点个数，xArr和yArr具有相同长度</span></span><br><span class="line">    m = len(yArr)</span><br><span class="line">    indexList = range(m)</span><br><span class="line">    errorMat = zeros((numVal,<span class="number">30</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 主循环 交叉验证循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numVal):</span><br><span class="line">        <span class="comment"># 随机拆分数据，将数据分为训练集（90%）和测试集（10%）</span></span><br><span class="line">        trainX=[]; trainY=[]</span><br><span class="line">        testX = []; testY = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对数据进行混洗操作</span></span><br><span class="line">        random.shuffle(indexList)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 切分训练集和测试集</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="keyword">if</span> j &lt; m*<span class="number">0.9</span>: </span><br><span class="line">                trainX.append(xArr[indexList[j]])</span><br><span class="line">                trainY.append(yArr[indexList[j]])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                testX.append(xArr[indexList[j]])</span><br><span class="line">                testY.append(yArr[indexList[j]])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得回归系数矩阵</span></span><br><span class="line">        wMat = ridgeTest(trainX,trainY)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 循环遍历矩阵中的30组回归系数</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">            <span class="comment"># 读取训练集和数据集</span></span><br><span class="line">            matTestX = mat(testX); matTrainX=mat(trainX)</span><br><span class="line">            <span class="comment"># 对数据进行标准化</span></span><br><span class="line">            meanTrain = mean(matTrainX,<span class="number">0</span>)</span><br><span class="line">            varTrain = var(matTrainX,<span class="number">0</span>)</span><br><span class="line">            matTestX = (matTestX-meanTrain)/varTrain</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 测试回归效果并存储</span></span><br><span class="line">            yEst = matTestX * mat(wMat[k,:]).T + mean(trainY)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算误差</span></span><br><span class="line">            errorMat[i,k] = ((yEst.T.A-array(testY))**<span class="number">2</span>).sum()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算误差估计值的均值</span></span><br><span class="line">    meanErrors = mean(errorMat,<span class="number">0</span>)</span><br><span class="line">    minMean = float(min(meanErrors))</span><br><span class="line">    bestWeights = wMat[nonzero(meanErrors==minMean)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不要使用标准化的数据，需要对数据进行还原来得到输出结果</span></span><br><span class="line">    xMat = mat(xArr); yMat=mat(yArr).T</span><br><span class="line">    meanX = mean(xMat,<span class="number">0</span>); varX = var(xMat,<span class="number">0</span>)</span><br><span class="line">    unReg = bestWeights/varX</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出构建的模型</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"the best model from Ridge Regression is:\n"</span>,unReg</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"with constant term: "</span>,<span class="number">-1</span>*sum(multiply(meanX,unReg)) + mean(yMat)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># predict for lego's price</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regression5</span><span class="params">()</span>:</span></span><br><span class="line">    lgX = []</span><br><span class="line">    lgY = []</span><br><span class="line"></span><br><span class="line">    setDataCollect(lgX, lgY)</span><br><span class="line">    crossValidation(lgX, lgY, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法：这次练习的目标就是生成数据模型</p>
</blockquote>
<h2 id="7、选读内容"><a href="#7、选读内容" class="headerlink" title="7、选读内容"></a>7、选读内容</h2><p>求解线性回归可以有很多种方式，除了上述的方法（正规方程 normal equation）解决之外，还有可以对Cost function 求导，其中最简单的方法就是梯度下降法。</p>
<p> 那么正规方程就可以直接得出真实值。而梯度下降法只能给出近似值。</p>
<p>以下是梯度下降法和正规方程的比较:</p>
<table>
<thead>
<tr>
<th>梯度下降法</th>
<th align="center">正规方程</th>
</tr>
</thead>
<tbody><tr>
<td>结果为真实值的近似值</td>
<td align="center">结果为真实值</td>
</tr>
<tr>
<td>需要循环多次</td>
<td align="center">无需循环</td>
</tr>
<tr>
<td>样本数量大的时候也ok</td>
<td align="center">样本数量特别大的时候会很慢（n&gt;10000）</td>
</tr>
</tbody></table>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a>      </li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第10章 K-Means（K-均值）聚类算法</title>
    <url>/2020/06/02/10.k-means%E8%81%9A%E7%B1%BB/</url>
    <content><![CDATA[<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><p>聚类，简单来说，就是将一个庞杂数据集中具有相似特征的数据自动归类到一起，称为一个簇，簇内的对象越相似，聚类的效果越好。它是一种无监督的学习(Unsupervised Learning)方法,不需要预先标注好的训练集。聚类与分类最大的区别就是分类的目标事先已知，例如猫狗识别，你在分类之前已经预先知道要将它分为猫、狗两个种类；而在你聚类之前，你对你的目标是未知的，同样以动物为例，对于一个动物集来说，你并不清楚这个数据集内部有多少种类的动物，你能做的只是利用聚类方法将它自动按照特征分为多类，然后人为给出这个聚类结果的定义（即簇识别）。例如，你将一个动物集分为了三簇（类），然后通过观察这三类动物的特征，你为每一个簇起一个名字，如大象、狗、猫等，这就是聚类的基本思想。     </p>
<a id="more"></a>

<p>至于“相似”这一概念，是利用距离这个评价标准来衡量的，我们通过计算对象与对象之间的距离远近来判断它们是否属于同一类别，即是否是同一个簇。至于距离如何计算，科学家们提出了许多种距离的计算方法，其中欧式距离是最为简单和常用的，除此之外还有曼哈顿距离和余弦相似性距离等。</p>
<p>欧式距离，我想大家再熟悉不过了，但为免有一些基础薄弱的同学，在此再说明一下，它的定义为:对于x点(坐标为(x1,x2,x3,…,xn))和 y点（坐标为(y1,y2,y3,…,yn)），两者的欧式距离为</p>
![d(x,y)={\sqrt  {(x_{1}-y_{1})^{2}+(x_{2}-y_{2})^{2}+\cdots +(x_{n}-y_{n})^{2}}}={\sqrt  {\sum _{{i=1}}^{n}(x_{i}-y_{i})^{2}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/bfa1815838113388d78c9402bba7308d734a4af2)  

<p>在二维平面，它就是我们初中时就学过的两点距离公式</p>
<h2 id="K-Means-算法"><a href="#K-Means-算法" class="headerlink" title="K-Means 算法"></a>K-Means 算法</h2><p>K-Means 是发现给定数据集的 K 个簇的聚类算法, 之所以称之为 <code>K-均值</code> 是因为它可以发现 K 个不同的簇, 且每个簇的中心采用簇中所含值的均值计算而成.<br>簇个数 K 是用户指定的, 每一个簇通过其质心（centroid）, 即簇中所有点的中心来描述.<br>聚类与分类算法的最大区别在于, 分类的目标类别已知, 而聚类的目标类别是未知的.  </p>
<p><strong>优点</strong>:</p>
<ul>
<li>属于无监督学习，无须准备训练集</li>
<li>原理简单，实现起来较为容易</li>
<li>结果可解释性较好</li>
</ul>
<p><strong>缺点</strong>:</p>
<ul>
<li><strong>需手动设置k值</strong>。 在算法开始预测之前，我们需要手动设置k值，即估计数据大概的类别个数，不合理的k值会使结果缺乏解释性</li>
<li>可能收敛到局部最小值, 在大规模数据集上收敛较慢</li>
<li>对于异常点、离群点敏感</li>
</ul>
<p>使用数据类型 : 数值型数据</p>
<h3 id="K-Means-场景"><a href="#K-Means-场景" class="headerlink" title="K-Means 场景"></a>K-Means 场景</h3><p>kmeans，如前所述，用于数据集内种类属性不明晰，希望能够通过数据挖掘出或自动归类出有相似特点的对象的场景。其商业界的应用场景一般为挖掘出具有相似特点的潜在客户群体以便公司能够重点研究、对症下药。  </p>
<p>例如，在2000年和2004年的美国总统大选中，候选人的得票数比较接近或者说非常接近。任一候选人得到的普选票数的最大百分比为50.7%而最小百分比为47.9% 如果1%的选民将手中的选票投向另外的候选人，那么选举结果就会截然不同。 实际上，如果妥善加以引导与吸引，少部分选民就会转换立场。尽管这类选举者占的比例较低，但当候选人的选票接近时，这些人的立场无疑会对选举结果产生非常大的影响。如何找出这类选民，以及如何在有限的预算下采取措施来吸引他们？ 答案就是聚类（Clustering)。</p>
<p>那么，具体如何实施呢？首先，收集用户的信息，可以同时收集用户满意或不满意的信息，这是因为任何对用户重要的内容都可能影响用户的投票结果。然后，将这些信息输入到某个聚类算法中。接着，对聚类结果中的每一个簇（最好选择最大簇 ）， 精心构造能够吸引该簇选民的消息。最后， 开展竞选活动并观察上述做法是否有效。</p>
<p>另一个例子就是产品部门的市场调研了。为了更好的了解自己的用户，产品部门可以采用聚类的方法得到不同特征的用户群体，然后针对不同的用户群体可以对症下药，为他们提供更加精准有效的服务。</p>
<h3 id="K-Means-术语"><a href="#K-Means-术语" class="headerlink" title="K-Means 术语"></a>K-Means 术语</h3><ul>
<li>簇: 所有数据的点集合，簇中的对象是相似的。</li>
<li>质心: 簇中所有点的中心（计算所有点的均值而来）.</li>
<li>SSE: Sum of Sqared Error（误差平方和）, 它被用来评估模型的好坏，SSE 值越小，表示越接近它们的质心. 聚类效果越好。由于对误差取了平方，因此更加注重那些远离中心的点（一般为边界点或离群点）。详情见kmeans的评价标准。</li>
</ul>
<p>有关 <code>簇</code> 和 <code>质心</code> 术语更形象的介绍, 请参考下图:</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/10.KMeans/apachecn-k-means-term-1.jpg" alt="K-Means 术语图"></p>
<h3 id="K-Means-工作流程"><a href="#K-Means-工作流程" class="headerlink" title="K-Means 工作流程"></a>K-Means 工作流程</h3><ol>
<li>首先, 随机确定 K 个初始点作为质心（<strong>不必是数据中的点</strong>）。</li>
<li>然后将数据集中的每个点分配到一个簇中, 具体来讲, 就是为每个点找到距其最近的质心, 并将其分配该质心所对应的簇. 这一步完成之后, 每个簇的质心更新为该簇所有点的平均值.</li>
<li>重复上述过程直到数据集中的所有点都距离它所对应的质心最近时结束。</li>
</ol>
<p>上述过程的 <code>伪代码</code> 如下:</p>
<ul>
<li>创建 k 个点作为起始质心（通常是随机选择）</li>
<li>当任意一个点的簇分配结果发生改变时（不改变时算法结束）<ul>
<li>对数据集中的每个数据点<ul>
<li>对每个质心<ul>
<li>计算质心与数据点之间的距离</li>
</ul>
</li>
<li>将数据点分配到距其最近的簇</li>
</ul>
</li>
<li>对每一个簇, 计算簇中所有点的均值并将均值作为质心</li>
</ul>
</li>
</ul>
<h3 id="K-Means-开发流程"><a href="#K-Means-开发流程" class="headerlink" title="K-Means 开发流程"></a>K-Means 开发流程</h3><figure class="highlight erlang"><table><tr><td class="code"><pre><span class="line">收集数据：使用任意方法</span><br><span class="line">准备数据：需要数值型数据类计算距离, 也可以将标称型数据映射为二值型数据再用于距离计算</span><br><span class="line">分析数据：使用任意方法</span><br><span class="line">训练算法：不适用于无监督学习，即无监督学习不需要训练步骤</span><br><span class="line">测试算法：应用聚类算法、观察结果.可以使用量化的误差指标如误差平方和（后面会介绍）来评价算法的结果.</span><br><span class="line">使用算法：可以用于所希望的任何应用.通常情况下, 簇质心可以代表整个簇的数据来做出决策.</span><br></pre></td></tr></table></figure>
<h3 id="K-Means-的评价标准"><a href="#K-Means-的评价标准" class="headerlink" title="K-Means 的评价标准"></a>K-Means 的评价标准</h3><p>k-means算法因为手动选取k值和初始化随机质心的缘故，每一次的结果不会完全一样，而且由于手动选取k值，我们需要知道我们选取的k值是否合理，聚类效果好不好，那么如何来评价某一次的聚类效果呢？也许将它们画在图上直接观察是最好的办法，但现实是，我们的数据不会仅仅只有两个特征，一般来说都有十几个特征，而观察十几维的空间对我们来说是一个无法完成的任务。因此，我们需要一个公式来帮助我们判断聚类的性能，这个公式就是<strong>SSE</strong> (Sum of Squared Error, 误差平方和 ），它其实就是每一个点到其簇内质心的距离的平方值的总和，这个数值对应kmeans函数中<strong>clusterAssment</strong>矩阵的第一列之和。 <strong>SSE</strong>值越小表示数据点越接近于它们的质心，聚类效果也越好。 因为对误差取了平方，因此更加重视那些远离中心的点。一种肯定可以降低<strong>SSE</strong>值的方法是增加簇的个数，但这违背了聚类的目标。聚类的目标是在保持簇数目不变的情况下提高簇的质量。</p>
<h3 id="K-Means-聚类算法函数"><a href="#K-Means-聚类算法函数" class="headerlink" title="K-Means 聚类算法函数"></a>K-Means 聚类算法函数</h3><h4 id="从文件加载数据集"><a href="#从文件加载数据集" class="headerlink" title="从文件加载数据集"></a>从文件加载数据集</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从文本中构建矩阵，加载文本文件，然后处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span>    <span class="comment"># 通用函数，用来解析以 tab 键分隔的 floats（浮点数），例如: 1.658985	4.285136</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        fltLine = map(float,curLine)    <span class="comment"># 映射所有的元素为 float（浮点数）类型</span></span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br></pre></td></tr></table></figure>

<h4 id="计算两个向量的欧氏距离"><a href="#计算两个向量的欧氏距离" class="headerlink" title="计算两个向量的欧氏距离"></a>计算两个向量的欧氏距离</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算两个向量的欧式距离（可根据场景选择其他距离公式）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum(power(vecA - vecB, <span class="number">2</span>))) <span class="comment"># la.norm(vecA-vecB)</span></span><br></pre></td></tr></table></figure>

<h4 id="构建一个包含-K-个随机质心的集合"><a href="#构建一个包含-K-个随机质心的集合" class="headerlink" title="构建一个包含 K 个随机质心的集合"></a>构建一个包含 K 个随机质心的集合</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为给定数据集构建一个包含 k 个随机质心的集合。随机质心必须要在整个数据集的边界之内，这可以通过找到数据集每一维的最小和最大值来完成。然后生成 0~1.0 之间的随机数并通过取值范围和最小值，以便确保随机点在数据的边界之内。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">    n = shape(dataSet)[<span class="number">1</span>] <span class="comment"># 列的数量，即数据的特征个数</span></span><br><span class="line">    centroids = mat(zeros((k,n))) <span class="comment"># 创建k个质心矩阵</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n): <span class="comment"># 创建随机簇质心，并且在每一维的边界内</span></span><br><span class="line">        minJ = min(dataSet[:,j])    <span class="comment"># 最小值</span></span><br><span class="line">        rangeJ = float(max(dataSet[:,j]) - minJ)    <span class="comment"># 范围 = 最大值 - 最小值</span></span><br><span class="line">        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,<span class="number">1</span>))    <span class="comment"># 随机生成，mat为numpy函数，需要在最开始写上 from numpy import *</span></span><br><span class="line">    <span class="keyword">return</span> centroids</span><br></pre></td></tr></table></figure>

<h4 id="K-Means-聚类算法"><a href="#K-Means-聚类算法" class="headerlink" title="K-Means 聚类算法"></a>K-Means 聚类算法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># k-means 聚类算法</span></span><br><span class="line"><span class="comment"># 该算法会创建k个质心，然后将每个点分配到最近的质心，再重新计算质心。</span></span><br><span class="line"><span class="comment"># 这个过程重复数次，直到数据点的簇分配结果不再改变位置。</span></span><br><span class="line"><span class="comment"># 运行结果（多次运行结果可能会不一样，可以试试，原因为随机质心的影响，但总的结果是对的， 因为数据足够相似，也可能会陷入局部最小值）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]    <span class="comment"># 行数，即数据个数</span></span><br><span class="line">    clusterAssment = mat(zeros((m, <span class="number">2</span>)))    <span class="comment"># 创建一个与 dataSet 行数一样，但是有两列的矩阵，用来保存簇分配结果</span></span><br><span class="line">    centroids = createCent(dataSet, k)    <span class="comment"># 创建质心，随机k个质心</span></span><br><span class="line">    clusterChanged = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        clusterChanged = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):    <span class="comment"># 循环每一个数据点并分配到最近的质心中去</span></span><br><span class="line">            minDist = inf; minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">                distJI = distMeas(centroids[j,:],dataSet[i,:])    <span class="comment"># 计算数据点到质心的距离</span></span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:    <span class="comment"># 如果距离比 minDist（最小距离）还小，更新 minDist（最小距离）和最小质心的 index（索引）</span></span><br><span class="line">                    minDist = distJI; minIndex = j</span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex:    <span class="comment"># 簇分配结果改变</span></span><br><span class="line">                clusterChanged = <span class="literal">True</span>    <span class="comment"># 簇改变</span></span><br><span class="line">                clusterAssment[i, :] = minIndex,minDist**<span class="number">2</span>    <span class="comment"># 更新簇分配结果为最小质心的 index（索引），minDist（最小距离）的平方</span></span><br><span class="line">        <span class="keyword">print</span> centroids</span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k): <span class="comment"># 更新质心</span></span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A==cent)[<span class="number">0</span>]] <span class="comment"># 获取该簇中的所有点</span></span><br><span class="line">            centroids[cent,:] = mean(ptsInClust, axis=<span class="number">0</span>) <span class="comment"># 将质心修改为簇中所有点的平均值，mean 就是求平均值的</span></span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br></pre></td></tr></table></figure>

<h4 id="测试函数"><a href="#测试函数" class="headerlink" title="测试函数"></a>测试函数</h4><ol>
<li>测试一下以上的基础函数是否可以如预期运行, 请看: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py</a></li>
<li>测试一下 kMeans 函数是否可以如预期运行, 请看: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py</a> </li>
</ol>
<p>参考运行结果如下:<br><img src="http://data.apachecn.org/img/AiLearning/ml/10.KMeans/apachecn-k-means-run-result-1.jpg" alt="K-Means 运行结果1"></p>
<h3 id="K-Means-聚类算法的缺陷"><a href="#K-Means-聚类算法的缺陷" class="headerlink" title="K-Means 聚类算法的缺陷"></a>K-Means 聚类算法的缺陷</h3><blockquote>
<p>在 kMeans 的函数测试中，可能偶尔会陷入局部最小值（局部最优的结果，但不是全局最优的结果）.</p>
</blockquote>
<p>局部最小值的的情况如下:<br><img src="http://data.apachecn.org/img/AiLearning/ml/10.KMeans/apachecn-kmeans-partial-best-result-1.jpg" alt="K-Means 局部最小值1"><br>出现这个问题有很多原因，可能是k值取的不合适，可能是距离函数不合适，可能是最初随机选取的质心靠的太近，也可能是数据本身分布的问题。</p>
<p>为了解决这个问题，我们可以对生成的簇进行后处理，一种方法是将具有最大<strong>SSE</strong>值的簇划分成两个簇。具体实现时可以将最大簇包含的点过滤出来并在这些点上运行K-均值算法，令k设为2。</p>
<p>为了保持簇总数不变，可以将某两个簇进行合并。从上图中很明显就可以看出，应该将上图下部两个出错的簇质心进行合并。那么问题来了，我们可以很容易对二维数据上的聚类进行可视化， 但是如果遇到40维的数据应该如何去做？</p>
<p>有两种可以量化的办法：合并最近的质心，或者合并两个使得<strong>SSE</strong>增幅最小的质心。 第一种思路通过计算所有质心之间的距离， 然后合并距离最近的两个点来实现。第二种方法需要合并两个簇然后计算总<strong>SSE</strong>值。必须在所有可能的两个簇上重复上述处理过程，直到找到合并最佳的两个簇为止。</p>
<p>因为上述后处理过程实在是有些繁琐，所以有更厉害的大佬提出了另一个称之为二分K-均值（bisecting K-Means）的算法.   </p>
<h3 id="二分-K-Means-聚类算法"><a href="#二分-K-Means-聚类算法" class="headerlink" title="二分 K-Means 聚类算法"></a>二分 K-Means 聚类算法</h3><p>该算法首先将所有点作为一个簇，然后将该簇一分为二。<br>之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分时候可以最大程度降低 SSE（平方和误差）的值。<br>上述基于 SSE 的划分过程不断重复，直到得到用户指定的簇数目为止。  </p>
<h4 id="二分-K-Means-聚类算法伪代码"><a href="#二分-K-Means-聚类算法伪代码" class="headerlink" title="二分 K-Means 聚类算法伪代码"></a>二分 K-Means 聚类算法伪代码</h4><ul>
<li>将所有点看成一个簇</li>
<li>当簇数目小于 k 时</li>
<li>对于每一个簇<ul>
<li>计算总误差</li>
<li>在给定的簇上面进行 KMeans 聚类（k=2）</li>
<li>计算将该簇一分为二之后的总误差</li>
</ul>
</li>
<li>选择使得误差最小的那个簇进行划分操作</li>
</ul>
<p>另一种做法是选择 SSE 最大的簇进行划分，直到簇数目达到用户指定的数目位置。<br>接下来主要介绍该做法的python2代码实现</p>
<h4 id="二分-K-Means-聚类算法代码"><a href="#二分-K-Means-聚类算法代码" class="headerlink" title="二分 K-Means 聚类算法代码"></a>二分 K-Means 聚类算法代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 二分 KMeans 聚类算法, 基于 kMeans 基础之上的优化，以避免陷入局部最小值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">biKMeans</span><span class="params">(dataSet, k, distMeas=distEclud)</span>:</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="number">2</span>))) <span class="comment"># 保存每个数据点的簇分配结果和平方误差</span></span><br><span class="line">    centroid0 = mean(dataSet, axis=<span class="number">0</span>).tolist()[<span class="number">0</span>] <span class="comment"># 质心初始化为所有数据点的均值</span></span><br><span class="line">    centList =[centroid0] <span class="comment"># 初始化只有 1 个质心的 list</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m): <span class="comment"># 计算所有数据点到初始质心的距离平方误差</span></span><br><span class="line">        clusterAssment[j,<span class="number">1</span>] = distMeas(mat(centroid0), dataSet[j,:])**<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> (len(centList) &lt; k): <span class="comment"># 当质心数量小于 k 时</span></span><br><span class="line">        lowestSSE = inf</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(centList)): <span class="comment"># 对每一个质心</span></span><br><span class="line">            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:,<span class="number">0</span>].A==i)[<span class="number">0</span>],:] <span class="comment"># 获取当前簇 i 下的所有数据点</span></span><br><span class="line">            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, <span class="number">2</span>, distMeas) <span class="comment"># 将当前簇 i 进行二分 kMeans 处理</span></span><br><span class="line">            sseSplit = sum(splitClustAss[:,<span class="number">1</span>]) <span class="comment"># 将二分 kMeans 结果中的平方和的距离进行求和</span></span><br><span class="line">            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:,<span class="number">0</span>].A!=i)[<span class="number">0</span>],<span class="number">1</span>]) <span class="comment"># 将未参与二分 kMeans 分配结果中的平方和的距离进行求和</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">"sseSplit, and notSplit: "</span>,sseSplit,sseNotSplit</span><br><span class="line">            <span class="keyword">if</span> (sseSplit + sseNotSplit) &lt; lowestSSE: <span class="comment"># 总的（未拆分和已拆分）误差和越小，越相似，效果越优化，划分的结果更好（注意：这里的理解很重要，不明白的地方可以和我们一起讨论）</span></span><br><span class="line">                bestCentToSplit = i</span><br><span class="line">                bestNewCents = centroidMat</span><br><span class="line">                bestClustAss = splitClustAss.copy()</span><br><span class="line">                lowestSSE = sseSplit + sseNotSplit</span><br><span class="line">        <span class="comment"># 找出最好的簇分配结果    </span></span><br><span class="line">        bestClustAss[nonzero(bestClustAss[:,<span class="number">0</span>].A == <span class="number">1</span>)[<span class="number">0</span>],<span class="number">0</span>] = len(centList) <span class="comment"># 调用二分 kMeans 的结果，默认簇是 0,1. 当然也可以改成其它的数字</span></span><br><span class="line">        bestClustAss[nonzero(bestClustAss[:,<span class="number">0</span>].A == <span class="number">0</span>)[<span class="number">0</span>],<span class="number">0</span>] = bestCentToSplit <span class="comment"># 更新为最佳质心</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'the bestCentToSplit is: '</span>,bestCentToSplit</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'the len of bestClustAss is: '</span>, len(bestClustAss)</span><br><span class="line">        <span class="comment"># 更新质心列表</span></span><br><span class="line">        centList[bestCentToSplit] = bestNewCents[<span class="number">0</span>,:].tolist()[<span class="number">0</span>] <span class="comment"># 更新原质心 list 中的第 i 个质心为使用二分 kMeans 后 bestNewCents 的第一个质心</span></span><br><span class="line">        centList.append(bestNewCents[<span class="number">1</span>,:].tolist()[<span class="number">0</span>]) <span class="comment"># 添加 bestNewCents 的第二个质心</span></span><br><span class="line">        clusterAssment[nonzero(clusterAssment[:,<span class="number">0</span>].A == bestCentToSplit)[<span class="number">0</span>],:]= bestClustAss <span class="comment"># 重新分配最好簇下的数据（质心）以及SSE</span></span><br><span class="line">    <span class="keyword">return</span> mat(centList), clusterAssment</span><br></pre></td></tr></table></figure>

<h4 id="测试二分-KMeans-聚类算法"><a href="#测试二分-KMeans-聚类算法" class="headerlink" title="测试二分 KMeans 聚类算法"></a>测试二分 KMeans 聚类算法</h4><ul>
<li>测试一下二分 KMeans 聚类算法，请看: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/10.kmeans/kMeans.py</a></li>
</ul>
<p>上述函数可以运行多次，聚类会收敛到全局最小值，而原始的 kMeans() 函数偶尔会陷入局部最小值。<br>运行参考结果如下:<br><img src="http://data.apachecn.org/img/AiLearning/ml/10.KMeans/apachecn-bikmeans-run-result-1.jpg" alt="二分 K-Means 运行结果1"></p>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a>      </li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第13章 利用PCA来简化数据</title>
    <url>/2020/06/03/13.%E5%88%A9%E7%94%A8PCA%E6%9D%A5%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h2 id="降维技术"><a href="#降维技术" class="headerlink" title="降维技术"></a>降维技术</h2><blockquote>
<p>场景</p>
</blockquote>
<ul>
<li><p>我们正通过电视观看体育比赛，在电视的显示器上有一个球。</p>
</li>
<li><p>显示器大概包含了100万像素点，而球则可能是由较少的像素点组成，例如说一千个像素点。</p>
</li>
<li><p>人们实时的将显示器上的百万像素转换成为一个三维图像，该图像就给出运动场上球的位置。</p>
</li>
<li><p>在这个过程中，人们已经将百万像素点的数据，降至为三维。这个过程就称为<code>降维(dimensionality reduction)</code></p>
<a id="more"></a>
<blockquote>
<p>数据显示 并非大规模特征下的唯一难题，对数据进行简化还有如下一系列的原因：</p>
</blockquote>
</li>
<li><p>1) 使得数据集更容易使用</p>
</li>
<li><p>2) 降低很多算法的计算开销</p>
</li>
<li><p>3) 去除噪音</p>
</li>
<li><p>4) 使得结果易懂</p>
</li>
</ul>
<blockquote>
<p>适用范围: </p>
</blockquote>
<ul>
<li>在已标注与未标注的数据上都有降维技术。</li>
<li>这里我们将主要关注未标注数据上的降维技术，将技术同样也可以应用于已标注的数据。</li>
</ul>
<blockquote>
<p>在以下3种降维技术中， PCA的应用目前最为广泛，因此本章主要关注PCA。</p>
</blockquote>
<ul>
<li>1) 主成分分析(Principal Component Analysis, PCA)<ul>
<li><code>通俗理解：就是找出一个最主要的特征，然后进行分析。</code></li>
<li><code>例如： 考察一个人的智力情况，就直接看数学成绩就行(存在：数学、语文、英语成绩)</code></li>
</ul>
</li>
<li>2) 因子分析(Factor Analysis)<ul>
<li><code>通俗理解：将多个实测变量转换为少数几个综合指标。它反映一种降维的思想，通过降维将相关性高的变量聚在一起,从而减少需要分析的变量的数量,而减少问题分析的复杂性</code></li>
<li><code>例如： 考察一个人的整体情况，就直接组合3样成绩(隐变量)，看平均成绩就行(存在：数学、语文、英语成绩)</code></li>
<li>应用的领域：社会科学、金融和其他领域</li>
<li>在因子分析中，我们<ul>
<li>假设观察数据的成分中有一些观察不到的隐变量(latent variable)。</li>
<li>假设观察数据是这些隐变量和某些噪音的线性组合。</li>
<li>那么隐变量的数据可能比观察数据的数目少，也就说通过找到隐变量就可以实现数据的降维。</li>
</ul>
</li>
</ul>
</li>
<li>3) 独立成分分析(Independ Component Analysis, ICA)<ul>
<li><code>通俗理解：ICA 认为观测信号是若干个独立信号的线性组合，ICA 要做的是一个解混过程。</code></li>
<li><code>例如：我们去ktv唱歌，想辨别唱的是什么歌曲？ICA 是观察发现是原唱唱的一首歌【2个独立的声音（原唱／主唱）】。</code></li>
<li>ICA 是假设数据是从 N 个数据源混合组成的，这一点和因子分析有些类似，这些数据源之间在统计上是相互独立的，而在 PCA 中只假设数据是不 相关（线性关系）的。</li>
<li>同因子分析一样，如果数据源的数目少于观察数据的数目，则可以实现降维过程。</li>
</ul>
</li>
</ul>
<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><h3 id="PCA-概述"><a href="#PCA-概述" class="headerlink" title="PCA 概述"></a>PCA 概述</h3><p>主成分分析(Principal Component Analysis, PCA)：<code>通俗理解：就是找出一个最主要的特征，然后进行分析。</code></p>
<h3 id="PCA-场景"><a href="#PCA-场景" class="headerlink" title="PCA 场景"></a>PCA 场景</h3><p><code>例如： 考察一个人的智力情况，就直接看数学成绩就行(存在：数学、语文、英语成绩)</code></p>
<h3 id="PCA-原理"><a href="#PCA-原理" class="headerlink" title="PCA 原理"></a>PCA 原理</h3><blockquote>
<p>PCA 工作原理</p>
</blockquote>
<ol>
<li>找出第一个主成分的方向，也就是数据 <code>方差最大</code> 的方向。</li>
<li>找出第二个主成分的方向，也就是数据 <code>方差次大</code> 的方向，并且该方向与第一个主成分方向 <code>正交(orthogonal 如果是二维空间就叫垂直)</code>。</li>
<li>通过这种方式计算出所有的主成分方向。</li>
<li>通过数据集的协方差矩阵及其特征值分析，我们就可以得到这些主成分的值。</li>
<li>一旦得到了协方差矩阵的特征值和特征向量，我们就可以保留最大的 N 个特征。这些特征向量也给出了 N 个最重要特征的真实结构，我们就可以通过将数据乘上这 N 个特征向量 从而将它转换到新的空间上。</li>
</ol>
<p>为什么正交？</p>
<ol>
<li>正交是为了数据有效性损失最小</li>
<li>正交的一个原因是特征值的特征向量是正交的</li>
</ol>
<p>例如下图：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/13.PCA/%E5%BA%94%E7%94%A8PCA%E9%99%8D%E7%BB%B4.png" alt="应用PCA降维"></p>
<blockquote>
<p>PCA 优缺点</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">优点：降低数据的复杂性，识别最重要的多个特征。</span><br><span class="line">缺点：不一定需要，且可能损失有用信息。</span><br><span class="line">适用数据类型：数值型数据。</span><br></pre></td></tr></table></figure>

<h3 id="项目案例-对半导体数据进行降维处理"><a href="#项目案例-对半导体数据进行降维处理" class="headerlink" title="项目案例: 对半导体数据进行降维处理"></a>项目案例: 对半导体数据进行降维处理</h3><h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">半导体是在一些极为先进的工厂中制造出来的。设备的生命早期有限，并且花费极其巨大。</span><br><span class="line">虽然通过早期测试和频繁测试来发现有瑕疵的产品，但仍有一些存在瑕疵的产品通过测试。</span><br><span class="line">如果我们通过机器学习技术用于发现瑕疵产品，那么它就会为制造商节省大量的资金。</span><br><span class="line"></span><br><span class="line">具体来讲，它拥有<span class="number">590</span>个特征。我们看看能否对这些特征进行降维处理。</span><br><span class="line"></span><br><span class="line">对于数据的缺失值的问题，我们有一些处理方法(参考第<span class="number">5</span>章)</span><br><span class="line">目前该章节处理的方案是：将缺失值NaN(Not a Number缩写)，全部用平均值来替代(如果用<span class="number">0</span>来处理的策略就太差劲了)。</span><br></pre></td></tr></table></figure>

<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><blockquote>
<p>收集数据：提供文本文件</p>
</blockquote>
<p>文件名：secom.data</p>
<p>文本文件数据格式如下：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">3030.93</span> <span class="number">2564</span> <span class="number">2187.7333</span> <span class="number">1411.1265</span> <span class="number">1.3602</span> <span class="number">100</span> <span class="number">97.6133</span> <span class="number">0.1242</span> <span class="number">1.5005</span> <span class="number">0.0162</span> <span class="number">-0.0034</span> <span class="number">0.9455</span> <span class="number">202.4396</span> <span class="number">0</span> <span class="number">7.9558</span> <span class="number">414.871</span> <span class="number">10.0433</span> <span class="number">0.968</span> <span class="number">192.3963</span> <span class="number">12.519</span> <span class="number">1.4026</span> <span class="number">-5419</span> <span class="number">2916.5</span> <span class="number">-4043.75</span> <span class="number">751</span> <span class="number">0.8955</span> <span class="number">1.773</span> <span class="number">3.049</span> <span class="number">64.2333</span> <span class="number">2.0222</span> <span class="number">0.1632</span> <span class="number">3.5191</span> <span class="number">83.3971</span> <span class="number">9.5126</span> <span class="number">50.617</span> <span class="number">64.2588</span> <span class="number">49.383</span> <span class="number">66.3141</span> <span class="number">86.9555</span> <span class="number">117.5132</span> <span class="number">61.29</span> <span class="number">4.515</span> <span class="number">70</span> <span class="number">352.7173</span> <span class="number">10.1841</span> <span class="number">130.3691</span> <span class="number">723.3092</span> <span class="number">1.3072</span> <span class="number">141.2282</span> <span class="number">1</span> <span class="number">624.3145</span> <span class="number">218.3174</span> <span class="number">0</span> <span class="number">4.592</span> <span class="number">4.841</span> <span class="number">2834</span> <span class="number">0.9317</span> <span class="number">0.9484</span> <span class="number">4.7057</span> <span class="number">-1.7264</span> <span class="number">350.9264</span> <span class="number">10.6231</span> <span class="number">108.6427</span> <span class="number">16.1445</span> <span class="number">21.7264</span> <span class="number">29.5367</span> <span class="number">693.7724</span> <span class="number">0.9226</span> <span class="number">148.6009</span> <span class="number">1</span> <span class="number">608.17</span> <span class="number">84.0793</span> NaN NaN <span class="number">0</span> <span class="number">0.0126</span> <span class="number">-0.0206</span> <span class="number">0.0141</span> <span class="number">-0.0307</span> <span class="number">-0.0083</span> <span class="number">-0.0026</span> <span class="number">-0.0567</span> <span class="number">-0.0044</span> <span class="number">7.2163</span> <span class="number">0.132</span> NaN <span class="number">2.3895</span> <span class="number">0.969</span> <span class="number">1747.6049</span> <span class="number">0.1841</span> <span class="number">8671.9301</span> <span class="number">-0.3274</span> <span class="number">-0.0055</span> <span class="number">-0.0001</span> <span class="number">0.0001</span> <span class="number">0.0003</span> <span class="number">-0.2786</span> <span class="number">0</span> <span class="number">0.3974</span> <span class="number">-0.0251</span> <span class="number">0.0002</span> <span class="number">0.0002</span> <span class="number">0.135</span> <span class="number">-0.0042</span> <span class="number">0.0003</span> <span class="number">0.0056</span> <span class="number">0</span> <span class="number">-0.2468</span> <span class="number">0.3196</span> NaN NaN NaN NaN <span class="number">0.946</span> <span class="number">0</span> <span class="number">748.6115</span> <span class="number">0.9908</span> <span class="number">58.4306</span> <span class="number">0.6002</span> <span class="number">0.9804</span> <span class="number">6.3788</span> <span class="number">15.88</span> <span class="number">2.639</span> <span class="number">15.94</span> <span class="number">15.93</span> <span class="number">0.8656</span> <span class="number">3.353</span> <span class="number">0.4098</span> <span class="number">3.188</span> <span class="number">-0.0473</span> <span class="number">0.7243</span> <span class="number">0.996</span> <span class="number">2.2967</span> <span class="number">1000.7263</span> <span class="number">39.2373</span> <span class="number">123</span> <span class="number">111.3</span> <span class="number">75.2</span> <span class="number">46.2</span> <span class="number">350.671</span> <span class="number">0.3948</span> <span class="number">0</span> <span class="number">6.78</span> <span class="number">0.0034</span> <span class="number">0.0898</span> <span class="number">0.085</span> <span class="number">0.0358</span> <span class="number">0.0328</span> <span class="number">12.2566</span> <span class="number">0</span> <span class="number">4.271</span> <span class="number">10.284</span> <span class="number">0.4734</span> <span class="number">0.0167</span> <span class="number">11.8901</span> <span class="number">0.41</span> <span class="number">0.0506</span> NaN NaN <span class="number">1017</span> <span class="number">967</span> <span class="number">1066</span> <span class="number">368</span> <span class="number">0.09</span> <span class="number">0.048</span> <span class="number">0.095</span> <span class="number">2</span> <span class="number">0.9</span> <span class="number">0.069</span> <span class="number">0.046</span> <span class="number">0.725</span> <span class="number">0.1139</span> <span class="number">0.3183</span> <span class="number">0.5888</span> <span class="number">0.3184</span> <span class="number">0.9499</span> <span class="number">0.3979</span> <span class="number">0.16</span> <span class="number">0</span> <span class="number">0</span> <span class="number">20.95</span> <span class="number">0.333</span> <span class="number">12.49</span> <span class="number">16.713</span> <span class="number">0.0803</span> <span class="number">5.72</span> <span class="number">0</span> <span class="number">11.19</span> <span class="number">65.363</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.292</span> <span class="number">5.38</span> <span class="number">20.1</span> <span class="number">0.296</span> <span class="number">10.62</span> <span class="number">10.3</span> <span class="number">5.38</span> <span class="number">4.04</span> <span class="number">16.23</span> <span class="number">0.2951</span> <span class="number">8.64</span> <span class="number">0</span> <span class="number">10.3</span> <span class="number">97.314</span> <span class="number">0</span> <span class="number">0.0772</span> <span class="number">0.0599</span> <span class="number">0.07</span> <span class="number">0.0547</span> <span class="number">0.0704</span> <span class="number">0.052</span> <span class="number">0.0301</span> <span class="number">0.1135</span> <span class="number">3.4789</span> <span class="number">0.001</span> NaN <span class="number">0.0707</span> <span class="number">0.0211</span> <span class="number">175.2173</span> <span class="number">0.0315</span> <span class="number">1940.3994</span> <span class="number">0</span> <span class="number">0.0744</span> <span class="number">0.0546</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0027</span> <span class="number">0.004</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN NaN <span class="number">0.0188</span> <span class="number">0</span> <span class="number">219.9453</span> <span class="number">0.0011</span> <span class="number">2.8374</span> <span class="number">0.0189</span> <span class="number">0.005</span> <span class="number">0.4269</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0472</span> <span class="number">40.855</span> <span class="number">4.5152</span> <span class="number">30.9815</span> <span class="number">33.9606</span> <span class="number">22.9057</span> <span class="number">15.9525</span> <span class="number">110.2144</span> <span class="number">0.131</span> <span class="number">0</span> <span class="number">2.5883</span> <span class="number">0.001</span> <span class="number">0.0319</span> <span class="number">0.0197</span> <span class="number">0.012</span> <span class="number">0.0109</span> <span class="number">3.9321</span> <span class="number">0</span> <span class="number">1.5123</span> <span class="number">3.5811</span> <span class="number">0.1337</span> <span class="number">0.0055</span> <span class="number">3.8447</span> <span class="number">0.1077</span> <span class="number">0.0167</span> NaN NaN <span class="number">418.1363</span> <span class="number">398.3185</span> <span class="number">496.1582</span> <span class="number">158.333</span> <span class="number">0.0373</span> <span class="number">0.0202</span> <span class="number">0.0462</span> <span class="number">0.6083</span> <span class="number">0.3032</span> <span class="number">0.02</span> <span class="number">0.0174</span> <span class="number">0.2827</span> <span class="number">0.0434</span> <span class="number">0.1342</span> <span class="number">0.2419</span> <span class="number">0.1343</span> <span class="number">0.367</span> <span class="number">0.1431</span> <span class="number">0.061</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">6.2698</span> <span class="number">0.1181</span> <span class="number">3.8208</span> <span class="number">5.3737</span> <span class="number">0.0254</span> <span class="number">1.6252</span> <span class="number">0</span> <span class="number">3.2461</span> <span class="number">18.0118</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0752</span> <span class="number">1.5989</span> <span class="number">6.5893</span> <span class="number">0.0913</span> <span class="number">3.0911</span> <span class="number">8.4654</span> <span class="number">1.5989</span> <span class="number">1.2293</span> <span class="number">5.3406</span> <span class="number">0.0867</span> <span class="number">2.8551</span> <span class="number">0</span> <span class="number">2.9971</span> <span class="number">31.8843</span> NaN NaN <span class="number">0</span> <span class="number">0.0215</span> <span class="number">0.0274</span> <span class="number">0.0315</span> <span class="number">0.0238</span> <span class="number">0.0206</span> <span class="number">0.0238</span> <span class="number">0.0144</span> <span class="number">0.0491</span> <span class="number">1.2708</span> <span class="number">0.0004</span> NaN <span class="number">0.0229</span> <span class="number">0.0065</span> <span class="number">55.2039</span> <span class="number">0.0105</span> <span class="number">560.2658</span> <span class="number">0</span> <span class="number">0.017</span> <span class="number">0.0148</span> <span class="number">0.0124</span> <span class="number">0.0114</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.001</span> <span class="number">0.0013</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN NaN <span class="number">0.0055</span> <span class="number">0</span> <span class="number">61.5932</span> <span class="number">0.0003</span> <span class="number">0.9967</span> <span class="number">0.0082</span> <span class="number">0.0017</span> <span class="number">0.1437</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0151</span> <span class="number">14.2396</span> <span class="number">1.4392</span> <span class="number">5.6188</span> <span class="number">3.6721</span> <span class="number">2.9329</span> <span class="number">2.1118</span> <span class="number">24.8504</span> <span class="number">29.0271</span> <span class="number">0</span> <span class="number">6.9458</span> <span class="number">2.738</span> <span class="number">5.9846</span> <span class="number">525.0965</span> <span class="number">0</span> <span class="number">3.4641</span> <span class="number">6.0544</span> <span class="number">0</span> <span class="number">53.684</span> <span class="number">2.4788</span> <span class="number">4.7141</span> <span class="number">1.7275</span> <span class="number">6.18</span> <span class="number">3.275</span> <span class="number">3.6084</span> <span class="number">18.7673</span> <span class="number">33.1562</span> <span class="number">26.3617</span> <span class="number">49.0013</span> <span class="number">10.0503</span> <span class="number">2.7073</span> <span class="number">3.1158</span> <span class="number">3.1136</span> <span class="number">44.5055</span> <span class="number">42.2737</span> <span class="number">1.3071</span> <span class="number">0.8693</span> <span class="number">1.1975</span> <span class="number">0.6288</span> <span class="number">0.9163</span> <span class="number">0.6448</span> <span class="number">1.4324</span> <span class="number">0.4576</span> <span class="number">0.1362</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">5.9396</span> <span class="number">3.2698</span> <span class="number">9.5805</span> <span class="number">2.3106</span> <span class="number">6.1463</span> <span class="number">4.0502</span> <span class="number">0</span> <span class="number">1.7924</span> <span class="number">29.9394</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">6.2052</span> <span class="number">311.6377</span> <span class="number">5.7277</span> <span class="number">2.7864</span> <span class="number">9.7752</span> <span class="number">63.7987</span> <span class="number">24.7625</span> <span class="number">13.6778</span> <span class="number">2.3394</span> <span class="number">31.9893</span> <span class="number">5.8142</span> <span class="number">0</span> <span class="number">1.6936</span> <span class="number">115.7408</span> <span class="number">0</span> <span class="number">613.3069</span> <span class="number">291.4842</span> <span class="number">494.6996</span> <span class="number">178.1759</span> <span class="number">843.1138</span> <span class="number">0</span> <span class="number">53.1098</span> <span class="number">0</span> <span class="number">48.2091</span> <span class="number">0.7578</span> NaN <span class="number">2.957</span> <span class="number">2.1739</span> <span class="number">10.0261</span> <span class="number">17.1202</span> <span class="number">22.3756</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">64.6707</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN NaN <span class="number">1.9864</span> <span class="number">0</span> <span class="number">29.3804</span> <span class="number">0.1094</span> <span class="number">4.856</span> <span class="number">3.1406</span> <span class="number">0.5064</span> <span class="number">6.6926</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2.057</span> <span class="number">4.0825</span> <span class="number">11.5074</span> <span class="number">0.1096</span> <span class="number">0.0078</span> <span class="number">0.0026</span> <span class="number">7.116</span> <span class="number">1.0616</span> <span class="number">395.57</span> <span class="number">75.752</span> <span class="number">0.4234</span> <span class="number">12.93</span> <span class="number">0.78</span> <span class="number">0.1827</span> <span class="number">5.7349</span> <span class="number">0.3363</span> <span class="number">39.8842</span> <span class="number">3.2687</span> <span class="number">1.0297</span> <span class="number">1.0344</span> <span class="number">0.4385</span> <span class="number">0.1039</span> <span class="number">42.3877</span> NaN NaN NaN NaN NaN NaN NaN NaN <span class="number">533.85</span> <span class="number">2.1113</span> <span class="number">8.95</span> <span class="number">0.3157</span> <span class="number">3.0624</span> <span class="number">0.1026</span> <span class="number">1.6765</span> <span class="number">14.9509</span> NaN NaN NaN NaN <span class="number">0.5005</span> <span class="number">0.0118</span> <span class="number">0.0035</span> <span class="number">2.363</span> NaN NaN NaN NaN</span><br><span class="line"><span class="number">3095.78</span> <span class="number">2465.14</span> <span class="number">2230.4222</span> <span class="number">1463.6606</span> <span class="number">0.8294</span> <span class="number">100</span> <span class="number">102.3433</span> <span class="number">0.1247</span> <span class="number">1.4966</span> <span class="number">-0.0005</span> <span class="number">-0.0148</span> <span class="number">0.9627</span> <span class="number">200.547</span> <span class="number">0</span> <span class="number">10.1548</span> <span class="number">414.7347</span> <span class="number">9.2599</span> <span class="number">0.9701</span> <span class="number">191.2872</span> <span class="number">12.4608</span> <span class="number">1.3825</span> <span class="number">-5441.5</span> <span class="number">2604.25</span> <span class="number">-3498.75</span> <span class="number">-1640.25</span> <span class="number">1.2973</span> <span class="number">2.0143</span> <span class="number">7.39</span> <span class="number">68.4222</span> <span class="number">2.2667</span> <span class="number">0.2102</span> <span class="number">3.4171</span> <span class="number">84.9052</span> <span class="number">9.7997</span> <span class="number">50.6596</span> <span class="number">64.2828</span> <span class="number">49.3404</span> <span class="number">64.9193</span> <span class="number">87.5241</span> <span class="number">118.1188</span> <span class="number">78.25</span> <span class="number">2.773</span> <span class="number">70</span> <span class="number">352.2445</span> <span class="number">10.0373</span> <span class="number">133.1727</span> <span class="number">724.8264</span> <span class="number">1.2887</span> <span class="number">145.8445</span> <span class="number">1</span> <span class="number">631.2618</span> <span class="number">205.1695</span> <span class="number">0</span> <span class="number">4.59</span> <span class="number">4.842</span> <span class="number">2853</span> <span class="number">0.9324</span> <span class="number">0.9479</span> <span class="number">4.682</span> <span class="number">0.8073</span> <span class="number">352.0073</span> <span class="number">10.3092</span> <span class="number">113.98</span> <span class="number">10.9036</span> <span class="number">19.1927</span> <span class="number">27.6301</span> <span class="number">697.1964</span> <span class="number">1.1598</span> <span class="number">154.3709</span> <span class="number">1</span> <span class="number">620.3582</span> <span class="number">82.3494</span> NaN NaN <span class="number">0</span> <span class="number">-0.0039</span> <span class="number">-0.0198</span> <span class="number">0.0004</span> <span class="number">-0.044</span> <span class="number">-0.0358</span> <span class="number">-0.012</span> <span class="number">-0.0377</span> <span class="number">0.0017</span> <span class="number">6.8043</span> <span class="number">0.1358</span> NaN <span class="number">2.3754</span> <span class="number">0.9894</span> <span class="number">1931.6464</span> <span class="number">0.1874</span> <span class="number">8407.0299</span> <span class="number">0.1455</span> <span class="number">-0.0015</span> <span class="number">0</span> <span class="number">-0.0005</span> <span class="number">0.0001</span> <span class="number">0.5854</span> <span class="number">0</span> <span class="number">-0.9353</span> <span class="number">-0.0158</span> <span class="number">-0.0004</span> <span class="number">-0.0004</span> <span class="number">-0.0752</span> <span class="number">-0.0045</span> <span class="number">0.0002</span> <span class="number">0.0015</span> <span class="number">0</span> <span class="number">0.0772</span> <span class="number">-0.0903</span> NaN NaN NaN NaN <span class="number">0.9425</span> <span class="number">0</span> <span class="number">731.2517</span> <span class="number">0.9902</span> <span class="number">58.668</span> <span class="number">0.5958</span> <span class="number">0.9731</span> <span class="number">6.5061</span> <span class="number">15.88</span> <span class="number">2.541</span> <span class="number">15.91</span> <span class="number">15.88</span> <span class="number">0.8703</span> <span class="number">2.771</span> <span class="number">0.4138</span> <span class="number">3.272</span> <span class="number">-0.0946</span> <span class="number">0.8122</span> <span class="number">0.9985</span> <span class="number">2.2932</span> <span class="number">998.1081</span> <span class="number">37.9213</span> <span class="number">98</span> <span class="number">80.3</span> <span class="number">81</span> <span class="number">56.2</span> <span class="number">219.7679</span> <span class="number">0.2301</span> <span class="number">0</span> <span class="number">5.7</span> <span class="number">0.0049</span> <span class="number">0.1356</span> <span class="number">0.06</span> <span class="number">0.0547</span> <span class="number">0.0204</span> <span class="number">12.3319</span> <span class="number">0</span> <span class="number">6.285</span> <span class="number">13.077</span> <span class="number">0.5666</span> <span class="number">0.0144</span> <span class="number">11.8428</span> <span class="number">0.35</span> <span class="number">0.0437</span> NaN NaN <span class="number">568</span> <span class="number">59</span> <span class="number">297</span> <span class="number">3277</span> <span class="number">0.112</span> <span class="number">0.115</span> <span class="number">0.124</span> <span class="number">2.2</span> <span class="number">1.1</span> <span class="number">0.079</span> <span class="number">0.561</span> <span class="number">1.0498</span> <span class="number">0.1917</span> <span class="number">0.4115</span> <span class="number">0.6582</span> <span class="number">0.4115</span> <span class="number">1.0181</span> <span class="number">0.2315</span> <span class="number">0.325</span> <span class="number">0</span> <span class="number">0</span> <span class="number">17.99</span> <span class="number">0.439</span> <span class="number">10.14</span> <span class="number">16.358</span> <span class="number">0.0892</span> <span class="number">6.92</span> <span class="number">0</span> <span class="number">9.05</span> <span class="number">82.986</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.222</span> <span class="number">3.74</span> <span class="number">19.59</span> <span class="number">0.316</span> <span class="number">11.65</span> <span class="number">8.02</span> <span class="number">3.74</span> <span class="number">3.659</span> <span class="number">15.078</span> <span class="number">0.358</span> <span class="number">8.96</span> <span class="number">0</span> <span class="number">8.02</span> <span class="number">134.25</span> <span class="number">0</span> <span class="number">0.0566</span> <span class="number">0.0488</span> <span class="number">0.1651</span> <span class="number">0.1578</span> <span class="number">0.0468</span> <span class="number">0.0987</span> <span class="number">0.0734</span> <span class="number">0.0747</span> <span class="number">3.9578</span> <span class="number">0.005</span> NaN <span class="number">0.0761</span> <span class="number">0.0014</span> <span class="number">128.4285</span> <span class="number">0.0238</span> <span class="number">1988</span> <span class="number">0</span> <span class="number">0.0203</span> <span class="number">0.0236</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0064</span> <span class="number">0.0036</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN NaN <span class="number">0.0154</span> <span class="number">0</span> <span class="number">193.0287</span> <span class="number">0.0007</span> <span class="number">3.8999</span> <span class="number">0.0187</span> <span class="number">0.0086</span> <span class="number">0.5749</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0411</span> <span class="number">29.743</span> <span class="number">3.6327</span> <span class="number">29.0598</span> <span class="number">28.9862</span> <span class="number">22.3163</span> <span class="number">17.4008</span> <span class="number">83.5542</span> <span class="number">0.0767</span> <span class="number">0</span> <span class="number">1.8459</span> <span class="number">0.0012</span> <span class="number">0.044</span> <span class="number">0.0171</span> <span class="number">0.0154</span> <span class="number">0.0069</span> <span class="number">3.9011</span> <span class="number">0</span> <span class="number">2.1016</span> <span class="number">3.9483</span> <span class="number">0.1662</span> <span class="number">0.0049</span> <span class="number">3.7836</span> <span class="number">0.1</span> <span class="number">0.0139</span> NaN NaN <span class="number">233.9865</span> <span class="number">26.5879</span> <span class="number">139.2082</span> <span class="number">1529.7622</span> <span class="number">0.0502</span> <span class="number">0.0561</span> <span class="number">0.0591</span> <span class="number">0.8151</span> <span class="number">0.3464</span> <span class="number">0.0291</span> <span class="number">0.1822</span> <span class="number">0.3814</span> <span class="number">0.0715</span> <span class="number">0.1667</span> <span class="number">0.263</span> <span class="number">0.1667</span> <span class="number">0.3752</span> <span class="number">0.0856</span> <span class="number">0.1214</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">5.6522</span> <span class="number">0.1417</span> <span class="number">2.9939</span> <span class="number">5.2445</span> <span class="number">0.0264</span> <span class="number">1.8045</span> <span class="number">0</span> <span class="number">2.7661</span> <span class="number">23.623</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0778</span> <span class="number">1.1506</span> <span class="number">5.9247</span> <span class="number">0.0878</span> <span class="number">3.3604</span> <span class="number">7.7421</span> <span class="number">1.1506</span> <span class="number">1.1265</span> <span class="number">5.0108</span> <span class="number">0.1013</span> <span class="number">2.4278</span> <span class="number">0</span> <span class="number">2.489</span> <span class="number">41.708</span> NaN NaN <span class="number">0</span> <span class="number">0.0142</span> <span class="number">0.023</span> <span class="number">0.0768</span> <span class="number">0.0729</span> <span class="number">0.0143</span> <span class="number">0.0513</span> <span class="number">0.0399</span> <span class="number">0.0365</span> <span class="number">1.2474</span> <span class="number">0.0017</span> NaN <span class="number">0.0248</span> <span class="number">0.0005</span> <span class="number">46.3453</span> <span class="number">0.0069</span> <span class="number">677.1873</span> <span class="number">0</span> <span class="number">0.0053</span> <span class="number">0.0059</span> <span class="number">0.0081</span> <span class="number">0.0033</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0022</span> <span class="number">0.0013</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN NaN <span class="number">0.0049</span> <span class="number">0</span> <span class="number">65.0999</span> <span class="number">0.0002</span> <span class="number">1.1655</span> <span class="number">0.0068</span> <span class="number">0.0027</span> <span class="number">0.1921</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.012</span> <span class="number">10.5837</span> <span class="number">1.0323</span> <span class="number">4.3465</span> <span class="number">2.5939</span> <span class="number">3.2858</span> <span class="number">2.5197</span> <span class="number">15.015</span> <span class="number">27.7464</span> <span class="number">0</span> <span class="number">5.5695</span> <span class="number">3.93</span> <span class="number">9.0604</span> <span class="number">0</span> <span class="number">368.9713</span> <span class="number">2.1196</span> <span class="number">6.1491</span> <span class="number">0</span> <span class="number">61.8918</span> <span class="number">3.1531</span> <span class="number">6.1188</span> <span class="number">1.4857</span> <span class="number">6.1911</span> <span class="number">2.8088</span> <span class="number">3.1595</span> <span class="number">10.4383</span> <span class="number">2.2655</span> <span class="number">8.4887</span> <span class="number">199.7866</span> <span class="number">8.6336</span> <span class="number">5.7093</span> <span class="number">1.6779</span> <span class="number">3.2153</span> <span class="number">48.5294</span> <span class="number">37.5793</span> <span class="number">16.4174</span> <span class="number">1.2364</span> <span class="number">1.9562</span> <span class="number">0.8123</span> <span class="number">1.0239</span> <span class="number">0.834</span> <span class="number">1.5683</span> <span class="number">0.2645</span> <span class="number">0.2751</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">5.1072</span> <span class="number">4.3737</span> <span class="number">7.6142</span> <span class="number">2.2568</span> <span class="number">6.9233</span> <span class="number">4.7448</span> <span class="number">0</span> <span class="number">1.4336</span> <span class="number">40.4475</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">4.7415</span> <span class="number">463.2883</span> <span class="number">5.5652</span> <span class="number">3.0652</span> <span class="number">10.2211</span> <span class="number">73.5536</span> <span class="number">19.4865</span> <span class="number">13.243</span> <span class="number">2.1627</span> <span class="number">30.8643</span> <span class="number">5.8042</span> <span class="number">0</span> <span class="number">1.2928</span> <span class="number">163.0249</span> <span class="number">0</span> <span class="number">0</span> <span class="number">246.7762</span> <span class="number">0</span> <span class="number">359.0444</span> <span class="number">130.635</span> <span class="number">820.79</span> <span class="number">194.4371</span> <span class="number">0</span> <span class="number">58.1666</span> <span class="number">3.6822</span> NaN <span class="number">3.2029</span> <span class="number">0.1441</span> <span class="number">6.6487</span> <span class="number">12.6788</span> <span class="number">23.6469</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">141.4365</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN NaN <span class="number">1.6292</span> <span class="number">0</span> <span class="number">26.397</span> <span class="number">0.0673</span> <span class="number">6.6475</span> <span class="number">3.131</span> <span class="number">0.8832</span> <span class="number">8.837</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1.791</span> <span class="number">2.9799</span> <span class="number">9.5796</span> <span class="number">0.1096</span> <span class="number">0.0078</span> <span class="number">0.0026</span> <span class="number">7.116</span> <span class="number">1.3526</span> <span class="number">408.798</span> <span class="number">74.64</span> <span class="number">0.7193</span> <span class="number">16</span> <span class="number">1.33</span> <span class="number">0.2829</span> <span class="number">7.1196</span> <span class="number">0.4989</span> <span class="number">53.1836</span> <span class="number">3.9139</span> <span class="number">1.7819</span> <span class="number">0.9634</span> <span class="number">0.1745</span> <span class="number">0.0375</span> <span class="number">18.1087</span> NaN NaN NaN NaN NaN NaN NaN NaN <span class="number">535.0164</span> <span class="number">2.4335</span> <span class="number">5.92</span> <span class="number">0.2653</span> <span class="number">2.0111</span> <span class="number">0.0772</span> <span class="number">1.1065</span> <span class="number">10.9003</span> <span class="number">0.0096</span> <span class="number">0.0201</span> <span class="number">0.006</span> <span class="number">208.2045</span> <span class="number">0.5019</span> <span class="number">0.0223</span> <span class="number">0.0055</span> <span class="number">4.4447</span> <span class="number">0.0096</span> <span class="number">0.0201</span> <span class="number">0.006</span> <span class="number">208.2045</span></span><br><span class="line"><span class="number">2932.61</span> <span class="number">2559.94</span> <span class="number">2186.4111</span> <span class="number">1698.0172</span> <span class="number">1.5102</span> <span class="number">100</span> <span class="number">95.4878</span> <span class="number">0.1241</span> <span class="number">1.4436</span> <span class="number">0.0041</span> <span class="number">0.0013</span> <span class="number">0.9615</span> <span class="number">202.0179</span> <span class="number">0</span> <span class="number">9.5157</span> <span class="number">416.7075</span> <span class="number">9.3144</span> <span class="number">0.9674</span> <span class="number">192.7035</span> <span class="number">12.5404</span> <span class="number">1.4123</span> <span class="number">-5447.75</span> <span class="number">2701.75</span> <span class="number">-4047</span> <span class="number">-1916.5</span> <span class="number">1.3122</span> <span class="number">2.0295</span> <span class="number">7.5788</span> <span class="number">67.1333</span> <span class="number">2.3333</span> <span class="number">0.1734</span> <span class="number">3.5986</span> <span class="number">84.7569</span> <span class="number">8.659</span> <span class="number">50.153</span> <span class="number">64.1114</span> <span class="number">49.847</span> <span class="number">65.8389</span> <span class="number">84.7327</span> <span class="number">118.6128</span> <span class="number">14.37</span> <span class="number">5.434</span> <span class="number">70</span> <span class="number">364.3782</span> <span class="number">9.8783</span> <span class="number">131.8027</span> <span class="number">734.7924</span> <span class="number">1.2992</span> <span class="number">141.0845</span> <span class="number">1</span> <span class="number">637.2655</span> <span class="number">185.7574</span> <span class="number">0</span> <span class="number">4.486</span> <span class="number">4.748</span> <span class="number">2936</span> <span class="number">0.9139</span> <span class="number">0.9447</span> <span class="number">4.5873</span> <span class="number">23.8245</span> <span class="number">364.5364</span> <span class="number">10.1685</span> <span class="number">115.6273</span> <span class="number">11.3019</span> <span class="number">16.1755</span> <span class="number">24.2829</span> <span class="number">710.5095</span> <span class="number">0.8694</span> <span class="number">145.8</span> <span class="number">1</span> <span class="number">625.9636</span> <span class="number">84.7681</span> <span class="number">140.6972</span> <span class="number">485.2665</span> <span class="number">0</span> <span class="number">-0.0078</span> <span class="number">-0.0326</span> <span class="number">-0.0052</span> <span class="number">0.0213</span> <span class="number">-0.0054</span> <span class="number">-0.1134</span> <span class="number">-0.0182</span> <span class="number">0.0287</span> <span class="number">7.1041</span> <span class="number">0.1362</span> NaN <span class="number">2.4532</span> <span class="number">0.988</span> <span class="number">1685.8514</span> <span class="number">0.1497</span> <span class="number">9317.1698</span> <span class="number">0.0553</span> <span class="number">0.0006</span> <span class="number">-0.0013</span> <span class="number">0</span> <span class="number">0.0002</span> <span class="number">-0.1343</span> <span class="number">0</span> <span class="number">-0.1427</span> <span class="number">0.1218</span> <span class="number">0.0006</span> <span class="number">-0.0001</span> <span class="number">0.0134</span> <span class="number">-0.0026</span> <span class="number">-0.0016</span> <span class="number">-0.0006</span> <span class="number">0.0013</span> <span class="number">-0.0301</span> <span class="number">-0.0728</span> NaN NaN NaN <span class="number">0.4684</span> <span class="number">0.9231</span> <span class="number">0</span> <span class="number">718.5777</span> <span class="number">0.9899</span> <span class="number">58.4808</span> <span class="number">0.6015</span> <span class="number">0.9772</span> <span class="number">6.4527</span> <span class="number">15.9</span> <span class="number">2.882</span> <span class="number">15.94</span> <span class="number">15.95</span> <span class="number">0.8798</span> <span class="number">3.094</span> <span class="number">0.4777</span> <span class="number">3.272</span> <span class="number">-0.1892</span> <span class="number">0.8194</span> <span class="number">0.9978</span> <span class="number">2.2592</span> <span class="number">998.444</span> <span class="number">42.0579</span> <span class="number">89</span> <span class="number">126.4</span> <span class="number">96.5</span> <span class="number">45.1001</span> <span class="number">306.038</span> <span class="number">0.3263</span> <span class="number">0</span> <span class="number">8.33</span> <span class="number">0.0038</span> <span class="number">0.0754</span> <span class="number">0.0483</span> <span class="number">0.0619</span> <span class="number">0.0221</span> <span class="number">8.266</span> <span class="number">0</span> <span class="number">4.819</span> <span class="number">8.443</span> <span class="number">0.4909</span> <span class="number">0.0177</span> <span class="number">8.2054</span> <span class="number">0.47</span> <span class="number">0.0497</span> NaN NaN <span class="number">562</span> <span class="number">788</span> <span class="number">759</span> <span class="number">2100</span> <span class="number">0.187</span> <span class="number">0.117</span> <span class="number">0.068</span> <span class="number">2.1</span> <span class="number">1.4</span> <span class="number">0.123</span> <span class="number">0.319</span> <span class="number">1.0824</span> <span class="number">0.0369</span> <span class="number">0.3141</span> <span class="number">0.5753</span> <span class="number">0.3141</span> <span class="number">0.9677</span> <span class="number">0.2706</span> <span class="number">0.326</span> <span class="number">0</span> <span class="number">0</span> <span class="number">17.78</span> <span class="number">0.745</span> <span class="number">13.31</span> <span class="number">22.912</span> <span class="number">0.1959</span> <span class="number">9.21</span> <span class="number">0</span> <span class="number">17.87</span> <span class="number">60.11</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.139</span> <span class="number">5.09</span> <span class="number">19.75</span> <span class="number">0.949</span> <span class="number">9.71</span> <span class="number">16.73</span> <span class="number">5.09</span> <span class="number">11.059</span> <span class="number">22.624</span> <span class="number">0.1164</span> <span class="number">13.3</span> <span class="number">0</span> <span class="number">16.73</span> <span class="number">79.618</span> <span class="number">0</span> <span class="number">0.0339</span> <span class="number">0.0494</span> <span class="number">0.0696</span> <span class="number">0.0406</span> <span class="number">0.0401</span> <span class="number">0.084</span> <span class="number">0.0349</span> <span class="number">0.0718</span> <span class="number">2.4266</span> <span class="number">0.0014</span> NaN <span class="number">0.0963</span> <span class="number">0.0152</span> <span class="number">182.4956</span> <span class="number">0.0284</span> <span class="number">839.6006</span> <span class="number">0</span> <span class="number">0.0192</span> <span class="number">0.017</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0062</span> <span class="number">0.004</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN <span class="number">0.1729</span> <span class="number">0.0273</span> <span class="number">0</span> <span class="number">104.4042</span> <span class="number">0.0007</span> <span class="number">4.1446</span> <span class="number">0.0733</span> <span class="number">0.0063</span> <span class="number">0.4166</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0487</span> <span class="number">29.621</span> <span class="number">3.9133</span> <span class="number">23.551</span> <span class="number">41.3837</span> <span class="number">32.6256</span> <span class="number">15.7716</span> <span class="number">97.3868</span> <span class="number">0.1117</span> <span class="number">0</span> <span class="number">2.5274</span> <span class="number">0.0012</span> <span class="number">0.0249</span> <span class="number">0.0152</span> <span class="number">0.0157</span> <span class="number">0.0075</span> <span class="number">2.8705</span> <span class="number">0</span> <span class="number">1.5306</span> <span class="number">2.5493</span> <span class="number">0.1479</span> <span class="number">0.0059</span> <span class="number">2.8046</span> <span class="number">0.1185</span> <span class="number">0.0167</span> NaN NaN <span class="number">251.4536</span> <span class="number">329.6406</span> <span class="number">325.0672</span> <span class="number">902.4576</span> <span class="number">0.08</span> <span class="number">0.0583</span> <span class="number">0.0326</span> <span class="number">0.6964</span> <span class="number">0.4031</span> <span class="number">0.0416</span> <span class="number">0.1041</span> <span class="number">0.3846</span> <span class="number">0.0151</span> <span class="number">0.1288</span> <span class="number">0.2268</span> <span class="number">0.1288</span> <span class="number">0.3677</span> <span class="number">0.1175</span> <span class="number">0.1261</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">5.7247</span> <span class="number">0.2682</span> <span class="number">3.8541</span> <span class="number">6.1797</span> <span class="number">0.0546</span> <span class="number">2.568</span> <span class="number">0</span> <span class="number">4.6067</span> <span class="number">16.0104</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0243</span> <span class="number">1.5481</span> <span class="number">5.9453</span> <span class="number">0.2777</span> <span class="number">3.16</span> <span class="number">8.9855</span> <span class="number">1.5481</span> <span class="number">2.9844</span> <span class="number">6.2277</span> <span class="number">0.0353</span> <span class="number">3.7663</span> <span class="number">0</span> <span class="number">5.6983</span> <span class="number">24.7959</span> <span class="number">13.5664</span> <span class="number">15.4488</span> <span class="number">0</span> <span class="number">0.0105</span> <span class="number">0.0208</span> <span class="number">0.0327</span> <span class="number">0.0171</span> <span class="number">0.0116</span> <span class="number">0.0428</span> <span class="number">0.0154</span> <span class="number">0.0383</span> <span class="number">0.7786</span> <span class="number">0.0005</span> NaN <span class="number">0.0302</span> <span class="number">0.0046</span> <span class="number">58.0575</span> <span class="number">0.0092</span> <span class="number">283.6616</span> <span class="number">0</span> <span class="number">0.0054</span> <span class="number">0.0043</span> <span class="number">0.003</span> <span class="number">0.0037</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0021</span> <span class="number">0.0015</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN <span class="number">0.0221</span> <span class="number">0.01</span> <span class="number">0</span> <span class="number">28.7334</span> <span class="number">0.0003</span> <span class="number">1.2356</span> <span class="number">0.019</span> <span class="number">0.002</span> <span class="number">0.1375</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.019</span> <span class="number">11.4871</span> <span class="number">1.1798</span> <span class="number">4.0782</span> <span class="number">4.3102</span> <span class="number">3.7696</span> <span class="number">2.0627</span> <span class="number">18.0233</span> <span class="number">21.6062</span> <span class="number">0</span> <span class="number">8.7236</span> <span class="number">3.0609</span> <span class="number">5.2231</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2.2943</span> <span class="number">4.0917</span> <span class="number">0</span> <span class="number">50.6425</span> <span class="number">2.0261</span> <span class="number">5.2707</span> <span class="number">1.8268</span> <span class="number">4.2581</span> <span class="number">3.7479</span> <span class="number">3.522</span> <span class="number">10.3162</span> <span class="number">29.1663</span> <span class="number">18.7546</span> <span class="number">109.5747</span> <span class="number">14.2503</span> <span class="number">5.765</span> <span class="number">0.8972</span> <span class="number">3.1281</span> <span class="number">60</span> <span class="number">70.9161</span> <span class="number">8.8647</span> <span class="number">1.2771</span> <span class="number">0.4264</span> <span class="number">0.6263</span> <span class="number">0.8973</span> <span class="number">0.6301</span> <span class="number">1.4698</span> <span class="number">0.3194</span> <span class="number">0.2748</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">4.8795</span> <span class="number">7.5418</span> <span class="number">10.0984</span> <span class="number">3.1182</span> <span class="number">15.079</span> <span class="number">6.528</span> <span class="number">0</span> <span class="number">2.8042</span> <span class="number">32.3594</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">3.0301</span> <span class="number">21.3645</span> <span class="number">5.4178</span> <span class="number">9.3327</span> <span class="number">8.3977</span> <span class="number">148.0287</span> <span class="number">31.4674</span> <span class="number">45.5423</span> <span class="number">3.1842</span> <span class="number">13.3923</span> <span class="number">9.1221</span> <span class="number">0</span> <span class="number">2.6727</span> <span class="number">93.9245</span> <span class="number">0</span> <span class="number">434.2674</span> <span class="number">151.7665</span> <span class="number">0</span> <span class="number">190.3869</span> <span class="number">746.915</span> <span class="number">74.0741</span> <span class="number">191.7582</span> <span class="number">250.1742</span> <span class="number">34.1573</span> <span class="number">1.0281</span> NaN <span class="number">3.9238</span> <span class="number">1.5357</span> <span class="number">10.8251</span> <span class="number">18.9849</span> <span class="number">9.0113</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">240.7767</span> <span class="number">244.2748</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN <span class="number">36.9067</span> <span class="number">2.9626</span> <span class="number">0</span> <span class="number">14.5293</span> <span class="number">0.0751</span> <span class="number">7.087</span> <span class="number">12.1831</span> <span class="number">0.6451</span> <span class="number">6.4568</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2.1538</span> <span class="number">2.9667</span> <span class="number">9.3046</span> <span class="number">0.1096</span> <span class="number">0.0078</span> <span class="number">0.0026</span> <span class="number">7.116</span> <span class="number">0.7942</span> <span class="number">411.136</span> <span class="number">74.654</span> <span class="number">0.1832</span> <span class="number">16.16</span> <span class="number">0.85</span> <span class="number">0.0857</span> <span class="number">7.1619</span> <span class="number">0.3752</span> <span class="number">23.0713</span> <span class="number">3.9306</span> <span class="number">1.1386</span> <span class="number">1.5021</span> <span class="number">0.3718</span> <span class="number">0.1233</span> <span class="number">24.7524</span> <span class="number">267.064</span> <span class="number">0.9032</span> <span class="number">1.1</span> <span class="number">0.6219</span> <span class="number">0.4122</span> <span class="number">0.2562</span> <span class="number">0.4119</span> <span class="number">68.8489</span> <span class="number">535.0245</span> <span class="number">2.0293</span> <span class="number">11.21</span> <span class="number">0.1882</span> <span class="number">4.0923</span> <span class="number">0.064</span> <span class="number">2.0952</span> <span class="number">9.2721</span> <span class="number">0.0584</span> <span class="number">0.0484</span> <span class="number">0.0148</span> <span class="number">82.8602</span> <span class="number">0.4958</span> <span class="number">0.0157</span> <span class="number">0.0039</span> <span class="number">3.1745</span> <span class="number">0.0584</span> <span class="number">0.0484</span> <span class="number">0.0148</span> <span class="number">82.8602</span></span><br><span class="line"><span class="number">2988.72</span> <span class="number">2479.9</span> <span class="number">2199.0333</span> <span class="number">909.7926</span> <span class="number">1.3204</span> <span class="number">100</span> <span class="number">104.2367</span> <span class="number">0.1217</span> <span class="number">1.4882</span> <span class="number">-0.0124</span> <span class="number">-0.0033</span> <span class="number">0.9629</span> <span class="number">201.8482</span> <span class="number">0</span> <span class="number">9.6052</span> <span class="number">422.2894</span> <span class="number">9.6924</span> <span class="number">0.9687</span> <span class="number">192.1557</span> <span class="number">12.4782</span> <span class="number">1.4011</span> <span class="number">-5468.25</span> <span class="number">2648.25</span> <span class="number">-4515</span> <span class="number">-1657.25</span> <span class="number">1.3137</span> <span class="number">2.0038</span> <span class="number">7.3145</span> <span class="number">62.9333</span> <span class="number">2.6444</span> <span class="number">0.2071</span> <span class="number">3.3813</span> <span class="number">84.9105</span> <span class="number">8.6789</span> <span class="number">50.51</span> <span class="number">64.1125</span> <span class="number">49.49</span> <span class="number">65.1951</span> <span class="number">86.6867</span> <span class="number">117.0442</span> <span class="number">76.9</span> <span class="number">1.279</span> <span class="number">70</span> <span class="number">363.0273</span> <span class="number">9.9305</span> <span class="number">131.8027</span> <span class="number">733.8778</span> <span class="number">1.3027</span> <span class="number">142.5427</span> <span class="number">1</span> <span class="number">637.3727</span> <span class="number">189.9079</span> <span class="number">0</span> <span class="number">4.486</span> <span class="number">4.748</span> <span class="number">2936</span> <span class="number">0.9139</span> <span class="number">0.9447</span> <span class="number">4.5873</span> <span class="number">24.3791</span> <span class="number">361.4582</span> <span class="number">10.2112</span> <span class="number">116.1818</span> <span class="number">13.5597</span> <span class="number">15.6209</span> <span class="number">23.4736</span> <span class="number">710.4043</span> <span class="number">0.9761</span> <span class="number">147.6545</span> <span class="number">1</span> <span class="number">625.2945</span> <span class="number">70.2289</span> <span class="number">160.321</span> <span class="number">464.9735</span> <span class="number">0</span> <span class="number">-0.0555</span> <span class="number">-0.0461</span> <span class="number">-0.04</span> <span class="number">0.04</span> <span class="number">0.0676</span> <span class="number">-0.1051</span> <span class="number">0.0028</span> <span class="number">0.0277</span> <span class="number">7.5925</span> <span class="number">0.1302</span> NaN <span class="number">2.4004</span> <span class="number">0.9904</span> <span class="number">1752.0968</span> <span class="number">0.1958</span> <span class="number">8205.7</span> <span class="number">0.0697</span> <span class="number">-0.0003</span> <span class="number">-0.0021</span> <span class="number">-0.0001</span> <span class="number">0.0002</span> <span class="number">0.0411</span> <span class="number">0</span> <span class="number">0.0177</span> <span class="number">-0.0195</span> <span class="number">-0.0002</span> <span class="number">0</span> <span class="number">-0.0699</span> <span class="number">-0.0059</span> <span class="number">0.0003</span> <span class="number">0.0003</span> <span class="number">0.0021</span> <span class="number">-0.0483</span> <span class="number">-0.118</span> NaN NaN NaN <span class="number">0.4647</span> <span class="number">0.9564</span> <span class="number">0</span> <span class="number">709.0867</span> <span class="number">0.9906</span> <span class="number">58.6635</span> <span class="number">0.6016</span> <span class="number">0.9761</span> <span class="number">6.4935</span> <span class="number">15.55</span> <span class="number">3.132</span> <span class="number">15.61</span> <span class="number">15.59</span> <span class="number">1.366</span> <span class="number">2.48</span> <span class="number">0.5176</span> <span class="number">3.119</span> <span class="number">0.2838</span> <span class="number">0.7244</span> <span class="number">0.9961</span> <span class="number">2.3802</span> <span class="number">980.451</span> <span class="number">41.1025</span> <span class="number">127</span> <span class="number">118</span> <span class="number">123.7</span> <span class="number">47.8</span> <span class="number">162.432</span> <span class="number">0.1915</span> <span class="number">0</span> <span class="number">5.51</span> <span class="number">0.003</span> <span class="number">0.114</span> <span class="number">0.0393</span> <span class="number">0.0613</span> <span class="number">0.019</span> <span class="number">13.2651</span> <span class="number">0</span> <span class="number">9.073</span> <span class="number">15.241</span> <span class="number">1.3029</span> <span class="number">0.015</span> <span class="number">11.9738</span> <span class="number">0.35</span> <span class="number">0.0699</span> NaN NaN <span class="number">859</span> <span class="number">355</span> <span class="number">3433</span> <span class="number">3004</span> <span class="number">0.068</span> <span class="number">0.108</span> <span class="number">0.1</span> <span class="number">1.7</span> <span class="number">0.9</span> <span class="number">0.086</span> <span class="number">0.241</span> <span class="number">0.9386</span> <span class="number">0.0356</span> <span class="number">0.2618</span> <span class="number">0.4391</span> <span class="number">0.2618</span> <span class="number">0.8567</span> <span class="number">0.2452</span> <span class="number">0.39</span> <span class="number">0</span> <span class="number">0</span> <span class="number">16.22</span> <span class="number">0.693</span> <span class="number">14.67</span> <span class="number">22.562</span> <span class="number">0.1786</span> <span class="number">5.69</span> <span class="number">0</span> <span class="number">18.2</span> <span class="number">52.571</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.139</span> <span class="number">5.92</span> <span class="number">23.6</span> <span class="number">1.264</span> <span class="number">10.63</span> <span class="number">13.56</span> <span class="number">5.92</span> <span class="number">11.382</span> <span class="number">24.32</span> <span class="number">0.3458</span> <span class="number">9.56</span> <span class="number">0</span> <span class="number">21.97</span> <span class="number">104.95</span> <span class="number">0</span> <span class="number">0.1248</span> <span class="number">0.0463</span> <span class="number">0.1223</span> <span class="number">0.0354</span> <span class="number">0.0708</span> <span class="number">0.0754</span> <span class="number">0.0643</span> <span class="number">0.0932</span> <span class="number">5.5398</span> <span class="number">0.0023</span> NaN <span class="number">0.0764</span> <span class="number">0.0015</span> <span class="number">152.0885</span> <span class="number">0.0573</span> <span class="number">820.3999</span> <span class="number">0</span> <span class="number">0.0152</span> <span class="number">0.0149</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0067</span> <span class="number">0.004</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN <span class="number">0.0191</span> <span class="number">0.0234</span> <span class="number">0</span> <span class="number">94.0954</span> <span class="number">0.001</span> <span class="number">3.2119</span> <span class="number">0.0406</span> <span class="number">0.0072</span> <span class="number">0.4212</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0513</span> <span class="number">31.83</span> <span class="number">3.1959</span> <span class="number">33.896</span> <span class="number">37.8477</span> <span class="number">44.3906</span> <span class="number">16.9347</span> <span class="number">50.3631</span> <span class="number">0.0581</span> <span class="number">0</span> <span class="number">2.1775</span> <span class="number">0.0007</span> <span class="number">0.0417</span> <span class="number">0.0115</span> <span class="number">0.0172</span> <span class="number">0.0063</span> <span class="number">4.2154</span> <span class="number">0</span> <span class="number">2.896</span> <span class="number">4.0526</span> <span class="number">0.3882</span> <span class="number">0.0049</span> <span class="number">3.9403</span> <span class="number">0.0916</span> <span class="number">0.0245</span> NaN NaN <span class="number">415.5048</span> <span class="number">157.0889</span> <span class="number">1572.6896</span> <span class="number">1377.4276</span> <span class="number">0.0285</span> <span class="number">0.0445</span> <span class="number">0.0465</span> <span class="number">0.6305</span> <span class="number">0.3046</span> <span class="number">0.0286</span> <span class="number">0.0824</span> <span class="number">0.3483</span> <span class="number">0.0128</span> <span class="number">0.1004</span> <span class="number">0.1701</span> <span class="number">0.1004</span> <span class="number">0.3465</span> <span class="number">0.0973</span> <span class="number">0.1675</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">5.444</span> <span class="number">0.2004</span> <span class="number">4.19</span> <span class="number">6.3329</span> <span class="number">0.0479</span> <span class="number">1.7339</span> <span class="number">0</span> <span class="number">4.966</span> <span class="number">15.7375</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0243</span> <span class="number">1.7317</span> <span class="number">6.6262</span> <span class="number">0.3512</span> <span class="number">3.2699</span> <span class="number">9.402</span> <span class="number">1.7317</span> <span class="number">3.0672</span> <span class="number">6.6839</span> <span class="number">0.0928</span> <span class="number">3.0229</span> <span class="number">0</span> <span class="number">6.3292</span> <span class="number">29.0339</span> <span class="number">8.4026</span> <span class="number">4.8851</span> <span class="number">0</span> <span class="number">0.0407</span> <span class="number">0.0198</span> <span class="number">0.0531</span> <span class="number">0.0167</span> <span class="number">0.0224</span> <span class="number">0.0422</span> <span class="number">0.0273</span> <span class="number">0.0484</span> <span class="number">1.8222</span> <span class="number">0.0006</span> NaN <span class="number">0.0252</span> <span class="number">0.0004</span> <span class="number">45.7058</span> <span class="number">0.0188</span> <span class="number">309.8492</span> <span class="number">0</span> <span class="number">0.0046</span> <span class="number">0.0049</span> <span class="number">0.0028</span> <span class="number">0.0034</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0024</span> <span class="number">0.0014</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN <span class="number">0.0038</span> <span class="number">0.0068</span> <span class="number">0</span> <span class="number">32.4228</span> <span class="number">0.0003</span> <span class="number">1.1135</span> <span class="number">0.0132</span> <span class="number">0.0023</span> <span class="number">0.1348</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0155</span> <span class="number">13.3972</span> <span class="number">1.1907</span> <span class="number">5.6363</span> <span class="number">3.9482</span> <span class="number">4.9881</span> <span class="number">2.1737</span> <span class="number">17.8537</span> <span class="number">14.5054</span> <span class="number">0</span> <span class="number">5.286</span> <span class="number">2.4643</span> <span class="number">7.6602</span> <span class="number">317.7362</span> <span class="number">0</span> <span class="number">1.9689</span> <span class="number">6.5718</span> <span class="number">0</span> <span class="number">94.4594</span> <span class="number">3.6091</span> <span class="number">13.442</span> <span class="number">1.5441</span> <span class="number">6.2313</span> <span class="number">2.8049</span> <span class="number">4.9898</span> <span class="number">15.7089</span> <span class="number">13.4051</span> <span class="number">76.0354</span> <span class="number">181.2641</span> <span class="number">5.176</span> <span class="number">5.3899</span> <span class="number">1.3671</span> <span class="number">2.7013</span> <span class="number">34.0336</span> <span class="number">41.5236</span> <span class="number">7.1274</span> <span class="number">1.1054</span> <span class="number">0.4097</span> <span class="number">0.5183</span> <span class="number">0.6849</span> <span class="number">0.529</span> <span class="number">1.3141</span> <span class="number">0.2829</span> <span class="number">0.3332</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">4.468</span> <span class="number">6.9785</span> <span class="number">11.1303</span> <span class="number">3.0744</span> <span class="number">13.7105</span> <span class="number">3.9918</span> <span class="number">0</span> <span class="number">2.8555</span> <span class="number">27.6824</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">3.0301</span> <span class="number">24.2831</span> <span class="number">6.5291</span> <span class="number">12.3786</span> <span class="number">9.1494</span> <span class="number">100.0021</span> <span class="number">37.8979</span> <span class="number">48.4887</span> <span class="number">3.4234</span> <span class="number">35.4323</span> <span class="number">6.4746</span> <span class="number">0</span> <span class="number">3.5135</span> <span class="number">149.4399</span> <span class="number">0</span> <span class="number">225.0169</span> <span class="number">100.4883</span> <span class="number">305.75</span> <span class="number">88.5553</span> <span class="number">104.666</span> <span class="number">71.7583</span> <span class="number">0</span> <span class="number">336.766</span> <span class="number">72.9635</span> <span class="number">1.767</span> NaN <span class="number">3.1817</span> <span class="number">0.1488</span> <span class="number">8.6804</span> <span class="number">29.2542</span> <span class="number">9.9979</span> <span class="number">0</span> <span class="number">0</span> <span class="number">711.6418</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">113.5593</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN <span class="number">4.12</span> <span class="number">2.4416</span> <span class="number">0</span> <span class="number">13.2699</span> <span class="number">0.0977</span> <span class="number">5.4751</span> <span class="number">6.7553</span> <span class="number">0.7404</span> <span class="number">6.4865</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2.1565</span> <span class="number">3.2465</span> <span class="number">7.7754</span> <span class="number">0.1096</span> <span class="number">0.0078</span> <span class="number">0.0026</span> <span class="number">7.116</span> <span class="number">1.165</span> <span class="number">372.822</span> <span class="number">72.442</span> <span class="number">1.8804</span> <span class="number">131.68</span> <span class="number">39.33</span> <span class="number">0.6812</span> <span class="number">56.9303</span> <span class="number">17.4781</span> <span class="number">161.4081</span> <span class="number">35.3198</span> <span class="number">54.2917</span> <span class="number">1.1613</span> <span class="number">0.7288</span> <span class="number">0.271</span> <span class="number">62.7572</span> <span class="number">268.228</span> <span class="number">0.6511</span> <span class="number">7.32</span> <span class="number">0.163</span> <span class="number">3.5611</span> <span class="number">0.067</span> <span class="number">2.729</span> <span class="number">25.0363</span> <span class="number">530.5682</span> <span class="number">2.0253</span> <span class="number">9.33</span> <span class="number">0.1738</span> <span class="number">2.8971</span> <span class="number">0.0525</span> <span class="number">1.7585</span> <span class="number">8.5831</span> <span class="number">0.0202</span> <span class="number">0.0149</span> <span class="number">0.0044</span> <span class="number">73.8432</span> <span class="number">0.499</span> <span class="number">0.0103</span> <span class="number">0.0025</span> <span class="number">2.0544</span> <span class="number">0.0202</span> <span class="number">0.0149</span> <span class="number">0.0044</span> <span class="number">73.8432</span></span><br><span class="line"><span class="number">3032.24</span> <span class="number">2502.87</span> <span class="number">2233.3667</span> <span class="number">1326.52</span> <span class="number">1.5334</span> <span class="number">100</span> <span class="number">100.3967</span> <span class="number">0.1235</span> <span class="number">1.5031</span> <span class="number">-0.0031</span> <span class="number">-0.0072</span> <span class="number">0.9569</span> <span class="number">201.9424</span> <span class="number">0</span> <span class="number">10.5661</span> <span class="number">420.5925</span> <span class="number">10.3387</span> <span class="number">0.9735</span> <span class="number">191.6037</span> <span class="number">12.4735</span> <span class="number">1.3888</span> <span class="number">-5476.25</span> <span class="number">2635.25</span> <span class="number">-3987.5</span> <span class="number">117</span> <span class="number">1.2887</span> <span class="number">1.9912</span> <span class="number">7.2748</span> <span class="number">62.8333</span> <span class="number">3.1556</span> <span class="number">0.2696</span> <span class="number">3.2728</span> <span class="number">86.3269</span> <span class="number">8.7677</span> <span class="number">50.248</span> <span class="number">64.1511</span> <span class="number">49.752</span> <span class="number">66.1542</span> <span class="number">86.1468</span> <span class="number">121.4364</span> <span class="number">76.39</span> <span class="number">2.209</span> <span class="number">70</span> <span class="number">353.34</span> <span class="number">10.4091</span> <span class="number">176.3136</span> <span class="number">789.7523</span> <span class="number">1.0341</span> <span class="number">138.0882</span> <span class="number">1</span> <span class="number">667.7418</span> <span class="number">233.5491</span> <span class="number">0</span> <span class="number">4.624</span> <span class="number">4.894</span> <span class="number">2865</span> <span class="number">0.9298</span> <span class="number">0.9449</span> <span class="number">4.6414</span> <span class="number">-12.2945</span> <span class="number">355.0809</span> <span class="number">9.7948</span> <span class="number">144.0191</span> <span class="number">21.9782</span> <span class="number">32.2945</span> <span class="number">44.1498</span> <span class="number">745.6025</span> <span class="number">0.9256</span> <span class="number">146.6636</span> <span class="number">1</span> <span class="number">645.7636</span> <span class="number">65.8417</span> NaN NaN <span class="number">0</span> <span class="number">-0.0534</span> <span class="number">0.0183</span> <span class="number">-0.0167</span> <span class="number">-0.0449</span> <span class="number">0.0034</span> <span class="number">-0.0178</span> <span class="number">-0.0123</span> <span class="number">-0.0048</span> <span class="number">7.5017</span> <span class="number">0.1342</span> NaN <span class="number">2.453</span> <span class="number">0.9902</span> <span class="number">1828.3846</span> <span class="number">0.1829</span> <span class="number">9014.46</span> <span class="number">0.0448</span> <span class="number">-0.0077</span> <span class="number">-0.0001</span> <span class="number">-0.0001</span> <span class="number">-0.0001</span> <span class="number">0.2189</span> <span class="number">0</span> <span class="number">-0.6704</span> <span class="number">-0.0167</span> <span class="number">0.0004</span> <span class="number">-0.0003</span> <span class="number">0.0696</span> <span class="number">-0.0045</span> <span class="number">0.0002</span> <span class="number">0.0078</span> <span class="number">0</span> <span class="number">-0.0799</span> <span class="number">-0.2038</span> NaN NaN NaN NaN <span class="number">0.9424</span> <span class="number">0</span> <span class="number">796.595</span> <span class="number">0.9908</span> <span class="number">58.3858</span> <span class="number">0.5913</span> <span class="number">0.9628</span> <span class="number">6.3551</span> <span class="number">15.75</span> <span class="number">3.148</span> <span class="number">15.73</span> <span class="number">15.71</span> <span class="number">0.946</span> <span class="number">3.027</span> <span class="number">0.5328</span> <span class="number">3.299</span> <span class="number">-0.5677</span> <span class="number">0.778</span> <span class="number">1.001</span> <span class="number">2.3715</span> <span class="number">993.1274</span> <span class="number">38.1448</span> <span class="number">119</span> <span class="number">143.2</span> <span class="number">123.1</span> <span class="number">48.8</span> <span class="number">296.303</span> <span class="number">0.3744</span> <span class="number">0</span> <span class="number">3.64</span> <span class="number">0.0041</span> <span class="number">0.0634</span> <span class="number">0.0451</span> <span class="number">0.0623</span> <span class="number">0.024</span> <span class="number">14.2354</span> <span class="number">0</span> <span class="number">9.005</span> <span class="number">12.506</span> <span class="number">0.4434</span> <span class="number">0.0126</span> <span class="number">13.9047</span> <span class="number">0.43</span> <span class="number">0.0538</span> NaN NaN <span class="number">699</span> <span class="number">283</span> <span class="number">1747</span> <span class="number">1443</span> <span class="number">0.147</span> <span class="number">0.04</span> <span class="number">0.113</span> <span class="number">3.9</span> <span class="number">0.8</span> <span class="number">0.101</span> <span class="number">0.499</span> <span class="number">0.576</span> <span class="number">0.0631</span> <span class="number">0.3053</span> <span class="number">0.583</span> <span class="number">0.3053</span> <span class="number">0.8285</span> <span class="number">0.1308</span> <span class="number">0.922</span> <span class="number">0</span> <span class="number">0</span> <span class="number">15.24</span> <span class="number">0.282</span> <span class="number">10.85</span> <span class="number">37.715</span> <span class="number">0.1189</span> <span class="number">3.98</span> <span class="number">0</span> <span class="number">25.54</span> <span class="number">72.149</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.25</span> <span class="number">5.52</span> <span class="number">15.76</span> <span class="number">0.519</span> <span class="number">10.71</span> <span class="number">19.77</span> <span class="number">5.52</span> <span class="number">8.446</span> <span class="number">33.832</span> <span class="number">0.3951</span> <span class="number">9.09</span> <span class="number">0</span> <span class="number">19.77</span> <span class="number">92.307</span> <span class="number">0</span> <span class="number">0.0915</span> <span class="number">0.0506</span> <span class="number">0.0769</span> <span class="number">0.1079</span> <span class="number">0.0797</span> <span class="number">0.1047</span> <span class="number">0.0924</span> <span class="number">0.1015</span> <span class="number">4.1338</span> <span class="number">0.003</span> NaN <span class="number">0.0802</span> <span class="number">0.0004</span> <span class="number">69.151</span> <span class="number">0.197</span> <span class="number">1406.4004</span> <span class="number">0</span> <span class="number">0.0227</span> <span class="number">0.0272</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0067</span> <span class="number">0.0031</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN NaN <span class="number">0.024</span> <span class="number">0</span> <span class="number">149.2172</span> <span class="number">0.0006</span> <span class="number">2.5775</span> <span class="number">0.0177</span> <span class="number">0.0214</span> <span class="number">0.4051</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0488</span> <span class="number">19.862</span> <span class="number">3.6163</span> <span class="number">34.125</span> <span class="number">55.9626</span> <span class="number">53.0876</span> <span class="number">17.4864</span> <span class="number">88.7672</span> <span class="number">0.1092</span> <span class="number">0</span> <span class="number">1.0929</span> <span class="number">0.0013</span> <span class="number">0.0257</span> <span class="number">0.0116</span> <span class="number">0.0163</span> <span class="number">0.008</span> <span class="number">4.4239</span> <span class="number">0</span> <span class="number">3.2376</span> <span class="number">3.6536</span> <span class="number">0.1293</span> <span class="number">0.004</span> <span class="number">4.3474</span> <span class="number">0.1275</span> <span class="number">0.0181</span> NaN NaN <span class="number">319.1252</span> <span class="number">128.0296</span> <span class="number">799.5884</span> <span class="number">628.3083</span> <span class="number">0.0755</span> <span class="number">0.0181</span> <span class="number">0.0476</span> <span class="number">1.35</span> <span class="number">0.2698</span> <span class="number">0.032</span> <span class="number">0.1541</span> <span class="number">0.2155</span> <span class="number">0.031</span> <span class="number">0.1354</span> <span class="number">0.2194</span> <span class="number">0.1354</span> <span class="number">0.3072</span> <span class="number">0.0582</span> <span class="number">0.3574</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">4.8956</span> <span class="number">0.0766</span> <span class="number">2.913</span> <span class="number">11.0583</span> <span class="number">0.0327</span> <span class="number">1.1229</span> <span class="number">0</span> <span class="number">7.3296</span> <span class="number">23.116</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0822</span> <span class="number">1.6216</span> <span class="number">4.7279</span> <span class="number">0.1773</span> <span class="number">3.155</span> <span class="number">9.7777</span> <span class="number">1.6216</span> <span class="number">2.5923</span> <span class="number">10.5352</span> <span class="number">0.1301</span> <span class="number">3.0939</span> <span class="number">0</span> <span class="number">6.3767</span> <span class="number">32.0537</span> NaN NaN <span class="number">0</span> <span class="number">0.0246</span> <span class="number">0.0221</span> <span class="number">0.0329</span> <span class="number">0.0522</span> <span class="number">0.0256</span> <span class="number">0.0545</span> <span class="number">0.0476</span> <span class="number">0.0463</span> <span class="number">1.553</span> <span class="number">0.001</span> NaN <span class="number">0.0286</span> <span class="number">0.0001</span> <span class="number">21.0312</span> <span class="number">0.0573</span> <span class="number">494.7368</span> <span class="number">0</span> <span class="number">0.0063</span> <span class="number">0.0077</span> <span class="number">0.0052</span> <span class="number">0.0027</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0025</span> <span class="number">0.0012</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN NaN <span class="number">0.0089</span> <span class="number">0</span> <span class="number">57.2692</span> <span class="number">0.0002</span> <span class="number">0.8495</span> <span class="number">0.0065</span> <span class="number">0.0077</span> <span class="number">0.1356</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0.0165</span> <span class="number">7.1493</span> <span class="number">1.1704</span> <span class="number">5.3823</span> <span class="number">4.7226</span> <span class="number">4.9184</span> <span class="number">2.185</span> <span class="number">22.3369</span> <span class="number">24.4142</span> <span class="number">0</span> <span class="number">3.6256</span> <span class="number">3.3208</span> <span class="number">4.2178</span> <span class="number">0</span> <span class="number">866.0295</span> <span class="number">2.5046</span> <span class="number">7.0492</span> <span class="number">0</span> <span class="number">85.2255</span> <span class="number">2.9734</span> <span class="number">4.2892</span> <span class="number">1.2943</span> <span class="number">7.257</span> <span class="number">3.4473</span> <span class="number">3.8754</span> <span class="number">12.7642</span> <span class="number">10.739</span> <span class="number">43.8119</span> <span class="number">0</span> <span class="number">11.4064</span> <span class="number">2.0088</span> <span class="number">1.5533</span> <span class="number">6.2069</span> <span class="number">25.3521</span> <span class="number">37.4691</span> <span class="number">15.247</span> <span class="number">0.6672</span> <span class="number">0.7198</span> <span class="number">0.6076</span> <span class="number">0.9088</span> <span class="number">0.6136</span> <span class="number">1.2524</span> <span class="number">0.1518</span> <span class="number">0.7592</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">4.3131</span> <span class="number">2.7092</span> <span class="number">6.1538</span> <span class="number">4.7756</span> <span class="number">11.4945</span> <span class="number">2.8822</span> <span class="number">0</span> <span class="number">3.8248</span> <span class="number">30.8924</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">5.3863</span> <span class="number">44.898</span> <span class="number">4.4384</span> <span class="number">5.2987</span> <span class="number">7.4365</span> <span class="number">89.9529</span> <span class="number">17.0927</span> <span class="number">19.1303</span> <span class="number">4.5375</span> <span class="number">42.6838</span> <span class="number">6.1979</span> <span class="number">0</span> <span class="number">3.0615</span> <span class="number">140.1953</span> <span class="number">0</span> <span class="number">171.4486</span> <span class="number">276.881</span> <span class="number">461.8619</span> <span class="number">240.1781</span> <span class="number">0</span> <span class="number">587.3773</span> <span class="number">748.1781</span> <span class="number">0</span> <span class="number">55.1057</span> <span class="number">2.2358</span> NaN <span class="number">3.2712</span> <span class="number">0.0372</span> <span class="number">3.7821</span> <span class="number">107.6905</span> <span class="number">15.6016</span> <span class="number">0</span> <span class="number">293.1396</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">148.0663</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> NaN NaN NaN NaN <span class="number">2.5512</span> <span class="number">0</span> <span class="number">18.7319</span> <span class="number">0.0616</span> <span class="number">4.4146</span> <span class="number">2.9954</span> <span class="number">2.2181</span> <span class="number">6.3745</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2.0579</span> <span class="number">1.9999</span> <span class="number">9.4805</span> <span class="number">0.1096</span> <span class="number">0.0078</span> <span class="number">0.0026</span> <span class="number">7.116</span> <span class="number">1.4636</span> <span class="number">399.914</span> <span class="number">79.156</span> <span class="number">1.0388</span> <span class="number">19.63</span> <span class="number">1.98</span> <span class="number">0.4287</span> <span class="number">9.7608</span> <span class="number">0.8311</span> <span class="number">70.9706</span> <span class="number">4.9086</span> <span class="number">2.5014</span> <span class="number">0.9778</span> <span class="number">0.2156</span> <span class="number">0.0461</span> <span class="number">22.05</span> NaN NaN NaN NaN NaN NaN NaN NaN <span class="number">532.0155</span> <span class="number">2.0275</span> <span class="number">8.83</span> <span class="number">0.2224</span> <span class="number">3.1776</span> <span class="number">0.0706</span> <span class="number">1.6597</span> <span class="number">10.9698</span> NaN NaN NaN NaN <span class="number">0.48</span> <span class="number">0.4766</span> <span class="number">0.1045</span> <span class="number">99.3032</span> <span class="number">0.0202</span> <span class="number">0.0149</span> <span class="number">0.0044</span> <span class="number">73.8432</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>准备数据：将value为NaN的替换为均值</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replaceNanWithMean</span><span class="params">()</span>:</span></span><br><span class="line">    datMat = loadDataSet(<span class="string">'data/13.PCA/secom.data'</span>, <span class="string">' '</span>)</span><br><span class="line">    numFeat = shape(datMat)[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeat):</span><br><span class="line">        <span class="comment"># 对value不为NaN的求均值</span></span><br><span class="line">        <span class="comment"># .A 返回矩阵基于的数组</span></span><br><span class="line">        meanVal = mean(datMat[nonzero(~isnan(datMat[:, i].A))[<span class="number">0</span>], i])</span><br><span class="line">        <span class="comment"># 将value为NaN的值赋值为均值</span></span><br><span class="line">        datMat[nonzero(isnan(datMat[:, i].A))[<span class="number">0</span>],i] = meanVal</span><br><span class="line">    <span class="keyword">return</span> datMat</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据：统计分析 N 的阈值</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/13.PCA/PCA%E5%88%86%E6%9E%90%E6%95%B0%E6%8D%AE%E8%BF%87%E7%A8%8B.jpg" alt="PCA分析数据过程"></p>
<blockquote>
<p>PCA 数据降维</p>
</blockquote>
<p>在等式 Av=入v 中，v 是特征向量， 入是特征值。<br/><br>表示 如果特征向量 v 被某个矩阵 A 左乘，那么它就等于某个标量 入 乘以 v.<br/><br>幸运的是： Numpy 中有寻找特征向量和特征值的模块 linalg，它有 eig() 方法，该方法用于求解特征向量和特征值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span><span class="params">(dataMat, topNfeat=<span class="number">9999999</span>)</span>:</span></span><br><span class="line">    <span class="string">"""pca</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMat   原数据集矩阵</span></span><br><span class="line"><span class="string">        topNfeat  应用的N个特征</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        lowDDataMat  降维后数据集</span></span><br><span class="line"><span class="string">        reconMat     新的数据集空间</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算每一列的均值</span></span><br><span class="line">    meanVals = mean(dataMat, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># print 'meanVals', meanVals</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个向量同时都减去 均值</span></span><br><span class="line">    meanRemoved = dataMat - meanVals</span><br><span class="line">    <span class="comment"># print 'meanRemoved=', meanRemoved</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># cov协方差=[(x1-x均值)*(y1-y均值)+(x2-x均值)*(y2-y均值)+...+(xn-x均值)*(yn-y均值)+]/(n-1)</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    方差：（一维）度量两个随机变量关系的统计量</span></span><br><span class="line"><span class="string">    协方差： （二维）度量各个维度偏离其均值的程度</span></span><br><span class="line"><span class="string">    协方差矩阵：（多维）度量各个维度偏离其均值的程度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    当 cov(X, Y)&gt;0时，表明X与Y正相关；(X越大，Y也越大；X越小Y，也越小。这种情况，我们称为“正相关”。)</span></span><br><span class="line"><span class="string">    当 cov(X, Y)&lt;0时，表明X与Y负相关；</span></span><br><span class="line"><span class="string">    当 cov(X, Y)=0时，表明X与Y不相关。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    covMat = cov(meanRemoved, rowvar=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># eigVals为特征值， eigVects为特征向量</span></span><br><span class="line">    eigVals, eigVects = linalg.eig(mat(covMat))</span><br><span class="line">    <span class="comment"># print 'eigVals=', eigVals</span></span><br><span class="line">    <span class="comment"># print 'eigVects=', eigVects</span></span><br><span class="line">    <span class="comment"># 对特征值，进行从小到大的排序，返回从小到大的index序号</span></span><br><span class="line">    <span class="comment"># 特征值的逆序就可以得到topNfeat个最大的特征向量</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; x = np.array([3, 1, 2])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; np.argsort(x)</span></span><br><span class="line"><span class="string">    array([1, 2, 0])  # index,1 = 1; index,2 = 2; index,0 = 3</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; y = np.argsort(x)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; y[::-1]</span></span><br><span class="line"><span class="string">    array([0, 2, 1])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; y[:-3:-1]</span></span><br><span class="line"><span class="string">    array([0, 2])  # 取出 -1, -2</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; y[:-6:-1]</span></span><br><span class="line"><span class="string">    array([0, 2, 1])</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    eigValInd = argsort(eigVals)</span><br><span class="line">    <span class="comment"># print 'eigValInd1=', eigValInd</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># -1表示倒序，返回topN的特征值[-1 到 -(topNfeat+1) 但是不包括-(topNfeat+1)本身的倒叙]</span></span><br><span class="line">    eigValInd = eigValInd[:-(topNfeat+<span class="number">1</span>):<span class="number">-1</span>]</span><br><span class="line">    <span class="comment"># print 'eigValInd2=', eigValInd</span></span><br><span class="line">    <span class="comment"># 重组 eigVects 最大到最小</span></span><br><span class="line">    redEigVects = eigVects[:, eigValInd]</span><br><span class="line">    <span class="comment"># print 'redEigVects=', redEigVects.T</span></span><br><span class="line">    <span class="comment"># 将数据转换到新空间</span></span><br><span class="line">    <span class="comment"># --- (1567, 590) (590, 20)</span></span><br><span class="line">    <span class="comment"># print "---", shape(meanRemoved), shape(redEigVects)</span></span><br><span class="line">    lowDDataMat = meanRemoved * redEigVects</span><br><span class="line">    reconMat = (lowDDataMat * redEigVects.T) + meanVals</span><br><span class="line">    <span class="comment"># print 'lowDDataMat=', lowDDataMat</span></span><br><span class="line">    <span class="comment"># print 'reconMat=', reconMat</span></span><br><span class="line">    <span class="keyword">return</span> lowDDataMat, reconMat</span><br></pre></td></tr></table></figure>

<p><a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/13.PCA/pca.py" target="_blank" rel="noopener">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/13.PCA/pca.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/13.PCA/pca.py</a></p>
<h3 id="要点补充"><a href="#要点补充" class="headerlink" title="要点补充"></a>要点补充</h3><figure class="highlight stata"><table><tr><td class="code"><pre><span class="line">降维技术使得数据变的更易使用，并且它们往往能够去除数据中的噪音，使得其他机器学习任务更加精确。</span><br><span class="line">降维往往作为预处理步骤，在数据应用到其他算法之前清洗数据。</span><br><span class="line">比较流行的降维技术： 独立成分分析、因子分析 和 主成分分析， 其中又以主成分分析应用最广泛。</span><br><span class="line"></span><br><span class="line">本章中的<span class="keyword">PCA</span>将所有的数据集都调入了内存，如果无法做到，就需要其他的方法来寻找其特征值。</span><br><span class="line">如果使用在线<span class="keyword">PCA</span>分析的方法，你可以参考一篇优秀的论文 <span class="string">"Incremental Eigenanalysis for Classification"</span>。 </span><br><span class="line">下一章要讨论的奇异值分解方法也可以用于特征值分析。</span><br></pre></td></tr></table></figure>

<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a>      </li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第12章 使用FP-growth算法来高效发现频繁项集</title>
    <url>/2020/06/03/12.%E4%BD%BF%E7%94%A8FP-growth%E7%AE%97%E6%B3%95%E6%9D%A5%E9%AB%98%E6%95%88%E5%8F%91%E7%8E%B0%E9%A2%91%E7%B9%81%E9%A1%B9%E9%9B%86/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在 <a href="">第11章</a> 时我们已经介绍了用 <code>Apriori</code> 算法发现 <code>频繁项集</code> 与 <code>关联规则</code>。<br>本章将继续关注发现 <code>频繁项集</code> 这一任务，并使用 <code>FP-growth</code> 算法更有效的挖掘 <code>频繁项集</code>。</p>
<a id="more"></a>
<h2 id="FP-growth-算法简介"><a href="#FP-growth-算法简介" class="headerlink" title="FP-growth 算法简介"></a>FP-growth 算法简介</h2><ul>
<li>一种非常好的发现频繁项集算法。</li>
<li>基于Apriori算法构建,但是数据结构不同，使用叫做 <code>FP树</code> 的数据结构结构来存储集合。下面我们会介绍这种数据结构。</li>
</ul>
<h2 id="FP-growth-算法步骤"><a href="#FP-growth-算法步骤" class="headerlink" title="FP-growth 算法步骤"></a>FP-growth 算法步骤</h2><ul>
<li>基于数据构建FP树   </li>
<li>从FP树种挖掘频繁项集 </li>
</ul>
<h2 id="FP树-介绍"><a href="#FP树-介绍" class="headerlink" title="FP树 介绍"></a>FP树 介绍</h2><ul>
<li>FP树的节点结构如下:</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">treeNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nameValue, numOccur, parentNode)</span>:</span></span><br><span class="line">        self.name = nameValue     <span class="comment"># 节点名称</span></span><br><span class="line">        self.count = numOccur     <span class="comment"># 节点出现次数</span></span><br><span class="line">        self.nodeLink = <span class="literal">None</span>      <span class="comment"># 不同项集的相同项通过nodeLink连接在一起</span></span><br><span class="line">        <span class="comment"># needs to be updated</span></span><br><span class="line">        self.parent = parentNode  <span class="comment"># 指向父节点</span></span><br><span class="line">        self.children = &#123;&#125;        <span class="comment"># 存储叶子节点</span></span><br><span class="line">```   </span><br><span class="line">         </span><br><span class="line"><span class="comment">## FP-growth 原理</span></span><br><span class="line">基于数据构建FP树</span><br><span class="line"></span><br><span class="line">步骤<span class="number">1</span>:</span><br><span class="line">   <span class="number">1.</span> 遍历所有的数据集合，计算所有项的支持度。</span><br><span class="line">   <span class="number">2.</span> 丢弃非频繁的项。</span><br><span class="line">   <span class="number">3.</span> 基于 支持度 降序排序所有的项。</span><br><span class="line">   ![](http://data.apachecn.org/img/AiLearning/ml/<span class="number">12.</span>FP-growth/步骤<span class="number">1</span><span class="number">-3.</span>png)</span><br><span class="line">   <span class="number">4.</span> 所有数据集合按照得到的顺序重新整理。</span><br><span class="line">   <span class="number">5.</span> 重新整理完成后，丢弃每个集合末尾非频繁的项。</span><br><span class="line">   ![](http://data.apachecn.org/img/AiLearning/ml/<span class="number">12.</span>FP-growth/步骤<span class="number">4</span><span class="number">-5.</span>png)</span><br><span class="line"></span><br><span class="line">步骤<span class="number">2</span>:</span><br><span class="line">   <span class="number">6.</span> 读取每个集合插入FP树中，同时用一个头部链表数据结构维护不同集合的相同项。  </span><br><span class="line">   ![](http://data.apachecn.org/img/AiLearning/ml/<span class="number">12.</span>FP-growth/步骤<span class="number">6</span><span class="number">-1.</span>png)</span><br><span class="line">   最终得到下面这样一棵FP树</span><br><span class="line">   ![](http://data.apachecn.org/img/AiLearning/ml/<span class="number">12.</span>FP-growth/步骤<span class="number">6</span><span class="number">-2.</span>png)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">从FP树中挖掘出频繁项集</span><br><span class="line"></span><br><span class="line">步骤<span class="number">3</span>:</span><br><span class="line">   <span class="number">1.</span> 对头部链表进行降序排序</span><br><span class="line">   <span class="number">2.</span> 对头部链表节点从小到大遍历，得到条件模式基，同时获得一个频繁项集。</span><br><span class="line">        ![](http://data.apachecn.org/img/AiLearning/ml/<span class="number">12.</span>FP-growth/步骤<span class="number">6</span><span class="number">-2.</span>png)</span><br><span class="line">        如上图，从头部链表 t 节点开始遍历，t 节点加入到频繁项集。找到以 t 节点为结尾的路径如下:</span><br><span class="line">        ![](http://data.apachecn.org/img/AiLearning/ml/<span class="number">12.</span>FP-growth/步骤<span class="number">7</span><span class="number">-1.</span>png)</span><br><span class="line">        去掉FP树中的t节点，得到条件模式基&lt;左边路径, 右边是值&gt;[z,x,y,s,t]:<span class="number">2</span>，[z,x,y,r,t]:<span class="number">1</span> 。条件模式基的值取决于末尾节点 t ，因为 t 的出现次数最小，一个频繁项集的支持度由支持度最小的项决定。所以 t 节点的条件模式基的值可以理解为对于以 t 节点为末尾的前缀路径出现次数。</span><br><span class="line">       </span><br><span class="line">   <span class="number">3.</span> 条件模式基继续构造条件 FP树， 得到频繁项集，和之前的频繁项组合起来，这是一个递归遍历头部链表生成FP树的过程，递归截止条件是生成的FP树的头部链表为空。</span><br><span class="line">        根据步骤 <span class="number">2</span> 得到的条件模式基 [z,x,y,s,t]:<span class="number">2</span>，[z,x,y,r,t]:<span class="number">1</span> 作为数据集继续构造出一棵FP树，计算支持度，去除非频繁项，集合按照支持度降序排序，重复上面构造FP树的步骤。最后得到下面 t-条件FP树 :</span><br><span class="line">        ![](http://data.apachecn.org/img/AiLearning/ml/<span class="number">12.</span>FP-growth/步骤<span class="number">7</span><span class="number">-2.</span>png)</span><br><span class="line">        然后根据 t-条件FP树 的头部链表进行遍历，从 y 开始。得到频繁项集 ty 。然后又得到 y 的条件模式基，构造出 ty的条件FP树，即 ty-条件FP树。继续遍历ty-条件FP树的头部链表，得到频繁项集 tyx，然后又得到频繁项集 tyxz. 然后得到构造tyxz-条件FP树的头部链表是空的，终止遍历。我们得到的频繁项集有 t-&gt;ty-&gt;tyz-&gt;tyzx，这只是一小部分。</span><br><span class="line">   * 条件模式基:头部链表中的某一点的前缀路径组合就是条件模式基，条件模式基的值取决于末尾节点的值。</span><br><span class="line">   * 条件FP树:以条件模式基为数据集构造的FP树叫做条件FP树。</span><br><span class="line"></span><br><span class="line">FP-growth 算法优缺点:</span><br></pre></td></tr></table></figure>
<ul>
<li>优点： 1. 因为 FP-growth 算法只需要对数据集遍历两次，所以速度更快。<pre><code>2. FP树将集合按照支持度降序排序，不同路径如果有相同前缀路径共用存储空间，使得数据得到了压缩。
3. 不需要生成候选集。
4. 比Apriori更快。</code></pre></li>
<li>缺点： 1. FP-Tree第二次遍历会存储很多中间过程的值，会占用很多内存。<pre><code>2. 构建FP-Tree是比较昂贵的。</code></pre></li>
<li>适用数据类型：标称型数据(离散型数据)。<pre><code>
</code></pre></li>
</ul>
<h2 id="FP-growth-代码讲解"><a href="#FP-growth-代码讲解" class="headerlink" title="FP-growth 代码讲解"></a>FP-growth 代码讲解</h2><p>完整代码地址: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py</a></p>
<p>main 方法大致步骤:</p>
<pre><code class="python"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:
    simpDat = loadSimpDat()                       <span class="comment">#加载数据集。</span>
    initSet = createInitSet(simpDat)              <span class="comment">#对数据集进行整理，相同集合进行合并。</span>
    myFPtree, myHeaderTab = createTree(initSet, <span class="number">3</span>)<span class="comment">#创建FP树。</span>
    freqItemList = []
    mineTree(myFPtree, myHeaderTab, <span class="number">3</span>, set([]), freqItemList) <span class="comment">#递归的从FP树中挖掘出频繁项集。</span>
    <span class="keyword">print</span> freqItemList</code></pre>
<p>大家看懂原理，再仔细跟踪一下代码。基本就没有问题了。</p>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a>      </li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第16章 推荐系统</title>
    <url>/2020/06/03/16.%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h2 id="背景与挖掘目标"><a href="#背景与挖掘目标" class="headerlink" title="背景与挖掘目标"></a>背景与挖掘目标</h2><p>随着互联网的快速发展，用户很难快速从海量信息中寻找到自己感兴趣的信息。因此诞生了：搜索引擎+推荐系统</p>
<p>本章节-推荐系统：</p>
<ol>
<li>帮助用户发现其感兴趣和可能感兴趣的信息。</li>
<li>让网站价值信息脱颖而出，得到广大用户的认可。</li>
<li>提高用户对网站的忠诚度和关注度，建立稳固用户群体。<a id="more"></a>
<h2 id="分析方法与过程"><a href="#分析方法与过程" class="headerlink" title="分析方法与过程"></a>分析方法与过程</h2></li>
</ol>
<p>本案例的目标是对用户进行推荐，即以一定的方式将用户与物品（本次指网页）之间建立联系。</p>
<p>由于用户访问网站的数据记录很多，如果不对数据进行分类处理，对所有的记录直接采用推荐系统进行推荐，这样会存在一下问题。</p>
<ol>
<li>数据量太大意味着物品数与用户数很多，在模型构建用户与物品稀疏矩阵时，出现设备内存空间不够的情况，并且模型计算需要消耗大量的时间。</li>
<li>用户区别很大，不同的用户关注的信息不一样，因此，即使能够得到推荐结果，其效果也会不好。</li>
</ol>
<p>为了避免出现上述问题，需要进行分类处理与分析。</p>
<p>正常的情况下，需要对用户的兴趣爱好以及需求进行分类。<br>因为在用户访问记录中，没有记录用户访问页面时间的长短，因此不容易判断用户兴趣爱好。<br>因此，本文根据用户浏览的网页信息进行分析处理，主要采用以下方法处理：以用户浏览网页的类型进行分类，然后对每个类型中的内容进行推荐。</p>
<p>分析过程如下：</p>
<ul>
<li>从系统中获取用户访问网站的原始记录。</li>
<li>对数据进行多维分析，包括用户访问内容，流失用户分析以及用户分类等分析。</li>
<li>对数据进行预处理，包含数据去重、数据变换和数据分类鞥处理过程。</li>
<li>以用户访问html后缀的页面为关键条件，对数据进行处理。</li>
<li>对比多种推荐算法进行推荐，通过模型评价，得到比较好的智能推荐模型。通过模型对样本数据进行预测，获得推荐结果。</li>
</ul>
<h2 id="主流推荐算法"><a href="#主流推荐算法" class="headerlink" title="主流推荐算法"></a>主流推荐算法</h2><table>
<thead>
<tr>
<th>推荐方法</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>基于内容推荐</td>
<td></td>
</tr>
<tr>
<td>协同过滤推荐</td>
<td></td>
</tr>
<tr>
<td>基于规则推荐</td>
<td></td>
</tr>
<tr>
<td>基于效用推荐</td>
<td></td>
</tr>
<tr>
<td>基于知识推荐</td>
<td></td>
</tr>
<tr>
<td>组合推荐</td>
<td></td>
</tr>
</tbody></table>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/16.RecommendedSystem/%E6%8E%A8%E8%8D%90%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94.png" alt="推荐方法对比"></p>
<h3 id="基于知识推荐"><a href="#基于知识推荐" class="headerlink" title="基于知识推荐"></a>基于知识推荐</h3><p>基于知识的推荐（Knowledge-based Recommendation）在某种程度是可以看成是一种推理（Inference）技术，它不是建立在用户需要和偏好基础上推荐的。基于知识的方法因它们所用的功能知识不同而有明显区别。效用知识（Functional Knowledge）是一种关于一个项目如何满足某一特定用户的知识，因此能解释需要和推荐的关系，所以用户资料可以是任何能支持推理的知识结构，它可以是用户已经规范化的查询，也可以是一个更详细的用户需要的表示。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/16.RecommendedSystem/%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E7%9A%84%E6%8E%A8%E8%8D%90.jpg" alt="基于知识的推荐"></p>
<h3 id="协同过滤推荐"><a href="#协同过滤推荐" class="headerlink" title="协同过滤推荐"></a>协同过滤推荐</h3><ul>
<li>memory-based推荐<ul>
<li>Item-based方法</li>
<li>User-based方法</li>
<li>Memory-based推荐方法通过执行最近邻搜索，把每一个Item或者User看成一个向量，计算其他所有Item或者User与它的相似度。有了Item或者User之间的两两相似度之后，就可以进行预测与推荐了。 </li>
</ul>
</li>
<li>model-based推荐<ul>
<li>Model-based推荐最常见的方法为Matrix factorization.</li>
<li>矩阵分解通过把原始的评分矩阵R分解为两个矩阵相乘，并且只考虑有评分的值，训练时不考虑missing项的值。R矩阵分解成为U与V两个矩阵后，评分矩阵R中missing的值就可以通过U矩阵中的某列和V矩阵的某行相乘得到</li>
<li>矩阵分解的目标函数: U矩阵与V矩阵的可以通过梯度下降(gradient descent)算法求得，通过交替更新u与v多次迭代收敛之后可求出U与V。 </li>
<li>矩阵分解背后的核心思想，找到两个矩阵，它们相乘之后得到的那个矩阵的值，与评分矩阵R中有值的位置中的值尽可能接近。这样一来，分解出来的两个矩阵相乘就尽可能还原了评分矩阵R，因为有值的地方，值都相差得尽可能地小，那么missing的值通过这样的方式计算得到，比较符合趋势。 </li>
</ul>
</li>
<li>协同过滤中主要存在如下两个问题：稀疏性与冷启动问题。已有的方案通常会通过引入多个不同的数据源或者辅助信息(Side information)来解决这些问题，用户的Side information可以是用户的基本个人信息、用户画像信息等，而Item的Side information可以是物品的content信息等。</li>
</ul>
<h2 id="效果评估"><a href="#效果评估" class="headerlink" title="效果评估"></a>效果评估</h2><ol>
<li>召回率和准确率 【人为统计分析】</li>
<li>F值(P-R曲线) 【偏重：非均衡问题】</li>
<li>ROC和AUC  【偏重：不同结果的对比】</li>
</ol>
<blockquote>
<p>摘录的原文地址：</p>
</blockquote>
<ul>
<li><a href="http://www.36dsj.com/archives/9519" target="_blank" rel="noopener">推荐系统中常用算法 以及优点缺点对比</a></li>
<li><a href="https://zhidao.baidu.com/question/2013524494179442228.html" target="_blank" rel="noopener">推荐算法的基于知识推荐</a></li>
<li><a href="http://www.iteye.com/news/32100" target="_blank" rel="noopener">推荐系统中基于深度学习的混合协同过滤模型</a></li>
</ul>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a> </li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第15章 大数据与MapReduce</title>
    <url>/2020/06/03/15.%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8EMapReduce/</url>
    <content><![CDATA[<h2 id="大数据-概述"><a href="#大数据-概述" class="headerlink" title="大数据 概述"></a>大数据 概述</h2><p><code>大数据: 收集到的数据已经远远超出了我们的处理能力。</code></p>
<h2 id="大数据-场景"><a href="#大数据-场景" class="headerlink" title="大数据 场景"></a>大数据 场景</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">假如你为一家网络购物商店工作，很多用户访问该网站，其中有些人会购买商品，有些人则随意浏览后就离开。</span><br><span class="line">对于你来说，可能很想识别那些有购物意愿的用户。</span><br><span class="line">那么问题就来了，数据集可能会非常大，在单机上训练要运行好几天。</span><br><span class="line">接下来：我们讲讲 MapRedece 如何来解决这样的问题</span><br></pre></td></tr></table></figure>

<a id="more"></a>
<h2 id="MapRedece"><a href="#MapRedece" class="headerlink" title="MapRedece"></a>MapRedece</h2><h3 id="Hadoop-概述"><a href="#Hadoop-概述" class="headerlink" title="Hadoop 概述"></a>Hadoop 概述</h3><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">Hadoop</span> <span class="string">是 MapRedece 框架的一个免费开源实现。</span></span><br><span class="line"><span class="attr">MapReduce</span>: <span class="string">分布式的计算框架，可以将单个计算作业分配给多台计算机执行。</span></span><br></pre></td></tr></table></figure>

<h3 id="MapRedece-原理"><a href="#MapRedece-原理" class="headerlink" title="MapRedece 原理"></a>MapRedece 原理</h3><blockquote>
<p>MapRedece 工作原理</p>
</blockquote>
<ul>
<li>主节点控制 MapReduce 的作业流程</li>
<li>MapReduce 的作业可以分成map任务和reduce任务</li>
<li>map 任务之间不做数据交流，reduce 任务也一样</li>
<li>在 map 和 reduce 阶段中间，有一个 sort 和 combine 阶段</li>
<li>数据被重复存放在不同的机器上，以防止某个机器失效</li>
<li>mapper 和 reducer 传输的数据形式为 key/value对</li>
</ul>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/15.BigData_MapReduce/mr_1_cluster.jpg" alt="MapReduce框架的示意图" title="MapReduce框架的示意图"></p>
<blockquote>
<p>MapRedece 特点</p>
</blockquote>
<figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">优点: 使程序以并行的方式执行，可在短时间内完成大量工作。</span></span><br><span class="line"><span class="section">缺点: 算法必须经过重写，需要对系统工程有一定的理解。</span></span><br><span class="line"><span class="section">适用数据类型: 数值型和标称型数据。</span></span><br></pre></td></tr></table></figure>

<h3 id="Hadoop-流-Python-调用"><a href="#Hadoop-流-Python-调用" class="headerlink" title="Hadoop 流(Python 调用)"></a>Hadoop 流(Python 调用)</h3><blockquote>
<p>理论简介</p>
</blockquote>
<p>例如: Hadoop流可以像Linux命令一样执行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat inputFile.txt | python mapper.py | sort | python reducer.py &gt; outputFile.txt</span><br></pre></td></tr></table></figure>

<p>类似的Hadoop流就可以在多台机器上分布式执行，用户可以通过Linux命令来测试Python语言编写的MapReduce脚本。</p>
<blockquote>
<p>实战脚本</p>
</blockquote>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line"># 测试 Mapper</span><br><span class="line"># Linux</span><br><span class="line"><span class="keyword">cat</span> data/<span class="number">15</span>.BigData_MapReduce/inputFile.txt | <span class="keyword">python</span> src/<span class="keyword">python</span>/<span class="number">15</span>.BigData_MapReduce/mrMeanMapper.<span class="keyword">py</span></span><br><span class="line"># Window</span><br><span class="line"># <span class="keyword">python</span> src/<span class="keyword">python</span>/<span class="number">15</span>.BigData_MapReduce/mrMeanMapper.<span class="keyword">py</span> &lt; data/<span class="number">15</span>.BigData_MapReduce/inputFile.txt</span><br><span class="line"></span><br><span class="line"># 测试 Reducer</span><br><span class="line"># Linux</span><br><span class="line"><span class="keyword">cat</span> data/<span class="number">15</span>.BigData_MapReduce/inputFile.txt | <span class="keyword">python</span> src/<span class="keyword">python</span>/<span class="number">15</span>.BigData_MapReduce/mrMeanMapper.<span class="keyword">py</span> | <span class="keyword">python</span> src/<span class="keyword">python</span>/<span class="number">15</span>.BigData_MapReduce/mrMeanReducer.<span class="keyword">py</span></span><br><span class="line"># Window</span><br><span class="line"># <span class="keyword">python</span> src/<span class="keyword">python</span>/<span class="number">15</span>.BigData_MapReduce/mrMeanMapper.<span class="keyword">py</span> &lt; data/<span class="number">15</span>.BigData_MapReduce/inputFile.txt | <span class="keyword">python</span> src/<span class="keyword">python</span>/<span class="number">15</span>.BigData_MapReduce/mrMeanReducer.<span class="keyword">py</span></span><br></pre></td></tr></table></figure>

<h3 id="MapReduce-机器学习"><a href="#MapReduce-机器学习" class="headerlink" title="MapReduce 机器学习"></a>MapReduce 机器学习</h3><h4 id="Mahout-in-Action"><a href="#Mahout-in-Action" class="headerlink" title="Mahout in Action"></a>Mahout in Action</h4><ol>
<li>简单贝叶斯：它属于为数不多的可以很自然的使用MapReduce的算法。通过统计在某个类别下某特征的概率。</li>
<li>k-近邻算法：高维数据下（如文本、图像和视频）流行的近邻查找方法是局部敏感哈希算法。</li>
<li>支持向量机(SVM)：使用随机梯度下降算法求解，如Pegasos算法。</li>
<li>奇异值分解：Lanczos算法是一个有效的求解近似特征值的算法。</li>
<li>k-均值聚类：canopy算法初始化k个簇，然后再运行K-均值求解结果。</li>
</ol>
<h3 id="使用-mrjob-库将-MapReduce-自动化"><a href="#使用-mrjob-库将-MapReduce-自动化" class="headerlink" title="使用 mrjob 库将 MapReduce 自动化"></a>使用 mrjob 库将 MapReduce 自动化</h3><blockquote>
<p>理论简介</p>
</blockquote>
<ul>
<li>MapReduce 作业流自动化的框架：Cascading 和 Oozie.</li>
<li>mrjob 是一个不错的学习工具，与2010年底实现了开源，来之于 Yelp(一个餐厅点评网站).</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python src/python/15.BigData_MapReduce/mrMean.py &lt; data/15.BigData_MapReduce/inputFile.txt &gt; data/15.BigData_MapReduce/myOut.txt</span><br></pre></td></tr></table></figure>

<blockquote>
<p>实战脚本</p>
</blockquote>
<figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta"># 测试 mrjob的案例</span></span><br><span class="line"><span class="meta"># 先测试一下mapper方法</span></span><br><span class="line"><span class="meta"># python src/python/15.BigData_MapReduce/mrMean.py --mapper &lt; data/15.BigData_MapReduce/inputFile.txt</span></span><br><span class="line"><span class="meta"># 运行整个程序，移除 --mapper 就行</span></span><br><span class="line">python src/python/<span class="number">15.</span>BigData_MapReduce/mrMean.py &lt; data/<span class="number">15.</span>BigData_MapReduce/inputFile.txt</span><br></pre></td></tr></table></figure>

<h3 id="项目案例：分布式-SVM-的-Pegasos-算法"><a href="#项目案例：分布式-SVM-的-Pegasos-算法" class="headerlink" title="项目案例：分布式 SVM 的 Pegasos 算法"></a>项目案例：分布式 SVM 的 Pegasos 算法</h3><p>Pegasos是指原始估计梯度求解器(Peimal Estimated sub-GrAdient Solver)</p>
<h4 id="Pegasos-工作原理"><a href="#Pegasos-工作原理" class="headerlink" title="Pegasos 工作原理"></a>Pegasos 工作原理</h4><ol>
<li>从训练集中随机挑选一些样本点添加到待处理列表中</li>
<li>按序判断每个样本点是否被正确分类<ul>
<li>如果是则忽略</li>
<li>如果不是则将其加入到待更新集合。</li>
</ul>
</li>
<li>批处理完毕后，权重向量按照这些错分的样本进行更新。</li>
</ol>
<p>上述算法伪代码如下：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">将 回归系数w 初始化为<span class="number">0</span></span><br><span class="line">对每次批处理</span><br><span class="line">    随机选择 k 个样本点(向量)</span><br><span class="line">    对每个向量</span><br><span class="line">        如果该向量被错分：</span><br><span class="line">            更新权重向量 w</span><br><span class="line">    累加对 w 的更新</span><br></pre></td></tr></table></figure>

<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">收集数据：数据按文本格式存放。</span><br><span class="line">准备数据：输入数据已经是可用的格式，所以不需任何准备工作。如果你需要解析一个大规模的数据集，建议使用 <span class="built_in">map</span> 作业来完成，从而达到并行处理的目的。</span><br><span class="line">分析数据：无。</span><br><span class="line">训练算法：与普通的 SVM 一样，在分类器训练上仍需花费大量的时间。</span><br><span class="line">测试算法：在二维空间上可视化之后，观察超平面，判断算法是否有效。</span><br><span class="line">使用算法：本例不会展示一个完整的应用，但会展示如何在大数据集上训练SVM。该算法其中一个应用场景就是本文分类，通常在文本分类里可能有大量的文档和成千上万的特征。</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据</p>
</blockquote>
<p>文本文件数据格式如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">0.365032</span>        <span class="number">2.465645</span>        <span class="number">-1</span></span><br><span class="line"><span class="number">-2.494175</span>       <span class="number">-0.292380</span>       <span class="number">-1</span></span><br><span class="line"><span class="number">-3.039364</span>       <span class="number">-0.123108</span>       <span class="number">-1</span></span><br><span class="line"><span class="number">1.348150</span>        <span class="number">0.255696</span>        <span class="number">1</span></span><br><span class="line"><span class="number">2.768494</span>        <span class="number">1.234954</span>        <span class="number">1</span></span><br><span class="line"><span class="number">1.232328</span>        <span class="number">-0.601198</span>       <span class="number">1</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>准备数据</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="comment"># dataMat.append([float(lineArr[0]), float(lineArr[1]), float(lineArr[2])])</span></span><br><span class="line">        dataMat.append([float(lineArr[<span class="number">0</span>]), float(lineArr[<span class="number">1</span>])])</span><br><span class="line">        labelMat.append(float(lineArr[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据: 无</p>
</blockquote>
<blockquote>
<p>训练算法</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchPegasos</span><span class="params">(dataSet, labels, lam, T, k)</span>:</span></span><br><span class="line">    <span class="string">"""batchPegasos()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMat    特征集合</span></span><br><span class="line"><span class="string">        labels     分类结果集合</span></span><br><span class="line"><span class="string">        lam        固定值</span></span><br><span class="line"><span class="string">        T          迭代次数</span></span><br><span class="line"><span class="string">        k          待处理列表大小</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        w          回归系数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m, n = shape(dataSet)</span><br><span class="line">    w = zeros(n)  <span class="comment"># 回归系数</span></span><br><span class="line">    dataIndex = range(m)</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, T+<span class="number">1</span>):</span><br><span class="line">        wDelta = mat(zeros(n))  <span class="comment"># 重置 wDelta</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 它是学习率，代表了权重调整幅度的大小。（也可以理解为随机梯度的步长，使它不断减小，便于拟合）</span></span><br><span class="line">        <span class="comment"># 输入T和K分别设定了迭代次数和待处理列表的大小。在T次迭代过程中，每次需要重新计算eta</span></span><br><span class="line">        eta = <span class="number">1.0</span>/(lam*t)</span><br><span class="line">        random.shuffle(dataIndex)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(k):      <span class="comment"># 全部的训练集  内循环中执行批处理，将分类错误的值全部做累加后更新权重向量</span></span><br><span class="line">            i = dataIndex[j]</span><br><span class="line">            p = predict(w, dataSet[i, :])              <span class="comment"># mapper 代码</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果预测正确，并且预测结果的绝对值&gt;=1，因为最大间隔为1, 认为没问题。</span></span><br><span class="line">            <span class="comment"># 否则算是预测错误, 通过预测错误的结果，来累计更新w.</span></span><br><span class="line">            <span class="keyword">if</span> labels[i]*p &lt; <span class="number">1</span>:                        <span class="comment"># mapper 代码</span></span><br><span class="line">                wDelta += labels[i]*dataSet[i, :].A    <span class="comment"># 累积变化</span></span><br><span class="line">        <span class="comment"># w通过不断的随机梯度的方式来优化</span></span><br><span class="line">        w = (<span class="number">1.0</span> - <span class="number">1</span>/t)*w + (eta/k)*wDelta             <span class="comment"># 在每个 T上应用更改</span></span><br><span class="line">        <span class="comment"># print '-----', w</span></span><br><span class="line">    <span class="comment"># print '++++++', w</span></span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure>

<p><a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/15.BigData_MapReduce/pegasos.py" target="_blank" rel="noopener">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/15.BigData_MapReduce/pegasos.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/15.BigData_MapReduce/pegasos.py</a></p>
<p>运行方式：<code>python /opt/git/MachineLearning/src/python/15.BigData_MapReduce/mrSVM.py &lt; data/15.BigData_MapReduce/inputFile.txt</code><br><a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py" target="_blank" rel="noopener">MR版本的代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py</a></p>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a>      </li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第14章 利用SVD简化数据</title>
    <url>/2020/06/03/14.%E5%88%A9%E7%94%A8SVD%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h2 id="SVD-概述"><a href="#SVD-概述" class="headerlink" title="SVD 概述"></a>SVD 概述</h2><figure class="highlight gauss"><table><tr><td class="code"><pre><span class="line">奇异值分解（<span class="built_in">SVD</span>, Singular Value Decomposition）:</span><br><span class="line">    提取信息的一种方法，可以把 <span class="built_in">SVD</span> 看成是从噪声数据中抽取相关特征。从生物信息学到金融学，<span class="built_in">SVD</span> 是提取信息的强大工具。</span><br></pre></td></tr></table></figure>

<h2 id="SVD-场景"><a href="#SVD-场景" class="headerlink" title="SVD 场景"></a>SVD 场景</h2><blockquote>
<p>信息检索-隐性语义检索（Latent Semantic Indexing, LSI）或 隐形语义分析（Latent Semantic Analysis, LSA）</p>
</blockquote>
<p>隐性语义索引：矩阵 = 文档 + 词语</p>
<ul>
<li>是最早的 SVD 应用之一，我们称利用 SVD 的方法为隐性语义索引（LSI）或隐性语义分析（LSA）。<a id="more"></a>
<img src="http://data.apachecn.org/img/AiLearning/ml/14.SVD/%E4%BD%BF%E7%94%A8SVD%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE-LSI%E4%B8%BE%E4%BE%8B.png" alt="LSA举例"></li>
</ul>
<blockquote>
<p>推荐系统</p>
</blockquote>
<ol>
<li>利用 SVD 从数据中构建一个主题空间。</li>
<li>再在该空间下计算其相似度。(从高维-低维空间的转化，在低维空间来计算相似度，SVD 提升了推荐系统的效率。)</li>
</ol>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/14.SVD/SVD_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E4%B8%BB%E9%A2%98%E7%A9%BA%E9%97%B4%E6%A1%88%E4%BE%8B1.jpg" alt="主题空间案例1"></p>
<ul>
<li>上图右边标注的为一组共同特征，表示美式 BBQ 空间；另一组在上图右边未标注的为日式食品 空间。</li>
</ul>
<blockquote>
<p>图像压缩</p>
</blockquote>
<p>例如：<code>32*32=1024 =&gt; 32*2+2*1+32*2=130</code>(2*1表示去掉了除对角线的0), 几乎获得了10倍的压缩比。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/14.SVD/%E4%BD%BF%E7%94%A8SVD%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE-SVD%E5%85%AC%E5%BC%8F.jpg" alt="SVD公式"></p>
<h2 id="SVD-原理"><a href="#SVD-原理" class="headerlink" title="SVD 原理"></a>SVD 原理</h2><h3 id="SVD-工作原理"><a href="#SVD-工作原理" class="headerlink" title="SVD 工作原理"></a>SVD 工作原理</h3><blockquote>
<p>矩阵分解</p>
</blockquote>
<ul>
<li>矩阵分解是将数据矩阵分解为多个独立部分的过程。</li>
<li>矩阵分解可以将原始矩阵表示成新的易于处理的形式，这种新形式是两个或多个矩阵的乘积。（类似代数中的因数分解）</li>
<li>举例：如何将12分解成两个数的乘积？（1，12）、（2，6）、（3，4）都是合理的答案。</li>
</ul>
<blockquote>
<p>SVD 是矩阵分解的一种类型，也是矩阵分解最常见的技术</p>
</blockquote>
<ul>
<li>SVD 将原始的数据集矩阵 Data 分解成三个矩阵 U、∑、V</li>
<li>举例：如果原始矩阵 \(Data_{m*n}\) 是m行n列，<ul>
<li>\(U_{m * k}\) 表示m行k列</li>
<li>\(∑_{k * k}\) 表示k行k列</li>
<li>\(V_{k * n}\) 表示k行n列。</li>
</ul>
</li>
</ul>
<p>\(Data_{m<em>n} = U_{m*k} \</em> ∑<em>{k*k} * V</em>{k*n}\)</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/14.SVD/%E4%BD%BF%E7%94%A8SVD%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE-SVD%E5%85%AC%E5%BC%8F.jpg" alt="SVD公式"></p>
<p>具体的案例：（大家可以试着推导一下：<a href="https://wenku.baidu.com/view/b7641217866fb84ae45c8d17.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/b7641217866fb84ae45c8d17.html</a> ）</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/14.SVD/SVD%E5%85%AC%E5%BC%8F%E7%9A%84%E6%B5%8B%E8%AF%95%E6%A1%88%E4%BE%8B.jpg" alt="SVD公式"></p>
<ul>
<li>上述分解中会构建出一个矩阵∑，该矩阵只有对角元素，其他元素均为0(近似于0)。另一个惯例就是，∑的对角元素是从大到小排列的。这些对角元素称为奇异值。</li>
<li>奇异值与特征值(PCA 数据中重要特征)是有关系的。这里的奇异值就是矩阵 \(Data * Data^T\) 特征值的平方根。</li>
<li>普遍的事实：在某个奇异值的数目(r 个=&gt;奇异值的平方和累加到总值的90%以上)之后，其他的奇异值都置为0(近似于0)。这意味着数据集中仅有 r 个重要特征，而其余特征则都是噪声或冗余特征。</li>
</ul>
<h3 id="SVD-算法特点"><a href="#SVD-算法特点" class="headerlink" title="SVD 算法特点"></a>SVD 算法特点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">优点：简化数据，去除噪声，优化算法的结果</span><br><span class="line">缺点：数据的转换可能难以理解</span><br><span class="line">使用的数据类型：数值型数据</span><br></pre></td></tr></table></figure>

<h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><h3 id="推荐系统-概述"><a href="#推荐系统-概述" class="headerlink" title="推荐系统 概述"></a>推荐系统 概述</h3><p><code>推荐系统是利用电子商务网站向客户提供商品信息和建议，帮助用户决定应该购买什么产品，模拟销售人员帮助客户完成购买过程。</code></p>
<h3 id="推荐系统-场景"><a href="#推荐系统-场景" class="headerlink" title="推荐系统 场景"></a>推荐系统 场景</h3><ol>
<li>Amazon 会根据顾客的购买历史向他们推荐物品</li>
<li>Netflix 会向其用户推荐电影</li>
<li>新闻网站会对用户推荐新闻频道</li>
</ol>
<h3 id="推荐系统-要点"><a href="#推荐系统-要点" class="headerlink" title="推荐系统 要点"></a>推荐系统 要点</h3><blockquote>
<p>基于协同过滤(collaborative filtering) 的推荐引擎</p>
</blockquote>
<ul>
<li>利用Python 实现 SVD(Numpy 有一个称为 linalg 的线性代数工具箱)</li>
<li>协同过滤：是通过将用户和其他用户的数据进行对比来实现推荐的。</li>
<li>当知道了两个用户或两个物品之间的相似度，我们就可以利用已有的数据来预测未知用户的喜好。</li>
</ul>
<blockquote>
<p>基于物品的相似度和基于用户的相似度：物品比较少则选择物品相似度，用户比较少则选择用户相似度。【矩阵还是小一点好计算】</p>
</blockquote>
<ul>
<li>基于物品的相似度：计算物品之间的距离。【耗时会随物品数量的增加而增加】</li>
<li>由于物品A和物品C 相似度(相关度)很高，所以给买A的人推荐C。</li>
</ul>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/14.SVD/%E4%BD%BF%E7%94%A8SVD%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE-%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9B%B8%E4%BC%BC%E5%BA%A6.png" alt="SVD公式"></p>
<ul>
<li>基于用户的相似度：计算用户之间的距离。【耗时会随用户数量的增加而增加】</li>
<li>由于用户A和用户C 相似度(相关度)很高，所以A和C是兴趣相投的人，对于C买的物品就会推荐给A。</li>
</ul>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/14.SVD/%E4%BD%BF%E7%94%A8SVD%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE-%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E7%9B%B8%E4%BC%BC%E5%BA%A6.png" alt="SVD公式"></p>
<blockquote>
<p>相似度计算</p>
</blockquote>
<ul>
<li>inA, inB 对应的是 列向量</li>
</ul>
<ol>
<li>欧氏距离：指在m维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离）。二维或三维中的欧氏距离就是两点之间的实际距离。<ul>
<li>相似度= 1/(1+欧式距离)</li>
<li><code>相似度= 1.0/(1.0 + la.norm(inA - inB))</code></li>
<li>物品对越相似，它们的相似度值就越大。</li>
</ul>
</li>
<li>皮尔逊相关系数：度量的是两个向量之间的相似度。<ul>
<li>相似度= 0.5 + 0.5<em>corrcoef() 【皮尔逊相关系数的取值范围从 -1 到 +1，通过函数0.5 + 0.5\</em>corrcoef()这个函数计算，把值归一化到0到1之间】</li>
<li><code>相似度= 0.5 + 0.5 * corrcoef(inA, inB, rowvar = 0)[0][1]</code></li>
<li>相对欧氏距离的优势：它对用户评级的量级并不敏感。</li>
</ul>
</li>
<li>余弦相似度：计算的是两个向量夹角的余弦值。<ul>
<li>余弦值 = (A·B)/(||A||·||B||) 【余弦值的取值范围也在-1到+1之间】</li>
<li>相似度= 0.5 + 0.5*余弦值</li>
<li><code>相似度= 0.5 + 0.5*( float(inA.T*inB) / la.norm(inA)*la.norm(inB))</code></li>
<li>如果夹角为90度，则相似度为0；如果两个向量的方向相同，则相似度为1.0。</li>
</ul>
</li>
</ol>
<blockquote>
<p>推荐系统的评价</p>
</blockquote>
<ul>
<li>采用交叉测试的方法。【拆分数据为训练集和测试集】</li>
<li>推荐引擎评价的指标： 最小均方根误差(Root mean squared error, RMSE)，也称标准误差(Standard error)，就是计算均方误差的平均值然后取其平方根。<ul>
<li>如果RMSE=1, 表示相差1个星级；如果RMSE=2.5, 表示相差2.5个星级。</li>
</ul>
</li>
</ul>
<h3 id="推荐系统-原理"><a href="#推荐系统-原理" class="headerlink" title="推荐系统 原理"></a>推荐系统 原理</h3><ul>
<li>推荐系统的工作过程：给定一个用户，系统会为此用户返回N个最好的推荐菜。</li>
<li>实现流程大致如下：<ol>
<li>寻找用户没有评级的菜肴，即在用户-物品矩阵中的0值。</li>
<li>在用户没有评级的所有物品中，对每个物品预计一个可能的评级分数。这就是说：我们认为用户可能会对物品的打分（这就是相似度计算的初衷）。</li>
<li>对这些物品的评分从高到低进行排序，返回前N个物品。</li>
</ol>
</li>
</ul>
<h3 id="项目案例-餐馆菜肴推荐系统"><a href="#项目案例-餐馆菜肴推荐系统" class="headerlink" title="项目案例: 餐馆菜肴推荐系统"></a>项目案例: 餐馆菜肴推荐系统</h3><h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p><code>假如一个人在家决定外出吃饭，但是他并不知道该到哪儿去吃饭，该点什么菜。推荐系统可以帮他做到这两点。</code></p>
<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><blockquote>
<p>收集 并 准备数据</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/14.SVD/%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5.jpg" alt="SVD 矩阵"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadExData3</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 利用SVD提高推荐效果，菜肴矩阵</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    行：代表人</span></span><br><span class="line"><span class="string">    列：代表菜肴名词</span></span><br><span class="line"><span class="string">    值：代表人对菜肴的评分，0表示未评分</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span>[[<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">4</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据: 这里不做过多的讨论(当然此处可以对比不同距离之间的差别)</p>
</blockquote>
<blockquote>
<p>训练算法: 通过调用 recommend() 函数进行推荐</p>
</blockquote>
<p>recommend() 会调用 基于物品相似度 或者是 基于SVD，得到推荐的物品评分。</p>
<ul>
<li>1.基于物品相似度</li>
</ul>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/14.SVD/%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9B%B8%E4%BC%BC%E5%BA%A6.jpg" alt="基于物品相似度"></p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/14.SVD/%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F.jpg" alt="欧式距离的计算方式"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 基于物品相似度的推荐引擎</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standEst</span><span class="params">(dataMat, user, simMeas, item)</span>:</span></span><br><span class="line">    <span class="string">"""standEst(计算某用户未评分物品中，以对该物品和其他物品评分的用户的物品相似度，然后进行综合评分)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMat         训练数据集</span></span><br><span class="line"><span class="string">        user            用户编号</span></span><br><span class="line"><span class="string">        simMeas         相似度计算方法</span></span><br><span class="line"><span class="string">        item            未评分的物品编号</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        ratSimTotal/simTotal     评分（0～5之间的值）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 得到数据集中的物品数目</span></span><br><span class="line">    n = shape(dataMat)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 初始化两个评分值</span></span><br><span class="line">    simTotal = <span class="number">0.0</span></span><br><span class="line">    ratSimTotal = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 遍历行中的每个物品（对用户评过分的物品进行遍历，并将它与其他物品进行比较）</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        userRating = dataMat[user, j]</span><br><span class="line">        <span class="comment"># 如果某个物品的评分值为0，则跳过这个物品</span></span><br><span class="line">        <span class="keyword">if</span> userRating == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># 寻找两个用户都评级的物品</span></span><br><span class="line">        <span class="comment"># 变量 overLap 给出的是两个物品当中已经被评分的那个元素的索引ID</span></span><br><span class="line">        <span class="comment"># logical_and 计算x1和x2元素的真值。</span></span><br><span class="line">        overLap = nonzero(logical_and(dataMat[:, item].A &gt; <span class="number">0</span>, dataMat[:, j].A &gt; <span class="number">0</span>))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 如果相似度为0，则两着没有任何重合元素，终止本次循环</span></span><br><span class="line">        <span class="keyword">if</span> len(overLap) == <span class="number">0</span>:</span><br><span class="line">            similarity = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 如果存在重合的物品，则基于这些重合物重新计算相似度。</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            similarity = simMeas(dataMat[overLap, item], dataMat[overLap, j])</span><br><span class="line">        <span class="comment"># print 'the %d and %d similarity is : %f'(iten,j,similarity)</span></span><br><span class="line">        <span class="comment"># 相似度会不断累加，每次计算时还考虑相似度和当前用户评分的乘积</span></span><br><span class="line">        <span class="comment"># similarity  用户相似度，   userRating 用户评分</span></span><br><span class="line">        simTotal += similarity</span><br><span class="line">        ratSimTotal += similarity * userRating</span><br><span class="line">    <span class="keyword">if</span> simTotal == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 通过除以所有的评分总和，对上述相似度评分的乘积进行归一化，使得最后评分在0~5之间，这些评分用来对预测值进行排序</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> ratSimTotal/simTotal</span><br></pre></td></tr></table></figure>

<ul>
<li>2.基于SVD(参考地址：<a href="http://www.codeweblog.com/svd-%E7%AC%94%E8%AE%B0/" target="_blank" rel="noopener">http://www.codeweblog.com/svd-%E7%AC%94%E8%AE%B0/</a>)</li>
</ul>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/14.SVD/%E5%9F%BA%E4%BA%8ESVD.png" alt="基于SVD.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 基于SVD的评分估计</span></span><br><span class="line"><span class="comment"># 在recommend() 中，这个函数用于替换对standEst()的调用，该函数对给定用户给定物品构建了一个评分估计值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svdEst</span><span class="params">(dataMat, user, simMeas, item)</span>:</span></span><br><span class="line">    <span class="string">"""svdEst(计算某用户未评分物品中，以对该物品和其他物品评分的用户的物品相似度，然后进行综合评分)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataMat         训练数据集</span></span><br><span class="line"><span class="string">        user            用户编号</span></span><br><span class="line"><span class="string">        simMeas         相似度计算方法</span></span><br><span class="line"><span class="string">        item            未评分的物品编号</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        ratSimTotal/simTotal     评分（0～5之间的值）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 物品数目</span></span><br><span class="line">    n = shape(dataMat)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 对数据集进行SVD分解</span></span><br><span class="line">    simTotal = <span class="number">0.0</span></span><br><span class="line">    ratSimTotal = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 奇异值分解</span></span><br><span class="line">    <span class="comment"># 在SVD分解之后，我们只利用包含了90%能量值的奇异值，这些奇异值会以NumPy数组的形式得以保存</span></span><br><span class="line">    U, Sigma, VT = la.svd(dataMat)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># # 分析 Sigma 的长度取值</span></span><br><span class="line">    <span class="comment"># analyse_data(Sigma, 20)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果要进行矩阵运算，就必须要用这些奇异值构建出一个对角矩阵</span></span><br><span class="line">    Sig4 = mat(eye(<span class="number">4</span>) * Sigma[: <span class="number">4</span>])</span><br><span class="line">    <span class="comment"># 利用U矩阵将物品转换到低维空间中，构建转换后的物品(物品+4个主要的特征)</span></span><br><span class="line">    xformedItems = dataMat.T * U[:, :<span class="number">4</span>] * Sig4.I</span><br><span class="line">    <span class="comment"># 对于给定的用户，for循环在用户对应行的元素上进行遍历，</span></span><br><span class="line">    <span class="comment"># 这和standEst()函数中的for循环的目的一样，只不过这里的相似度计算时在低维空间下进行的。</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        userRating = dataMat[user, j]</span><br><span class="line">        <span class="keyword">if</span> userRating == <span class="number">0</span> <span class="keyword">or</span> j == item:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># 相似度的计算方法也会作为一个参数传递给该函数</span></span><br><span class="line">        similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T)</span><br><span class="line">        <span class="comment"># for 循环中加入了一条print语句，以便了解相似度计算的进展情况。如果觉得累赘，可以去掉</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'the %d and %d similarity is: %f'</span> % (item, j, similarity)</span><br><span class="line">        <span class="comment"># 对相似度不断累加求和</span></span><br><span class="line">        simTotal += similarity</span><br><span class="line">        <span class="comment"># 对相似度及对应评分值的乘积求和</span></span><br><span class="line">        ratSimTotal += similarity * userRating</span><br><span class="line">    <span class="keyword">if</span> simTotal == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 计算估计评分</span></span><br><span class="line">        <span class="keyword">return</span> ratSimTotal/simTotal</span><br></pre></td></tr></table></figure>

<p>排序获取最后的推荐结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># recommend()函数，就是推荐引擎，它默认调用standEst()函数，产生了最高的N个推荐结果。</span></span><br><span class="line"><span class="comment"># 如果不指定N的大小，则默认值为3。该函数另外的参数还包括相似度计算方法和估计方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommend</span><span class="params">(dataMat, user, N=<span class="number">3</span>, simMeas=cosSim, estMethod=standEst)</span>:</span></span><br><span class="line">    <span class="comment"># 寻找未评级的物品</span></span><br><span class="line">    <span class="comment"># 对给定的用户建立一个未评分的物品列表</span></span><br><span class="line">    unratedItems = nonzero(dataMat[user, :].A == <span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 如果不存在未评分物品，那么就退出函数</span></span><br><span class="line">    <span class="keyword">if</span> len(unratedItems) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'you rated everything'</span></span><br><span class="line">    <span class="comment"># 物品的编号和评分值</span></span><br><span class="line">    itemScores = []</span><br><span class="line">    <span class="comment"># 在未评分物品上进行循环</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> unratedItems:</span><br><span class="line">        estimatedScore = estMethod(dataMat, user, simMeas, item)</span><br><span class="line">        <span class="comment"># 寻找前N个未评级物品，调用standEst()来产生该物品的预测得分，该物品的编号和估计值会放在一个元素列表itemScores中</span></span><br><span class="line">        itemScores.append((item, estimatedScore))</span><br><span class="line">        <span class="comment"># 按照估计得分，对该列表进行排序并返回。列表逆排序，第一个值就是最大值</span></span><br><span class="line">    <span class="keyword">return</span> sorted(itemScores, key=<span class="keyword">lambda</span> jj: jj[<span class="number">1</span>], reverse=<span class="literal">True</span>)[: N]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试 和 项目调用，可直接参考我们的代码</p>
</blockquote>
<p><a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/14.SVD/svdRecommend.py" target="_blank" rel="noopener">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/14.SVD/svdRecommend.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/14.SVD/svdRecommend.py</a></p>
<h4 id="要点补充"><a href="#要点补充" class="headerlink" title="要点补充"></a>要点补充</h4><blockquote>
<p>基于内容(content-based)的推荐</p>
</blockquote>
<ol>
<li>通过各种标签来标记菜肴</li>
<li>将这些属性作为相似度计算所需要的数据</li>
<li>这就是：基于内容的推荐。</li>
</ol>
<blockquote>
<p>构建推荐引擎面临的挑战</p>
</blockquote>
<p>问题</p>
<ul>
<li>1）在大规模的数据集上，SVD分解会降低程序的速度</li>
<li>2）存在其他很多规模扩展性的挑战性问题，比如矩阵的表示方法和计算相似度得分消耗资源。</li>
<li>3）如何在缺乏数据时给出好的推荐-称为冷启动【简单说：用户不会喜欢一个无效的物品，而用户不喜欢的物品又无效】</li>
</ul>
<p>建议</p>
<ul>
<li>1）在大型系统中，SVD分解(可以在程序调入时运行一次)每天运行一次或者其频率更低，并且还要离线运行。</li>
<li>2）在实际中，另一个普遍的做法就是离线计算并保存相似度得分。(物品相似度可能被用户重复的调用)</li>
<li>3）冷启动问题，解决方案就是将推荐看成是搜索问题，通过各种标签／属性特征进行<code>基于内容的推荐</code>。</li>
</ul>
<h3 id="项目案例-基于-SVD-的图像压缩"><a href="#项目案例-基于-SVD-的图像压缩" class="headerlink" title="项目案例: 基于 SVD 的图像压缩"></a>项目案例: 基于 SVD 的图像压缩</h3><blockquote>
<p>收集 并 准备数据</p>
</blockquote>
<p>将文本数据转化为矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载并转换数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imgLoadData</span><span class="params">(filename)</span>:</span></span><br><span class="line">    myl = []</span><br><span class="line">    <span class="comment"># 打开文本文件，并从文件以数组方式读入字符</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> open(filename).readlines():</span><br><span class="line">        newRow = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">            newRow.append(int(line[i]))</span><br><span class="line">        myl.append(newRow)</span><br><span class="line">    <span class="comment"># 矩阵调入后，就可以在屏幕上输出该矩阵</span></span><br><span class="line">    myMat = mat(myl)</span><br><span class="line">    <span class="keyword">return</span> myMat</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据: 分析 Sigma 的长度个数</p>
</blockquote>
<p>通常保留矩阵 80% ～ 90% 的能量，就可以得到重要的特征并去除噪声。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analyse_data</span><span class="params">(Sigma, loopNum=<span class="number">20</span>)</span>:</span></span><br><span class="line">    <span class="string">"""analyse_data(分析 Sigma 的长度取值)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Sigma         Sigma的值</span></span><br><span class="line"><span class="string">        loopNum       循环次数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 总方差的集合（总能量值）</span></span><br><span class="line">    Sig2 = Sigma**<span class="number">2</span></span><br><span class="line">    SigmaSum = sum(Sig2)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(loopNum):</span><br><span class="line">        SigmaI = sum(Sig2[:i+<span class="number">1</span>])</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        根据自己的业务情况，就行处理，设置对应的 Singma 次数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        通常保留矩阵 80% ～ 90% 的能量，就可以得到重要的特征并取出噪声。</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'主成分：%s, 方差占比：%s%%'</span> % (format(i+<span class="number">1</span>, <span class="string">'2.0f'</span>), format(SigmaI/SigmaSum*<span class="number">100</span>, <span class="string">'4.2f'</span>))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法: 对比使用 SVD 前后的数据差异对比，对于存储大家可以试着写写</p>
</blockquote>
<p>例如：<code>32*32=1024 =&gt; 32*2+2*1+32*2=130</code>(2*1表示去掉了除对角线的0), 几乎获得了10倍的压缩比。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打印矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printMat</span><span class="params">(inMat, thresh=<span class="number">0.8</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 由于矩阵保护了浮点数，因此定义浅色和深色，遍历所有矩阵元素，当元素大于阀值时打印1，否则打印0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">            <span class="keyword">if</span> float(inMat[i, k]) &gt; thresh:</span><br><span class="line">                <span class="keyword">print</span> <span class="number">1</span>,</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">print</span> <span class="number">0</span>,</span><br><span class="line">        <span class="keyword">print</span> <span class="string">''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现图像压缩，允许基于任意给定的奇异值数目来重构图像</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imgCompress</span><span class="params">(numSV=<span class="number">3</span>, thresh=<span class="number">0.8</span>)</span>:</span></span><br><span class="line">    <span class="string">"""imgCompress( )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        numSV       Sigma长度   </span></span><br><span class="line"><span class="string">        thresh      判断的阈值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 构建一个列表</span></span><br><span class="line">    myMat = imgLoadData(<span class="string">'data/14.SVD/0_5.txt'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"****original matrix****"</span></span><br><span class="line">    <span class="comment"># 对原始图像进行SVD分解并重构图像e</span></span><br><span class="line">    printMat(myMat, thresh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过Sigma 重新构成SigRecom来实现</span></span><br><span class="line">    <span class="comment"># Sigma是一个对角矩阵，因此需要建立一个全0矩阵，然后将前面的那些奇异值填充到对角线上。</span></span><br><span class="line">    U, Sigma, VT = la.svd(myMat)</span><br><span class="line">    <span class="comment"># SigRecon = mat(zeros((numSV, numSV)))</span></span><br><span class="line">    <span class="comment"># for k in range(numSV):</span></span><br><span class="line">    <span class="comment">#     SigRecon[k, k] = Sigma[k]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分析插入的 Sigma 长度</span></span><br><span class="line">    analyse_data(Sigma, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    SigRecon = mat(eye(numSV) * Sigma[: numSV])</span><br><span class="line">    reconMat = U[:, :numSV] * SigRecon * VT[:numSV, :]</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"****reconstructed matrix using %d singular values *****"</span> % numSV</span><br><span class="line">    printMat(reconMat, thresh)</span><br></pre></td></tr></table></figure>

<p><a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/14.SVD/svdRecommend.py" target="_blank" rel="noopener">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/14.SVD/svdRecommend.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/14.SVD/svdRecommend.py</a></p>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a>      </li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第11章 使用Apriori算法进行关联分析</title>
    <url>/2020/06/03/11.%E4%BD%BF%E7%94%A8Apriori%E7%AE%97%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="关联分析"><a href="#关联分析" class="headerlink" title="关联分析"></a>关联分析</h2><p>关联分析是一种在大规模数据集中寻找有趣关系的任务。<br>这些关系可以有两种形式: </p>
<ul>
<li><p>频繁项集（frequent item sets）: 经常出现在一块的物品的集合。</p>
</li>
<li><p>关联规则（associational rules）: 暗示两种物品之间可能存在很强的关系。</p>
<a id="more"></a>
<h2 id="相关术语"><a href="#相关术语" class="headerlink" title="相关术语"></a>相关术语</h2></li>
<li><p>关联分析（关联规则学习): 从大规模数据集中寻找物品间的隐含关系被称作 <code>关联分析(associati analysis)</code> 或者 <code>关联规则学习（association rule learning）</code> 。<br>下面是用一个 <code>杂货店</code> 例子来说明这两个概念，如下图所示:<br><img src="http://data.apachecn.org/img/AiLearning/ml/11.Apriori/apachecn_apriori_association_demo_1.jpg" alt="关联分析示例1"></p>
</li>
<li><p>频繁项集: {葡萄酒, 尿布, 豆奶} 就是一个频繁项集的例子。</p>
</li>
<li><p>关联规则: 尿布 -&gt; 葡萄酒 就是一个关联规则。这意味着如果顾客买了尿布，那么他很可能会买葡萄酒。</p>
</li>
</ul>
<p>那么 <code>频繁</code> 的定义是什么呢？怎么样才算频繁呢？<br>度量它们的方法有很多种，这里我们来简单的介绍下支持度和可信度。</p>
<ul>
<li>支持度: 数据集中包含该项集的记录所占的比例。例如上图中，{豆奶} 的支持度为 4/5。{豆奶, 尿布} 的支持度为 3/5。</li>
<li>可信度: 针对一条诸如 {尿布} -&gt; {葡萄酒} 这样具体的关联规则来定义的。这条规则的 <code>可信度</code> 被定义为 <code>支持度({尿布, 葡萄酒})/支持度({尿布})</code>，从图中可以看出 支持度({尿布, 葡萄酒}) = 3/5，支持度({尿布}) = 4/5，所以 {尿布} -&gt; {葡萄酒} 的可信度 = 3/5 / 4/5 = 3/4 = 0.75。</li>
</ul>
<p><code>支持度</code> 和 <code>可信度</code> 是用来量化 <code>关联分析</code> 是否成功的一个方法。<br>假设想找到支持度大于 0.8 的所有项集，应该如何去做呢？<br>一个办法是生成一个物品所有可能组合的清单，然后对每一种组合统计它出现的频繁程度，但是当物品成千上万时，上述做法就非常非常慢了。<br>我们需要详细分析下这种情况并讨论下 Apriori 原理，该原理会减少关联规则学习时所需的计算量。</p>
<h2 id="Apriori-原理"><a href="#Apriori-原理" class="headerlink" title="Apriori 原理"></a>Apriori 原理</h2><p>假设我们一共有 4 个商品: 商品0, 商品1, 商品2, 商品3。<br>所有可能的情况如下:<br><img src="http://data.apachecn.org/img/AiLearning/ml/11.Apriori/apachecn_apriori_goods_all_1.jpg" alt="4种商品的所有组合"><br>如果我们计算所有组合的支持度，也需要计算 15 次。即 2^N - 1 = 2^4 - 1 = 15。<br>随着物品的增加，计算的次数呈指数的形式增长 …<br>为了降低计算次数和时间，研究人员发现了一种所谓的 Apriori 原理，即某个项集是频繁的，那么它的所有子集也是频繁的。<br>例如，如果 {0, 1} 是频繁的，那么 {0}, {1} 也是频繁的。<br>该原理直观上没有什么帮助，但是如果反过来看就有用了，也就是说如果一个项集是 <code>非频繁项集</code>，那么它的所有超集也是非频繁项集，如下图所示:  </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/11.Apriori/%E9%9D%9E%E9%A2%91%E7%B9%81%E9%A1%B9%E9%9B%86.png" alt="非频繁项集"></p>
<p>在图中我们可以看到，已知灰色部分 {2,3} 是 <code>非频繁项集</code>，那么利用上面的知识，我们就可以知道 {0,2,3} {1,2,3} {0,1,2,3} 都是 <code>非频繁的</code>。<br>也就是说，计算出 {2,3} 的支持度，知道它是 <code>非频繁</code> 的之后，就不需要再计算 {0,2,3} {1,2,3} {0,1,2,3} 的支持度，因为我们知道这些集合不会满足我们的要求。<br>使用该原理就可以避免项集数目的指数增长，从而在合理的时间内计算出频繁项集。</p>
<p>Apriori 算法优缺点</p>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="bullet">* </span>优点：易编码实现</span><br><span class="line"><span class="bullet">* </span>缺点：在大数据集上可能较慢</span><br><span class="line"><span class="bullet">* </span>适用数据类型：数值型 或者 标称型数据。</span><br></pre></td></tr></table></figure>

<p>Apriori 算法流程步骤：</p>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="bullet">* </span>收集数据：使用任意方法。</span><br><span class="line"><span class="bullet">* </span>准备数据：任何数据类型都可以，因为我们只保存集合。</span><br><span class="line"><span class="bullet">* </span>分析数据：使用任意方法。</span><br><span class="line"><span class="bullet">* </span>训练数据：使用Apiori算法来找到频繁项集。</span><br><span class="line"><span class="bullet">* </span>测试算法：不需要测试过程。</span><br><span class="line"><span class="bullet">* </span>使用算法：用于发现频繁项集以及物品之间的关联规则。</span><br></pre></td></tr></table></figure>

<h2 id="Apriori-算法的使用"><a href="#Apriori-算法的使用" class="headerlink" title="Apriori 算法的使用"></a>Apriori 算法的使用</h2><p>前面提到，关联分析的目标包括两项: 发现 <code>频繁项集</code> 和发现 <code>关联规则</code>。<br>首先需要找到 <code>频繁项集</code>，然后才能发现 <code>关联规则</code>。<br><code>Apriori</code> 算法是发现 <code>频繁项集</code> 的一种方法。<br>Apriori 算法的两个输入参数分别是最小支持度和数据集。<br>该算法首先会生成所有单个物品的项集列表。<br>接着扫描交易记录来查看哪些项集满足最小支持度要求，那些不满足最小支持度要求的集合会被去掉。<br>燃尽后对生下来的集合进行组合以声场包含两个元素的项集。<br>接下来再重新扫描交易记录，去掉不满足最小支持度的项集。<br>该过程重复进行直到所有项集被去掉。</p>
<h3 id="生成候选项集"><a href="#生成候选项集" class="headerlink" title="生成候选项集"></a>生成候选项集</h3><p>下面会创建一个用于构建初始集合的函数，也会创建一个通过扫描数据集以寻找交易记录子集的函数，<br>数据扫描的伪代码如下: </p>
<ul>
<li>对数据集中的每条交易记录 tran</li>
<li>对每个候选项集 can<ul>
<li>检查一下 can 是否是 tran 的子集: 如果是则增加 can 的计数值</li>
</ul>
</li>
<li>对每个候选项集<ul>
<li>如果其支持度不低于最小值，则保留该项集</li>
<li>返回所有频繁项集列表<br>以下是一些辅助函数。</li>
</ul>
</li>
</ul>
<h4 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [[<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">5</span>]]</span><br></pre></td></tr></table></figure>

<h4 id="创建集合-C1。即对-dataSet-进行去重，排序，放入-list-中，然后转换所有的元素为-frozenset"><a href="#创建集合-C1。即对-dataSet-进行去重，排序，放入-list-中，然后转换所有的元素为-frozenset" class="headerlink" title="创建集合 C1。即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset"></a>创建集合 C1。即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建集合 C1。即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createC1</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""createC1（创建集合 C1）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 原始数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        frozenset 返回一个 frozenset 格式的 list</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    C1 = []</span><br><span class="line">    <span class="keyword">for</span> transaction <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> transaction:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> [item] <span class="keyword">in</span> C1:</span><br><span class="line">                <span class="comment"># 遍历所有的元素，如果不在 C1 出现过，那么就 append</span></span><br><span class="line">                C1.append([item])</span><br><span class="line">    <span class="comment"># 对数组进行 `从小到大` 的排序</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'sort 前='</span>, C1</span><br><span class="line">    C1.sort()</span><br><span class="line">    <span class="comment"># frozenset 表示冻结的 set 集合，元素无改变；可以把它当字典的 key 来使用</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'sort 后='</span>, C1</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'frozenset='</span>, map(frozenset, C1)</span><br><span class="line">    <span class="keyword">return</span> map(frozenset, C1)</span><br></pre></td></tr></table></figure>

<h4 id="计算候选数据集-CK-在数据集-D-中的支持度，并返回支持度大于最小支持度（minSupport）的数据"><a href="#计算候选数据集-CK-在数据集-D-中的支持度，并返回支持度大于最小支持度（minSupport）的数据" class="headerlink" title="计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度（minSupport）的数据"></a>计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度（minSupport）的数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度（minSupport）的数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanD</span><span class="params">(D, Ck, minSupport)</span>:</span></span><br><span class="line">    <span class="string">"""scanD（计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度 minSupport 的数据）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        D 数据集</span></span><br><span class="line"><span class="string">        Ck 候选项集列表</span></span><br><span class="line"><span class="string">        minSupport 最小支持度</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        retList 支持度大于 minSupport 的集合</span></span><br><span class="line"><span class="string">        supportData 候选项集支持度数据</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ssCnt 临时存放选数据集 Ck 的频率. 例如: a-&gt;10, b-&gt;5, c-&gt;8</span></span><br><span class="line">    ssCnt = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> tid <span class="keyword">in</span> D:</span><br><span class="line">        <span class="keyword">for</span> can <span class="keyword">in</span> Ck:</span><br><span class="line">            <span class="comment"># s.issubset(t)  测试是否 s 中的每一个元素都在 t 中</span></span><br><span class="line">            <span class="keyword">if</span> can.issubset(tid):</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> ssCnt.has_key(can):</span><br><span class="line">                    ssCnt[can] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    ssCnt[can] += <span class="number">1</span></span><br><span class="line">    numItems = float(len(D)) <span class="comment"># 数据集 D 的数量</span></span><br><span class="line">    retList = []</span><br><span class="line">    supportData = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> ssCnt:</span><br><span class="line">        <span class="comment"># 支持度 = 候选项（key）出现的次数 / 所有数据集的数量</span></span><br><span class="line">        support = ssCnt[key]/numItems</span><br><span class="line">        <span class="keyword">if</span> support &gt;= minSupport:</span><br><span class="line">            <span class="comment"># 在 retList 的首位插入元素，只存储支持度满足频繁项集的值</span></span><br><span class="line">            retList.insert(<span class="number">0</span>, key)</span><br><span class="line">        <span class="comment"># 存储所有的候选项（key）和对应的支持度（support）</span></span><br><span class="line">        supportData[key] = support</span><br><span class="line">    <span class="keyword">return</span> retList, supportData</span><br></pre></td></tr></table></figure>

<p>完整代码地址: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/11.Apriori/apriori.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/11.Apriori/apriori.py</a></p>
<h3 id="组织完整的-Apriori-算法"><a href="#组织完整的-Apriori-算法" class="headerlink" title="组织完整的 Apriori 算法"></a>组织完整的 Apriori 算法</h3><h4 id="输入频繁项集列表-Lk-与返回的元素个数-k，然后输出所有可能的候选项集-Ck"><a href="#输入频繁项集列表-Lk-与返回的元素个数-k，然后输出所有可能的候选项集-Ck" class="headerlink" title="输入频繁项集列表 Lk 与返回的元素个数 k，然后输出所有可能的候选项集 Ck"></a>输入频繁项集列表 Lk 与返回的元素个数 k，然后输出所有可能的候选项集 Ck</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入频繁项集列表 Lk 与返回的元素个数 k，然后输出所有可能的候选项集 Ck</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aprioriGen</span><span class="params">(Lk, k)</span>:</span></span><br><span class="line">    <span class="string">"""aprioriGen（输入频繁项集列表 Lk 与返回的元素个数 k，然后输出候选项集 Ck。</span></span><br><span class="line"><span class="string">       例如: 以 &#123;0&#125;,&#123;1&#125;,&#123;2&#125; 为输入且 k = 2 则输出 &#123;0,1&#125;, &#123;0,2&#125;, &#123;1,2&#125;. 以 &#123;0,1&#125;,&#123;0,2&#125;,&#123;1,2&#125; 为输入且 k = 3 则输出 &#123;0,1,2&#125;</span></span><br><span class="line"><span class="string">       仅需要计算一次，不需要将所有的结果计算出来，然后进行去重操作</span></span><br><span class="line"><span class="string">       这是一个更高效的算法）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Lk 频繁项集列表</span></span><br><span class="line"><span class="string">        k 返回的项集元素个数（若元素的前 k-2 相同，就进行合并）</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        retList 元素两两合并的数据集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    retList = []</span><br><span class="line">    lenLk = len(Lk)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(lenLk):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, lenLk):</span><br><span class="line">            L1 = list(Lk[i])[: k<span class="number">-2</span>]</span><br><span class="line">            L2 = list(Lk[j])[: k<span class="number">-2</span>]</span><br><span class="line">            <span class="comment"># print '-----i=', i, k-2, Lk, Lk[i], list(Lk[i])[: k-2]</span></span><br><span class="line">            <span class="comment"># print '-----j=', j, k-2, Lk, Lk[j], list(Lk[j])[: k-2]</span></span><br><span class="line">            L1.sort()</span><br><span class="line">            L2.sort()</span><br><span class="line">            <span class="comment"># 第一次 L1,L2 为空，元素直接进行合并，返回元素两两合并的数据集</span></span><br><span class="line">            <span class="comment"># if first k-2 elements are equal</span></span><br><span class="line">            <span class="keyword">if</span> L1 == L2:</span><br><span class="line">                <span class="comment"># set union</span></span><br><span class="line">                <span class="comment"># print 'union=', Lk[i] | Lk[j], Lk[i], Lk[j]</span></span><br><span class="line">                retList.append(Lk[i] | Lk[j])</span><br><span class="line">    <span class="keyword">return</span> retList</span><br></pre></td></tr></table></figure>

<h4 id="找出数据集-dataSet-中支持度-gt-最小支持度的候选项集以及它们的支持度。即我们的频繁项集。"><a href="#找出数据集-dataSet-中支持度-gt-最小支持度的候选项集以及它们的支持度。即我们的频繁项集。" class="headerlink" title="找出数据集 dataSet 中支持度 &gt;= 最小支持度的候选项集以及它们的支持度。即我们的频繁项集。"></a>找出数据集 dataSet 中支持度 &gt;= 最小支持度的候选项集以及它们的支持度。即我们的频繁项集。</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 找出数据集 dataSet 中支持度 &gt;= 最小支持度的候选项集以及它们的支持度。即我们的频繁项集。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apriori</span><span class="params">(dataSet, minSupport=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""apriori（首先构建集合 C1，然后扫描数据集来判断这些只有一个元素的项集是否满足最小支持度的要求。那么满足最小支持度要求的项集构成集合 L1。然后 L1 中的元素相互组合成 C2，C2 再进一步过滤变成 L2，然后以此类推，知道 CN 的长度为 0 时结束，即可找出所有频繁项集的支持度。）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 原始数据集</span></span><br><span class="line"><span class="string">        minSupport 支持度的阈值</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        L 频繁项集的全集</span></span><br><span class="line"><span class="string">        supportData 所有元素和支持度的全集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># C1 即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset</span></span><br><span class="line">    C1 = createC1(dataSet)</span><br><span class="line">    <span class="comment"># 对每一行进行 set 转换，然后存放到集合中</span></span><br><span class="line">    D = map(set, dataSet)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'D='</span>, D</span><br><span class="line">    <span class="comment"># 计算候选数据集 C1 在数据集 D 中的支持度，并返回支持度大于 minSupport 的数据</span></span><br><span class="line">    L1, supportData = scanD(D, C1, minSupport)</span><br><span class="line">    <span class="comment"># print "L1=", L1, "\n", "outcome: ", supportData</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># L 加了一层 list, L 一共 2 层 list</span></span><br><span class="line">    L = [L1]</span><br><span class="line">    k = <span class="number">2</span></span><br><span class="line">    <span class="comment"># 判断 L 的第 k-2 项的数据长度是否 &gt; 0。第一次执行时 L 为 [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]]。L[k-2]=L[0]=[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]，最后面 k += 1</span></span><br><span class="line">    <span class="keyword">while</span> (len(L[k<span class="number">-2</span>]) &gt; <span class="number">0</span>):</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'k='</span>, k, L, L[k<span class="number">-2</span>]</span><br><span class="line">        Ck = aprioriGen(L[k<span class="number">-2</span>], k) <span class="comment"># 例如: 以 &#123;0&#125;,&#123;1&#125;,&#123;2&#125; 为输入且 k = 2 则输出 &#123;0,1&#125;, &#123;0,2&#125;, &#123;1,2&#125;. 以 &#123;0,1&#125;,&#123;0,2&#125;,&#123;1,2&#125; 为输入且 k = 3 则输出 &#123;0,1,2&#125;</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'Ck'</span>, Ck</span><br><span class="line"></span><br><span class="line">        Lk, supK = scanD(D, Ck, minSupport) <span class="comment"># 计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于 minSupport 的数据</span></span><br><span class="line">        <span class="comment"># 保存所有候选项集的支持度，如果字典没有，就追加元素，如果有，就更新元素</span></span><br><span class="line">        supportData.update(supK)</span><br><span class="line">        <span class="keyword">if</span> len(Lk) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># Lk 表示满足频繁子项的集合，L 元素在增加，例如: </span></span><br><span class="line">        <span class="comment"># l=[[set(1), set(2), set(3)]]</span></span><br><span class="line">        <span class="comment"># l=[[set(1), set(2), set(3)], [set(1, 2), set(2, 3)]]</span></span><br><span class="line">        L.append(Lk)</span><br><span class="line">        k += <span class="number">1</span></span><br><span class="line">        <span class="comment"># print 'k=', k, len(L[k-2])</span></span><br><span class="line">    <span class="keyword">return</span> L, supportData</span><br></pre></td></tr></table></figure>

<p>到这一步，我们就找出我们所需要的 <code>频繁项集</code> 和他们的 <code>支持度</code> 了，接下来再找出关联规则即可！</p>
<p>完整代码地址: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/11.Apriori/apriori.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/11.Apriori/apriori.py</a></p>
<h2 id="从频繁项集中挖掘关联规则"><a href="#从频繁项集中挖掘关联规则" class="headerlink" title="从频繁项集中挖掘关联规则"></a>从频繁项集中挖掘关联规则</h2><p>前面我们介绍了用于发现 <code>频繁项集</code> 的 Apriori 算法，现在要解决的问题是如何找出 <code>关联规则</code>。</p>
<p>要找到 <code>关联规则</code>，我们首先从一个 <code>频繁项集</code> 开始。<br>我们知道集合中的元素是不重复的，但我们想知道基于这些元素能否获得其它内容。<br>某个元素或某个元素集合可能会推导出另一个元素。<br>从先前 <code>杂货店</code> 的例子可以得到，如果有一个频繁项集 {豆奶,莴苣}，那么就可能有一条关联规则 “豆奶 -&gt; 莴苣”。<br>这意味着如果有人买了豆奶，那么在统计上他会购买莴苣的概率比较大。<br>但是，这一条件反过来并不总是成立。<br>也就是说 “豆奶 -&gt; 莴苣” 统计上显著，那么 “莴苣 -&gt; 豆奶” 也不一定成立。</p>
<p>前面我们给出了 <code>频繁项集</code> 的量化定义，即它满足最小支持度要求。<br>对于 <code>关联规则</code>，我们也有类似的量化方法，这种量化指标称之为 <code>可信度</code>。<br>一条规则 A -&gt; B 的可信度定义为 support(A | B) / support(A)。（注意: 在 python 中 | 表示集合的并操作，而数学书集合并的符号是 U）。<br><code>A | B</code> 是指所有出现在集合 A 或者集合 B 中的元素。<br>由于我们先前已经计算出所有 <code>频繁项集</code> 的支持度了，现在我们要做的只不过是提取这些数据做一次除法运算即可。</p>
<h3 id="一个频繁项集可以产生多少条关联规则呢？"><a href="#一个频繁项集可以产生多少条关联规则呢？" class="headerlink" title="一个频繁项集可以产生多少条关联规则呢？"></a>一个频繁项集可以产生多少条关联规则呢？</h3><p>如下图所示，给出的是项集 {0,1,2,3} 产生的所有关联规则:<br><img src="http://data.apachecn.org/img/AiLearning/ml/11.Apriori/apachecn_association_rule_demo_1.jpg" alt="关联规则网格示意图"><br>与我们前面的 <code>频繁项集</code> 生成一样，我们可以为每个频繁项集产生许多关联规则。<br>如果能减少规则的数目来确保问题的可解析，那么计算起来就会好很多。<br>通过观察，我们可以知道，如果某条规则并不满足 <code>最小可信度</code> 要求，那么该规则的所有子集也不会满足 <code>最小可信度</code> 的要求。<br>如上图所示，假设 <code>123 -&gt; 3</code>  并不满足最小可信度要求，那么就知道任何左部为 {0,1,2} 子集的规则也不会满足 <code>最小可信度</code> 的要求。<br>即 <code>12 -&gt; 03</code> , <code>02 -&gt; 13</code> , <code>01 -&gt; 23</code> , <code>2 -&gt; 013</code>, <code>1 -&gt; 023</code>, <code>0 -&gt; 123</code> 都不满足 <code>最小可信度</code> 要求。  </p>
<p>可以利用关联规则的上述性质属性来减少需要测试的规则数目，跟先前 Apriori 算法的套路一样。<br>以下是一些辅助函数: </p>
<h4 id="计算可信度"><a href="#计算可信度" class="headerlink" title="计算可信度"></a>计算可信度</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算可信度（confidence）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcConf</span><span class="params">(freqSet, H</span></span></span><br><span class="line"><span class="function"><span class="params">, supportData, brl, minConf=<span class="number">0.7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""calcConf（对两个元素的频繁项，计算可信度，例如： &#123;1,2&#125;/&#123;1&#125; 或者 &#123;1,2&#125;/&#123;2&#125; 看是否满足条件）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        freqSet 频繁项集中的元素，例如: frozenset([1, 3])    </span></span><br><span class="line"><span class="string">        H 频繁项集中的元素的集合，例如: [frozenset([1]), frozenset([3])]</span></span><br><span class="line"><span class="string">        supportData 所有元素的支持度的字典</span></span><br><span class="line"><span class="string">        brl 关联规则列表的空数组</span></span><br><span class="line"><span class="string">        minConf 最小可信度</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        prunedH 记录 可信度大于阈值的集合</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 记录可信度大于最小可信度（minConf）的集合</span></span><br><span class="line">    prunedH = []</span><br><span class="line">    <span class="keyword">for</span> conseq <span class="keyword">in</span> H: <span class="comment"># 假设 freqSet = frozenset([1, 3]), H = [frozenset([1]), frozenset([3])]，那么现在需要求出 frozenset([1]) -&gt; frozenset([3]) 的可信度和 frozenset([3]) -&gt; frozenset([1]) 的可信度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print 'confData=', freqSet, H, conseq, freqSet-conseq</span></span><br><span class="line">        conf = supportData[freqSet]/supportData[freqSet-conseq] <span class="comment"># 支持度定义: a -&gt; b = support(a | b) / support(a). 假设  freqSet = frozenset([1, 3]), conseq = [frozenset([1])]，那么 frozenset([1]) 至 frozenset([3]) 的可信度为 = support(a | b) / support(a) = supportData[freqSet]/supportData[freqSet-conseq] = supportData[frozenset([1, 3])] / supportData[frozenset([1])]</span></span><br><span class="line">        <span class="keyword">if</span> conf &gt;= minConf:</span><br><span class="line">            <span class="comment"># 只要买了 freqSet-conseq 集合，一定会买 conseq 集合（freqSet-conseq 集合和 conseq 集合是全集）</span></span><br><span class="line">            <span class="keyword">print</span> freqSet-conseq, <span class="string">'--&gt;'</span>, conseq, <span class="string">'conf:'</span>, conf</span><br><span class="line">            brl.append((freqSet-conseq, conseq, conf))</span><br><span class="line">            prunedH.append(conseq)</span><br><span class="line">    <span class="keyword">return</span> prunedH</span><br><span class="line">`</span><br></pre></td></tr></table></figure>

<h4 id="递归计算频繁项集的规则"><a href="#递归计算频繁项集的规则" class="headerlink" title="递归计算频繁项集的规则"></a>递归计算频繁项集的规则</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 递归计算频繁项集的规则</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rulesFromConseq</span><span class="params">(freqSet, H, supportData, brl, minConf=<span class="number">0.7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""rulesFromConseq</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        freqSet 频繁项集中的元素，例如: frozenset([2, 3, 5])    </span></span><br><span class="line"><span class="string">        H 频繁项集中的元素的集合，例如: [frozenset([2]), frozenset([3]), frozenset([5])]</span></span><br><span class="line"><span class="string">        supportData 所有元素的支持度的字典</span></span><br><span class="line"><span class="string">        brl 关联规则列表的数组</span></span><br><span class="line"><span class="string">        minConf 最小可信度</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># H[0] 是 freqSet 的元素组合的第一个元素，并且 H 中所有元素的长度都一样，长度由 aprioriGen(H, m+1) 这里的 m + 1 来控制</span></span><br><span class="line">    <span class="comment"># 该函数递归时，H[0] 的长度从 1 开始增长 1 2 3 ...</span></span><br><span class="line">    <span class="comment"># 假设 freqSet = frozenset([2, 3, 5]), H = [frozenset([2]), frozenset([3]), frozenset([5])]</span></span><br><span class="line">    <span class="comment"># 那么 m = len(H[0]) 的递归的值依次为 1 2</span></span><br><span class="line">    <span class="comment"># 在 m = 2 时, 跳出该递归。假设再递归一次，那么 H[0] = frozenset([2, 3, 5])，freqSet = frozenset([2, 3, 5]) ，没必要再计算 freqSet 与 H[0] 的关联规则了。</span></span><br><span class="line">    m = len(H[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> (len(freqSet) &gt; (m + <span class="number">1</span>)):</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'freqSet******************'</span>, len(freqSet), m + <span class="number">1</span>, freqSet, H, H[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 生成 m+1 个长度的所有可能的 H 中的组合，假设 H = [frozenset([2]), frozenset([3]), frozenset([5])]</span></span><br><span class="line">        <span class="comment"># 第一次递归调用时生成 [frozenset([2, 3]), frozenset([2, 5]), frozenset([3, 5])]</span></span><br><span class="line">        <span class="comment"># 第二次 。。。没有第二次，递归条件判断时已经退出了</span></span><br><span class="line">        Hmp1 = aprioriGen(H, m+<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 返回可信度大于最小可信度的集合</span></span><br><span class="line">        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'Hmp1='</span>, Hmp1</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'len(Hmp1)='</span>, len(Hmp1), <span class="string">'len(freqSet)='</span>, len(freqSet)</span><br><span class="line">        <span class="comment"># 计算可信度后，还有数据大于最小可信度的话，那么继续递归调用，否则跳出递归</span></span><br><span class="line">        <span class="keyword">if</span> (len(Hmp1) &gt; <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'----------------------'</span>, Hmp1</span><br><span class="line">            <span class="comment"># print len(freqSet),  len(Hmp1[0]) + 1</span></span><br><span class="line">            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)</span><br></pre></td></tr></table></figure>

<h4 id="生成关联规则"><a href="#生成关联规则" class="headerlink" title="生成关联规则"></a>生成关联规则</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成关联规则</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateRules</span><span class="params">(L, supportData, minConf=<span class="number">0.7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""generateRules</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        L 频繁项集列表</span></span><br><span class="line"><span class="string">        supportData 频繁项集支持度的字典</span></span><br><span class="line"><span class="string">        minConf 最小置信度</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bigRuleList 可信度规则列表（关于 (A-&gt;B+置信度) 3个字段的组合）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    bigRuleList = []</span><br><span class="line">    <span class="comment"># 假设 L = [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])], [frozenset([2, 3, 5])]]</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(L)):</span><br><span class="line">        <span class="comment"># 获取频繁项集中每个组合的所有元素</span></span><br><span class="line">        <span class="keyword">for</span> freqSet <span class="keyword">in</span> L[i]:</span><br><span class="line">            <span class="comment"># 假设：freqSet= frozenset([1, 3]), H1=[frozenset([1]), frozenset([3])]</span></span><br><span class="line">            <span class="comment"># 组合总的元素并遍历子元素，并转化为 frozenset 集合，再存放到 list 列表中</span></span><br><span class="line">            H1 = [frozenset([item]) <span class="keyword">for</span> item <span class="keyword">in</span> freqSet]</span><br><span class="line">            <span class="comment"># 2 个的组合，走 else, 2 个以上的组合，走 if</span></span><br><span class="line">            <span class="keyword">if</span> (i &gt; <span class="number">1</span>):</span><br><span class="line">                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                calcConf(freqSet, H1, supportData, bigRuleList, minConf)</span><br><span class="line">    <span class="keyword">return</span> bigRuleList</span><br></pre></td></tr></table></figure>

<p>到这里为止，通过调用 generateRules 函数即可得出我们所需的 <code>关联规则</code>。</p>
<ul>
<li>分级法： 频繁项集-&gt;关联规则<ul>
<li>1.首先从一个频繁项集开始，接着创建一个规则列表，其中规则右部分只包含一个元素，然后对这个规则进行测试。</li>
<li>2.接下来合并所有剩余规则来创建一个新的规则列表，其中规则右部包含两个元素。</li>
<li>如下图：</li>
<li><img src="http://data.apachecn.org/img/AiLearning/ml/11.Apriori/%E6%89%80%E6%9C%89%E5%8F%AF%E8%83%BD%E7%9A%84%E9%A1%B9%E9%9B%86%E7%BB%84%E5%90%88.png" alt="所有可能的项集组合"></li>
</ul>
</li>
<li>最后： 每次增加频繁项集的大小，Apriori 算法都会重新扫描整个数据集，是否有优化空间呢？ 下一章：FP-growth算法等着你的到来</li>
</ul>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a>      </li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Snowball for Few-Shot Relation Learning-翻译</title>
    <url>/2020/07/21/%E7%BF%BB%E8%AF%91-Neural%20Snowball%20for%20Few-Shot%20Relation%20Learning/</url>
    <content><![CDATA[<h1 id="Neural-Snowball-for-Few-Shot-Relation-Learning"><a href="#Neural-Snowball-for-Few-Shot-Relation-Learning" class="headerlink" title="Neural Snowball for Few-Shot Relation Learning"></a>Neural Snowball for Few-Shot Relation Learning</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>&emsp;&emsp;知识图谱正处于新的关系不断增长的状况，基于大量充足信息预先定义好关系的关系抽取方法不能很好的处理这种状况。在利用少量的样本来处理新的关系的目标下，我们提出了一种新颖的自展方法-<strong>Neural Snowball</strong>，通过转换现有关系的语义知识来学习新的关系。更详细地说，我们使用了关系连体网络（<a href="https://blog.csdn.net/hei653779919/article/details/106588973" target="_blank" rel="noopener">RSN</a>）来学习基于已有关系及其标记数据的实例之间的关系相似性度量，然后，在给定一个新关系及其少量实例的基础上，利用RSN从未标记的语料库中积累可靠的实例，并用这些实例训练一个关系分类器，进一步识别新关系的新事实。这个过程就像雪球一样反复进行。实验表明我们的模型对于小样本学习可以获取更高质量的实例，并且相比于baseline，性能有重大的提升。代码与数据集发布在<a href="https://github.com/thunlp/Neural-Snowball" target="_blank" rel="noopener">https://github.com/thunlp/Neural-Snowball</a>。<img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/KG-paper-001-01.PNG" alt=""></p>
<a id="more"></a>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>&emsp;&emsp;知识图谱(KG)，如WordNet、Freebase 和 Wikidata，在<strong>信息检索</strong>、<strong>问答系统</strong>、<strong>推荐系统</strong>等领域有广泛应用。知识图谱由表示实体<strong>$e_h$</strong>和实体之<strong>$e_t$</strong>间联系r的三元组形式<strong>$(e_h,r,e_t)$</strong>的事实联系组成。虽然已有的知识图谱包含了大量的事实，但相比较于现实世界的数据来说，知识图谱还有很大的完善空间。为了丰富知识图谱，关系抽取（RE）就是研究从纯文本信息中抽取事实关系的。</p>
<p>&emsp;&emsp;知识抽取（RE）的一大挑战就是，在知识图谱中新型的关系出现的较快，然而大多数关系抽取模型不能很好处理这种出现新关系的情况，原因是抽取模型是基于有限预先定义好关系的数据集，最大的抽取数据集之一FewRel在2018年只有100个关系，然而Wikidata在2014年就已经有了920个关系，更不用说它目前已经包含了6000多个关系。</p>
<p>&emsp;&emsp;为了提取新关系中的关系事实，大多数方法研究了自展的知识抽取，其通过少量的关系事实种子去提取新的三元组关系。Brin（1998）提出用一小组（作者，书名）pairs集合作为输入去提取作者-书名的关系事实，它不断地从网站查找种子pairs集合的信息，然后从这些信息中提取句子样式，然后通过样式匹配去发现新的pairs。Agichtein和Gravano (2000)进一步改进改方法，而且将其命名为<strong>Snowball</strong>，因为信息以及事实的累积像滚雪球一样。</p>
<p>&emsp;&emsp;然而，大多数现有的自展模型都局限于他们自己只利用种子关系事实，而未利用大规模标记数据集的优势，其已经被证明是一种有价值的资源。虽然已存在的关系事实数据与新的关系可能有很不同的分布，但是这些数据依旧可以使用去训练深度学习的模型，该模型可以抽取对既符合事实又没法可见的高纬度的表示特征(Bengio 2012)。这种叫迁移学习的技术已近被广泛被应用与图小样本任务。先前的工作已经研究了用以衡量对象与元信息之间相似度（Ravi and Larochelle 2017）的转移指标（Koch,Zemel, and Salakhutdinov 2015），其为了快速适应新的任务。</p>
<p>&emsp;&emsp;基于自展和迁移学习的方法，我们提出了<strong>Neural Snowball</strong>方法在利用不充足的训练数据去对新关系进行分类。输入一些新的关系事实作为种子实例，<strong>Neural Snowball</strong>发现这些事实的可信赖信息，然后讲这些信息用来去训练通过新关系事实去发现可信赖实例的关系分类器，这些实例然后又作为新的种子输入进行迭代。</p>
<p>我们也利用关系连体网络（RSN）去选择高可信度的实例。连体网络一般包含双编码器和通过学习度量来衡量两个对象之间的相似性。Wu(2019)等人用神经连体网络去判断两个句子是否表达的是相同的关系，设计了RSN 。在传统的自展系统中，模板用于去选择新的实例。由于神经网络具有比模板更好的泛化性能，我们使用RSN通过比较候选实例与现有的来选择高可信度的新实例。</p>
<p>&emsp;&emsp;实验结果表明，在少样本情况下，<strong>Neural Snowball</strong>方法学习新的关系性能有了显著的提升。进一步的研究表明了该方法以及该迭代过程的有效性，而且证明了该方法有能力去选择更可信的实例以及抽取新的关系事实。</p>
<p>&emsp;&emsp;总结一下，我们主要的贡献在一下三个方向：</p>
<ul>
<li>我们提出了<strong>Neural Snowball</strong>新颖的方法，该方法在利用少量新的关系实例情况下，能够从未标签的数据中，利用先前已存在的关系，不断迭代地去累积新的实例与关系事实，能够更好地去训练关系神经网络关系分类器。</li>
<li>为了给新关系选择更好的新的能够支撑的实例，我们研究了RSN来评估候选实例与存在实例之间关系的相似度。</li>
<li>实验结果以及后续的分析表明了该模型的有效性和鲁棒性。   </li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Supervised-RE"><a href="#Supervised-RE" class="headerlink" title="Supervised RE"></a><strong>Supervised RE</strong></h3><p>&emsp;&emsp;早先的监督学习RE用核函数方法(Zelenko, Aone, and Richardella 2003)和嵌入式方法(Gormley, Yu, and Dredze 2015)去利用语法信息来预测关系，近年来，提出的如RNN和CNN的神经模型能够更好的从词序列中提取特征，除此之外，依存句法分析树法也被证实在RE中非常有效。</p>
<h3 id="Distant-Supervision"><a href="#Distant-Supervision" class="headerlink" title="Distant Supervision"></a><strong>Distant Supervision</strong></h3><p>&emsp;&emsp;监督RE方法依赖手工标注的语料库，其通常只覆盖有限的关系和实例的数量。Mintz等人提出远程监督的方法，通过把语料库和知识图谱之间的实体对齐来自动生成新的关系。为减少错误标签，Riedel, Yao, 和McCallum (2010）以及Hoffmann et al. (2011)将远程监督设计成多任务多标签任务模型。</p>
<h3 id="RE-for-New-Relations"><a href="#RE-for-New-Relations" class="headerlink" title="RE for New Relations"></a><strong>RE for New Relations</strong></h3><p>&emsp;&emsp;自展的RE方法可以快速利用少数事实或者句子作为种子集合来适应新的关系。Brin1998年首次提出通过从网上不断迭代扩增的模式来抽取关系事实，Agichtein和Gravano (2000)提出的<strong>Snowball</strong>利用更好的模式抽取和评估方法来提升这种不断迭代的机制。基于上诉，Zhu2019年对于模式选择采用了概率统计的方法。Batista, Martins, 和Silva (2015)使用词嵌入方法去提升<strong>Snowball</strong>方法，许多相似的自展想法已经被研究应用在了RE上面。</p>
<p>&emsp;&emsp;与远程监督方法相比，自展方法迭代地扩增关系事实，从而有很高的准确率。除此之外外，远程监督方法还限制于预先定义好的关系，时至今日自展的方法对于开放式的关系增长更具扩展性，当然其他半监督方法也可以应用在RE中，然而这些方法需要足够多的注释而且其目标主要是分类预先定义好的关系而不是去发现新的关系，因此我们不在进一步讨论这些方法。</p>
<p>&emsp;&emsp;受到人类可以通过少数的实例来掌握新知识现象的启发，解决数据缺乏的小样本学习方法收到了研究人员的青睐。少样本学习的关键点是从已有的数据，将与任务无关的信息从已有的信息转移到新的任务中去。zhang等人研究了一种学习距离分布，以最近邻策略来对新的种类进行分类。Ravi等人提出元学习方法来理解怎么使用少量的样例来快速优化模型。Qiao等人提出的通过学习来预测新任务的分类参数，现有的少样本学习模型主要应用于视觉任务。为了在文本上利用该方法，Han等人发布了一个大规模的少任务抽取数据集-FewRel。</p>
<h3 id="OpenRE"><a href="#OpenRE" class="headerlink" title="OpenRE"></a><strong>OpenRE</strong></h3><p>&emsp;&emsp;上述的自展和少任务学习方法对新任务的学习仅需要少量人为的参与。开放式关系抽取（OpenRE）一方面主要是从没有预先定义关系的文本信息中抽取关系，一种OpenER系统主要聚焦于发现关系信息，而其他的探究通过聚集语义模式来自动形成关系类型。相比较于传统的方法这是一种不同的具有挑战性的观点，值得被继续探索。</p>
<h3 id="Siamese-Networks"><a href="#Siamese-Networks" class="headerlink" title="Siamese Networks"></a><strong>Siamese Networks</strong></h3><p>&emsp;&emsp;连体网络通过其双编码器和距离训练函数来评估两个对象之间的相似性。它的目的是为了少样本学习和评估文本相似度的。Wu提出RSN网络用来学习所给的实例的关系度量，这里我们使用RSN，通过比较候选的样例与已有的样例，来选择更高的可信实例。</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a><strong>Methodology</strong></h2><p>&emsp;&emsp;这节我们将以介绍<strong>Neural Snowball</strong>的相关符号以及定义开始。</p>
<h3 id="Terminology-and-Problem-Definition"><a href="#Terminology-and-Problem-Definition" class="headerlink" title="Terminology and Problem Definition"></a>Terminology and Problem Definition</h3><p>&emsp;&emsp;所给的实例 $x$ 是包含目标实体 $e_h$ 和 $e_t$ 的一个词序列 ${w_1,w_2,…,w_l}$ ,RE的目的是预测实体 $e_h$ 和 $e_t$ 之间的关系标签 $r$ 。<strong>Relation mentions</strong>是指表示所给关系的实例。<strong>Entity pair mentions</strong>是指所给的实体对。<strong>Relation facts</strong>是指三元组<strong>$(e_h,r,e_t)$</strong>表示在实体$e_h$ 和 $e_t$ 之间存在关系 $r$ 。$x^r$ 表示 $x$   是关系 $r$ 的关系描述实例。</p>
<p>&emsp;&emsp;既然我强调从真实现实场景中学习去抽取新的关系，我们在已有的监督RE和少样本RE采取了不同的问题设置。在已有的关系提供大量的标签数据和对于新的关系提供少量的实例集情况下，我们的目的是从包含已有关系，新关系和隐藏关系的查询集合中抽取新关系的实例。</p>
<p>&emsp;&emsp;该任务的输入由一个大量的标签语义库 $S_N = {x_j^{r_i} | r_i \epsilon R_N }$ ，其中 $R_N$ 是预先定义好的关系集合，为标签的语义库 $T$ 和一个包含 $k$ 个新关系 $r$ 实例的种子集合 $S_r$ 组成。我们首先在 $S_N$ 基础上训练神经网络，然后对于新的关系 $r$ ，我们训练一个二分类器 $g$ 。更具体一点，给一个实例 $x$ ，$g(x)$ 输出 $x$  表示关系 $r$ 的概率 ，在试验阶段，分类器 $g$ 对属于 $R_N$ 预先定义好关系的实例，包含新关系 $r$ 的实例以及一些包含隐藏关系的实例进行分类，这相当于现实场景的模拟。 </p>
<h3 id="Neural-Snowball-Process"><a href="#Neural-Snowball-Process" class="headerlink" title="Neural Snowball Process"></a>Neural Snowball Process</h3><p>&emsp;&emsp;以一个小的种子集合  $S_r$ 作为输入， <strong>Neural Snowball</strong>对新关系 $r$ 不断地累积可信赖的实例。每轮迭代中，$S_r$ 都会被选择的未标签实例进行扩充，然后新的 $S_r$ 作为下一轮的输入。</p>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/KG-paper-001-02.PNG" alt=""></p>
<p><strong>&emsp;&emsp;输入：</strong>关于关系的种子实例集合 $S_r$</p>
<p><strong>&emsp;&emsp;第一阶段：</strong>结构化实体对<br>$$<br>\varepsilon = {(e_h,e_t)\space|\space Ent(x) = (e_h,e_t),\space x\space \in \space S_r} \tag1<br>$$<br>&emsp;&emsp;其中 $Ent(x)$ 表示实例 $x$ 的实体对，然后我们从语义库 $T$ 里面得到候选集合 $C1$ ，其满足<br>$$<br>C_1 = {X\space | \space Ent(x)\space \in \space \varepsilon， x \in T} \tag2<br>$$<br>&emsp;&emsp;因为  $C_1$ 的实例的实体对和 $S_r$ 中一样，我们认为其之间很有可能是关系 $r$ 。为了进一步缓解错误实例的影响，对于每个在  $C_1$ 中的实例   $x$ ，我们将其和 $x’\in S_r$ 中具有相同是实数对的实例匹配，利用RSN去获得相似的分数，然后求其分数的均值作为其评估分 $score_1(x)$  。</p>
<p>&emsp;&emsp;接下里我们对 $C_1$ 里的实例按照评估分进行降序排序，选取前 $K_1$ 名加入到集合 $S_r$ 中去。因为在$K_1$ 中有些不属于关系 $r$  这种情况存在，我们添加了额外的条件，对于实例评估分数少于某个门槛值  $\alpha$  的实例将不会加入集合 $S_r$ 中去。</p>
<p>&emsp;&emsp;在这些步骤之后，我们获取了具有高可信度的关于关系 $r$  的新实例。伴随着集合 $S_r$ 的扩增，我们可以微调接下来将讲解的关系分类器 $g$ ，因为分类器在下一步骤中将使用。</p>
<p><strong>&emsp;&emsp;第二阶段：</strong>经过上一步骤，我们扩增了集合 $S_r$ ，然而实体对依旧保存没变。所以这个阶段，我们的目标是<strong>去发现关于关系 $r$  的新的实体对</strong>。</p>
<p>&emsp;&emsp;在该阶段构使用关系分类器 $g$  造候选实体集合 $C_2$，<br>$$<br>C_2 = {x\space | \space g(x)\space &gt; \space \theta， x \in T} \tag3<br>$$<br> &emsp;&emsp; 其中 $\theta$ 是可信度的阈值 。每一个候选实例 $x$ 和 每一个$x’\in S_r$ 作为RSN的输入，$score_2(x)$ 作为其可信度的评价函数，其是与所有实体$x’$ 匹配的均值 ，对于排序后的前 $K_2$ 名同时$score_2(x)$ 大于阈值 $\beta$ 的加入到集合 $S_r$ 中去。</p>
<p>&emsp;&emsp;经过这个阶段后，我们回到第一阶段重新开始。伴随着过程进行，实例集合 $S_r$ 不断扩增，而且分类器的性能会一直到达其顶峰。关于其最佳的迭代次数以及参数将在下面进行讨论。</p>
<h3 id="Neural-Modules"><a href="#Neural-Modules" class="headerlink" title="Neural Modules"></a>Neural Modules</h3><p><strong>&emsp;&emsp;Neural Snowball</strong>包含两个关键部分：一个是从未标签数据中心，通过评估候选实例与存在的关系实例之间相似性来选择高可信度的实例，另一个是判断实例是否属于新关系的分类器。</p>
<p><strong>&emsp;&emsp;Relational Siamese Network (RSN)</strong> $s(x,y)$ ，其输入为两个实例，输出在0到1之间，用来表示其实例之间是否具有相同的关系类型。图3展示了RSN的结构，其包含两个具有相同参数的编码器 $f_s$ 和一个距离评估函数。</p>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/KG-paper-001-03.PNG" alt=""></p>
<p>&emsp;&emsp;随着两个实例输入，编码器将其表示为向量，然后我们用下面的公式计算实例之间的相似分数：<br>$$<br>s(x,y) = \sigma(w^T_s(f_s(x)-f_s(y))^2+b_s) \tag4<br>$$<br>&emsp;&emsp;其中的平方符号是对向量每一维度进行平方，而不是向量的点乘，而 $\sigma(\cdot)$ 为激活函数。距离评估函数是带权重的L2范数距离，由可训练的权重 $w_s$ 和偏差参数 $b_s$ 组成。一个高的距离评估分数说明两个实例很有可能关系是同一类型（其中权重 $w_s$ 取负值可以达到这种效果）。</p>
<p><strong>&emsp;&emsp;Relation Classifier $g(x)$</strong>，其有一个神经编码器 $f$ 组成，其作用是将原始数据转换为实值向量，一个由参数$w$和$b$组成的线性层来判断输入实例是否属于关系 $r$ ，如下所表示：<br>$$<br>g(x)=\sigma(w^Tf(x)+b) \tag5<br>$$<br>&emsp;&emsp;其中$g(x)$ 输出类别可能性，而 $\sigma(\cdot)$ 激活函数将结果限制在0到1之间。注意函数 $g(x)$ 只是一个二分类器，所以其只输出一个值，而不是一个N类别情景下的一个向量。</p>
<p>&emsp;&emsp;选择二分类器的而不选择训练N分类器或者使用softmax去限制输出的原因是现实世界中的关系抽取需要处理那些负样本，那些表示这未知的关系而且占语义集很大一部分。这些负样本的表示不能被类聚，而且将其考虑为一种类别也不合适。另一个原因是通过二分类器，我们可以通过增加性分类器来应对新的关系的发生情况。而对于N分类器来说则需要重新训练，而且数据的不平衡可能对新关系和已有的关系导致更差的结果。</p>
<p>&emsp;&emsp;使用N个二分类器，我们可以将其结果与N个分类器比较判断，选取最高概率，其结果和N分类器一样，当没有结果超过阈值的时候，这个句子我们认为为负样本，其表示的关系不存在于已有的关系中。</p>
<p><strong>&emsp;&emsp;Pre-training and Fine-tuning</strong>，为了衡量实例之间在新关系的相似度以及快速使分类器适应新的任务，我们需要提前训练两个神经网络。通过存在的标签数据 $S_N$ ，我们可以通过监督学习N分类方法去预先训练分类器的隐藏层参数，对于RSN，我们可以从 $S_N$ 中随机选取包含相同关系或者不同关系的样例，通过<a href="https://zhuanlan.zhihu.com/p/35709485" target="_blank" rel="noopener">交叉熵</a>损失函数来训练模型。</p>
<p>&emsp;&emsp;当给与一个新关系 $r$ 的集合  $S_r$，对整个RSN以及关系分类器的编码器的参数将固定，因为他们已经通过预先的训练学到了一般性的特征，后续的通过一小部分数据进行的调整可能会对参数的分布带来噪音和偏差。</p>
<p>&emsp;&emsp;然后我们通过从 $S_r$中选取小批量样本作为正样本，从 $S_N$中选取的为负样本对分类器参数$w$和$b$进行优化，将正样本表示为$S_b$，负样本表示为$T_b$ ，损失函数为如下：<br>$$<br>L_{S_b,T_b}(g_{w,b}) = \sum_{x\in S_b}log \space g_{w,b}(x)\space +\space \mu \sum_{x\in T_b}log \space(1- g_{w,b}(x)) \tag6<br>$$<br>&emsp;&emsp;其中$\mu$是负样本系数，所以我们对每一组正负样本可以选取固定的数目大小，实际在新关系中正样本与负样本的数量相差很大，所以很有必要将负样本系数设置的更小一些。</p>
<p>&emsp;&emsp;基于上述的样本选择策略以及损失函数，我们可以基于梯度对参数$w$和$b$进行优化，这里我们选择Adam (Kingma and Ba 2015)作为我们的优化器，那些超参数包含训练周期数$e$，分批次的大小$b_s$吗，学习率$\lambda$，以及负样本的损失系数$\mu$。算法1表述该过程<img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/KG-paper-001-04.PNG" alt=""></p>
<p>&emsp;&emsp;微调过程做我们baseline一部分，我们在对$S_r$累积了新的实例后，我们在<strong>Neural Snowball</strong>中的每一步都采用了该算法，虽然这是一种简单获取参数$w$和$b$的方式，但是他比基于度量的小样本算法更好，因为它对于新的关系的适应性更好，而基于度量的通常在小样本基础上获取所有参数，相比较它对于大范围的训练数据更具扩展性。同事负样本能够改善模型对新关系抽取的精度。</p>
<h3 id="Neural-Encoders"><a href="#Neural-Encoders" class="headerlink" title="Neural Encoders"></a>Neural Encoders</h3><p>&emsp;&emsp;正如上述的，编码器使我们RSN和分类器的部分，目的是从标记的实体和原始句子中提取抽象的一般性特征。这份报告中我们采用了CNN和BERT编码器。</p>
<p>&emsp;&emsp;CNN的模型我们采用了Nguyen and Grishman(2015)的CNN编码器模型，这个模型采用词嵌入和位子嵌入作为输入，然后嵌入序列被输入到一个卷积神经网络中去提取特征，然后这些特征被最大池化为一个实值向量作为实例的表示。</p>
<p>&emsp;&emsp;BERT由Devlin等人在2019年提出的一个新颖的语言模型，是<strong>Bidirectional Encoder Representations from Transformers</strong>的简称，相比较于CNN或者RNN模型，在NLP相关任务中去得新的成就。BERT将句子的符号作为输入，经过几个注意层后输出隐藏的特征。为了适应关系抽取任务，我们在句子的前后端添加了特殊的标记。前后的标记是不同的。然后我们取隐藏特征的第一个符号作为句子的表示。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>&emsp;&emsp;这部分我们将展示关系分类器怎么通过<strong>Neural Snowball</strong>机制来实现较大的性能提升，其是相比较于小样本关系学习的baseline。我们还进行了两次定量分析进一步证明<strong>RSN</strong>和<strong>Snowball</strong>的有效性。</p>
<h3 id="Datasets-and-Evaluation-Settings"><a href="#Datasets-and-Evaluation-Settings" class="headerlink" title="Datasets and Evaluation Settings"></a>Datasets and Evaluation Settings</h3><p>&emsp;&emsp;我们的实验要求一个大量的，精确人为注释的，同时方便远程监督学习的数据集。目前满足条件的数据是FewRel (Han et al. 2018），这个包含来自Wikipedia的100中关系和7000个实例。这个数据集被分为一下三个子集。训练数据集包含64个关系，验证数据集包含16个关系，以及测试数据集包含20个关系。我们还从Wikipedia下载了大量未标签包含实体标签的语义集，其中实例899996个，以及464218对实体对，这些是用来训练snowball过程的。</p>
<p>&emsp;&emsp;我们实验采用原先相关节的设置。首先我们将训练数据分为集合$A$和$B$，我们经集合$A$作为$S_N$，在验证集或者测试集的中抽取一张关系作为新的关系$r$，取其前k个实例作为$S_r$，从集合$B$和测试或者验证集中抽取一个查询集合$Q$，然后模型对查询实例进行二分类，判断每一个实例是否是描述的关系$r$。注意查询集合$Q$包含训练数据集的N种关系，关系$r$的少数实例以及大量的位置关系。这是一种非常具有挑战性的设置，相比较于N-way K-shot few-shot来说更接近于现实世界情况，因为在现实中的语义库不会局限于有限的关系。</p>
<h3 id="Parameter-Settings"><a href="#Parameter-Settings" class="headerlink" title="Parameter Settings"></a>Parameter Settings</h3><p>&emsp;&emsp;我们在验证集上调整超参数。对于于编码器的参数，我们遵循Han等人（2018）的CNN和Devlin等人（2019）年的BERT。对于微调，在网格搜索之后，我们采用训练周期e=50，批处理尺寸bs=10，学习率为0.05，负损失系数为0.2。BERT微调共享相同的参数,除了学习率0.01和负损失系数为0,5。</p>
<p>&emsp;&emsp;对于Neural Snowball过程，我们依旧还是通过网格搜索方法进行确认，我们设置$K_1$和$K_2$，每次增加的实例树为5，RSN的阈值为0.5，分类器阈值为0.9。</p>
<p>&emsp;&emsp;在我们实验中，所有模型的评估都是对查询实例输出对于某种关系的可能性描述，为了获得一个可信的结果，我们设置一个阈值，对于fine-tuning和Neural Snowball我们设置为0.5，而对于RSN我们设置为0.7。</p>
<h3 id="Few-Shot-Relation-Learning"><a href="#Few-Shot-Relation-Learning" class="headerlink" title="Few-Shot Relation Learning"></a>Few-Shot Relation Learning</h3><p>&emsp;&emsp;Table 1展示了小样本学习任务的结果。</p>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/KG-paper-001-05.PNG" alt=""></p>
<hr>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/KG-paper-001-06.PNG" alt=""></p>
<p>&emsp;&emsp;我们评估了五种模型结构。BREDS是原始snowball的改进版本，其实使用词嵌入来进行模式选择。Fine-tuning是直接使用算法1对少样本实例进行训练分类器。Relational Siamese Network (RSN)通过计算查询实例和每个$S_r$中的实例的相似度，然后去其平均值作为结果。Distant Supervision是指用所有实例共享种子实例的实体对信息用来进行训练，也是使用算法1。Neural Snowball是我们提出的方法，我们没有评估其他半监督方法和样本学习方法，因为不适宜我们对新关系学习的设置。</p>
<p>&emsp;&emsp;从表1中我们可以确认，我们的Neural Snowball在两个集合中以及编码器中结果都较好。然而fine-tuning, distant supervision和Neural Snowball随着种子数目增多而性能提升，而BREDS和RSN则提升较小。</p>
<p>&emsp;&emsp;进一步将Neural Snowball与其他模型比较，我们模型具有较好的召回率，同时获取比较高的准确率。这表明我们的Neural Snowball不仅可以能累积高质量的训练实例，而且也可以比较成功地抽取新关系事实以及模式来扩增对新关系的实例。</p>
<h3 id="Analysis-on-Relational-Siamese-Network"><a href="#Analysis-on-Relational-Siamese-Network" class="headerlink" title="Analysis on Relational Siamese Network"></a>Analysis on Relational Siamese Network</h3><p>&emsp;&emsp;为了评估RSN选取的实例的质量，我们随机化取样一种关系以及5个相应的实例，将剩下的实例作为查询集合。我们使用前面提及的方法去评估计算分数，然后我们计算前N个实例的准确率。</p>
<p>&emsp;&emsp;我们可以看到，RSN在5%的实例下获得了82.15%的准确率。考虑这个值比较高我们只给少数量的实例，甚至一些没见过的关系。同时注意到，RSN只训练训练集里面关系，对于测试集里面的关系的表现只有很小的差距，进一步证实了RSN的有效性。</p>
<h3 id="Analysis-on-Neural-Snowball-Process"><a href="#Analysis-on-Neural-Snowball-Process" class="headerlink" title="Analysis on Neural Snowball Process"></a>Analysis on Neural Snowball Process</h3><p>&emsp;&emsp;进一步分析Neural Snowball的迭代过程，我们对新累积的实例和分类器给出了主关系上定量分析，是以五个种子实例为增量的。注意到它是随机选择关系和其他关系展示了相关性。由于空间有限，我们只选择了主关系进行样例。</p>
<p>&emsp;&emsp;图4展示了伴随迭代过程中评估函数的变化过程。这里我们采用两种设置：NS setting是关于由Neural Snowball选择实例的分类器微调设置，另一个random setting是指在知道实例所有关系前提下，对随机选取的和NS一样数目的主关系实例的微调设置。注意到随机设置以一种理想情况，因为其反应了新关系随机分布的真实情况，而且将随机设置的整体表现作为上限。</p>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/KG-paper-001-07.PNG" alt=""></p>
<p>&emsp;&emsp;从随机设置的结果来看，我们可以看到二分类器获得了较高的召回率以及稍逊的准确率，在训练大范围的随机分布的数据下。这个由更多的数据带来更多的在表达方面的模式，改善了抽取的完全性虽然牺牲了部分质量。</p>
<p>&emsp;&emsp;然后再比较两者设置，我们得到两个观测结果。1.随着迭代次数的增加以及实例数目的增加，分类器在NS上的微调获得了更高的准确率，这个证实了RSN能在抽取高可信度实例以及高质量模式上的性能较好。2.NS的召回率比预期增长较慢，表明RSN可能对于存在的模式过拟合了。为了使模型的具有高准确率，NS陷入了已有高质量模式的局部最优区，从而无法跳出这个区域去发现更多种类的模式。我们打算未来研究这个。</p>
<h2 id="Conclusion-and-Future-Work"><a href="#Conclusion-and-Future-Work" class="headerlink" title="Conclusion and Future Work"></a>Conclusion and Future Work</h2><p>&emsp;&emsp;这篇论文中，我们提出了Neural Snowball，一种新颖的方法，以小样本数据去学习分类新关系。我们使用RSN，它预先基于历史数据去迭代从未标签数据中抽取可信赖的实例。大规模的关系抽取评估解释了Neural Snowball在使用少样例基础上，抽取新关系中带来了显著的性能提升。进一步分析证实了RSN以及snowball process的有效性。未来我们将探究一下方向：</p>
<p>（1）我们模型的缺点是抽取的模式与所给的语义很相近，这个限制了召回率。在未来，我们将探索怎样跳出局部最优区去发现更多实例种类。</p>
<p>（2）目前RSN在新关系学习中参数被固定了，如果能利用好新给的关系实例，使RSN更具适应性将更好。我们将研究这个并提升其RSN的性能。</p>
<h2 id="Acknowledgments"><a href="#Acknowledgments" class="headerlink" title="Acknowledgments"></a>Acknowledgments</h2><p>&emsp;&emsp;这项工作得到了国家自然科学基金的支持，中国基金（国家自然科学基金第6157227361661146007号，<br>61772302）和清华大学研究基金-腾讯互联网创新技术联合实验室。Han和Gao得到2018年和2019年腾讯的支持犀鸟精英训练计划。Gao也是清华大学自主科研资助项目。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2>]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>NLP</tag>
        <tag>知识抽取ER</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记003-Entity Alignment between Knowledge Graphs Using Attribute Embeddings</title>
    <url>/2020/07/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0003-Entity%20Alignment%20between%20Knowledge%20Graphs%20Using%20Attribute%20Embeddings/</url>
    <content><![CDATA[<center><font face="微软雅黑" size=4>《Entity Alignment between Knowledge Graphs Using Attribute Embeddings》</font></center>

<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><blockquote>
<p>题目：《Entity Alignment between Knowledge Graphs Using Attribute Embeddings》   </p>
<p>来源：AAAI-2019    </p>
<p>链接：<a href="https://people.eng.unimelb.edu.au/jianzhongq/papers/AAAI2019_EntityAlignment.pdf" target="_blank" rel="noopener">原文地址</a> </p>
<p>代码：<a href="https://bitbucket.org/bayudt/kba/src/master/" target="_blank" rel="noopener">Code和Dataset</a>   </p>
</blockquote>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>&emsp;&emsp;早期的实体对齐研究<strong>基于属性之间的相似性</strong>，依赖于<strong>用户定义的规则来确定实体之间需要比较的属性</strong>。由于<strong>不同实体之间可能需要不同的属性</strong>来进行比较，所以这种方法容易出现误差。</p>
<p>&emsp;&emsp;最近，针对实体对齐任务提出了<strong>基于嵌入的模型</strong>，它要求将两个KG嵌入到同一个向量空间中，以适应KG嵌入在两个KG之间的实体对齐。但该方法<strong>需要大量的种子实体</strong>，这在现实使用中难以获取。</p>
<p>本文针对上述的局限性，提出了一种新的嵌入模型，本文的主要贡献如下：</p>
<ul>
<li>提出两个KG之间的实体对齐框架，由<strong>谓词对齐模块(predicate alignment module)</strong>、<strong>嵌入学习模块(embedding learning module)</strong>、<strong>实体对齐模块(entity alignment module)</strong>组成</li>
<li>提出一种新的嵌入模型，将实体嵌入和属性嵌入集成在一起，用来学习对于两个KGs的统一嵌入空间</li>
<li>在三对KGs上对模型进行评估，就<a href="https://blog.csdn.net/hello_acm/article/details/95070669" target="_blank" rel="noopener">hits@1</a>指数而言，模型优于现有模型50%以上</li>
</ul>
<a id="more"></a>


<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p> &emsp;&emsp; 一个知识图谱 $G$ 由三元组$&lt;h,r,t&gt;$组成，对于  $G_1和G_2$，实体对齐的任务是寻找实体对，其中$&lt;h_1,h_2&gt;$，其中$h_1\in G_1,h_2 \in G_2$  。其模型主要由三个部分组成：<img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/003/KG-paper-003-01.PNG" alt=""></p>
<h3 id="谓词对齐-predicate-alignment"><a href="#谓词对齐-predicate-alignment" class="headerlink" title="谓词对齐(predicate alignment)"></a>谓词对齐(predicate alignment)</h3><p>&emsp;&emsp;谓词对齐模块通过使用统一的命名方案重命名两个KG的谓词来合并两个KG，以便为关系嵌入提供统一的向量空间。除了<strong>符合命名规范</strong>的谓词，如：rdfs:label、geo:wgs84 pos#lat等；还有一些相互匹配的谓词，例如：dbp:bornIn 和yago:wasBornIn 我们就需要统一命名，比如将dbp:bornIn和yago:wasBornIn统一为 :bornIn。</p>
<p>为了找到部分匹配的谓词，作者计算谓词URI的最后部分的<a href="https://www.jianshu.com/p/a617d20162cf" target="_blank" rel="noopener">编辑距离</a>（例如，bornIn与wasBornIn）并将0.95设置为相似性阈值。</p>
<h3 id="嵌入学习-Embedding-Learning"><a href="#嵌入学习-Embedding-Learning" class="headerlink" title="嵌入学习(Embedding Learning)"></a>嵌入学习(Embedding Learning)</h3><h4 id="结构嵌入-Structure-Embedding"><a href="#结构嵌入-Structure-Embedding" class="headerlink" title="结构嵌入(Structure Embedding)"></a>结构嵌入(Structure Embedding)</h4><p>&emsp;&emsp;作者采用<a href="https://zhuanlan.zhihu.com/p/32993044" target="_blank" rel="noopener">TransE</a>来学习对于实体的结构嵌入。与<a href="https://zhuanlan.zhihu.com/p/32993044" target="_blank" rel="noopener">TransE</a>不同的是，模型希望更关注已对齐的三元组，也就是包含<strong>对齐谓词</strong>的三元组。模型通过添加权重来实现这一目的。Structure embedding的目标函数 $J_{SE}$ 如下：<br>$$<br>J_{SE}=\sum_{t_r \in T_r} \sum_{t_r^\prime \in T_r^\prime }max(0 \ , \ \gamma + \alpha(f(t_r)-f(t^\prime_r)))  \<br>  \alpha = \frac{count(r)}{|T|}<br>$$<br>&emsp;&emsp;其中$t_r$是有效关系的三元组集合，而$t_r^\prime$是通过随机替换三元组$&lt;h,r,t&gt;$中实体$h$或者$r$形成的损坏样本，即负样本。$T$为合并后的$G_{1_2}$中三元组总数目，$count(r)$为关系$r$出现的次数，$\alpha$为权重，对于对齐的谓词关系，正常情况下，由于在两个KGs中都包含，其占的比例高一些，也就是更希望模型学习对齐的关系。</p>
<h4 id="属性字符嵌入-Attribute-Character-Embedding"><a href="#属性字符嵌入-Attribute-Character-Embedding" class="headerlink" title="属性字符嵌入(Attribute Character Embedding)"></a>属性字符嵌入(Attribute Character Embedding)</h4><p>&emsp;&emsp;对于属性字符嵌入，也参考<a href="https://zhuanlan.zhihu.com/p/32993044" target="_blank" rel="noopener">TransE</a>的思想，将谓词 $r$ 解释为从头部实体 $h$ 到属性 $a$ 的转换。但是，相同的属性$a$可以在两个KG中以不同的形式出现，例如50.9989对50.9988888889作为实体的纬度；“Barack Obama”与“Barack Hussein Obama”作为人名等。因此，本文提出使用组合函数对属性值进行编码，并将属性三元组中每个元素的关系定义为$h+r=f_a(a)$。 其中$f_a(a)$是组合函数，$a$是属性值$a={c_1,c_2,\cdots,c_t}$的字符序列。 组合函数将属性值编码为单个向量，并将类似的属性值映射到类似的向量表示。 作者定义了三个组成函数如下：</p>
<h5 id="Sum-compositional-function-SUM"><a href="#Sum-compositional-function-SUM" class="headerlink" title="Sum compositional function (SUM)"></a>Sum compositional function (SUM)</h5><p>&emsp;&emsp;思路：简单的求属性值的所有字符的嵌入总和</p>
<p>&emsp;&emsp;函数：$f_a(a)\ = c_1+c_2+ \dots +c_t$</p>
<p>&emsp;&emsp;问题：包含相同字符不同顺序的属性值会有相同的向量表示，如50.15和15.05将具有相同的向量表示</p>
<h5 id="LSTM-based-compositional-function-LSTM"><a href="#LSTM-based-compositional-function-LSTM" class="headerlink" title="LSTM-based compositional function (LSTM)"></a>LSTM-based compositional function (LSTM)</h5><p>&emsp;&emsp;思路：使用LSTM模型将一个字符序列编码为一个向量，将最终隐藏状态作为属性值的向量表示</p>
<p>&emsp;&emsp;函数：$f_a(a)\ = f_{lstm}(c_1,c_2,\dots,c_t)$</p>
<h5 id="N-gram-based-compositional-function-N-gram"><a href="#N-gram-based-compositional-function-N-gram" class="headerlink" title="N-gram-based compositional function (N-gram)"></a>N-gram-based compositional function (<a href="https://blog.csdn.net/songbinxu/article/details/80209197" target="_blank" rel="noopener">N-gram</a>)</h5><p>&emsp;&emsp;思路：备选方案，使用属性值的n-gram组合的总和</p>
<p>&emsp;&emsp;函数：$f_a(a)\ = \ \sum^N_{n=1}(\frac{\sum^t_{i=1}\sum^n_{j=1}c_j}{t-i-1})$</p>
<p>&emsp;&emsp;其中N表示n-gram组合中使用的N的最大值N=10，t为属性值的长度，其最小化目标函数$J_{CE}$为：<br>$$<br>J_{CE}=\sum_{t_a \in T_a} \sum_{t_a^\prime \in T_a^\prime }max(0 \ , \ \gamma + \alpha(f(t_a)-f(t^\prime_a))) \<br>T_a \ = \ {&lt;h^\prime,r,a&gt; \in G } \<br>f(t_a) \ = \ ||h \ + \ r-f_a(a)|| \<br>T_a^\prime \ = \ {&lt;h^\prime,r,a&gt;|h^\prime \in E \ } \cup \ {&lt;h,r,a^\prime&gt;|a^\prime \in A \ }<br>$$</p>
<h4 id="结构嵌入和属性特征嵌入的联合学习-Joint-Learning-of-Structure-Embedding-and-Attribute-Character-Embedding"><a href="#结构嵌入和属性特征嵌入的联合学习-Joint-Learning-of-Structure-Embedding-and-Attribute-Character-Embedding" class="headerlink" title="结构嵌入和属性特征嵌入的联合学习(Joint Learning of Structure Embedding and Attribute Character Embedding)"></a>结构嵌入和属性特征嵌入的联合学习(Joint Learning of Structure Embedding and Attribute Character Embedding)</h4><p>&emsp;&emsp;通过属性字符嵌入 $h_{ce}$ 来帮助结构嵌入 $h_{se}$ 在同一向量空间进行训练，最小目标函数$J_{SIM}$为：<br>$$<br>J_{SIM} \ = \ \sum_{h  \in G_1 \cup G_2}[1-\cos(h_{ce},h_{se})]<br>$$<br>&emsp;&emsp;结构嵌入基于实体关系获取实体之间的相似度，属性字符嵌入基于属性值获取实体之间的相似度。结构嵌入和属性特征嵌入联合学习的总体目标函数$J$为:<br>$$<br>J = J_{SE}+J_{CE}+J_{SIM}<br>$$</p>
<h3 id="实体对齐-Entity-Alignment"><a href="#实体对齐-Entity-Alignment" class="headerlink" title="实体对齐(Entity Alignment)"></a>实体对齐(Entity Alignment)</h3><p>&emsp;&emsp;在经过上述训练过程之后，来自不同KG的相似的实体将会有相似的向量表示，因此可通过下面公式确定潜在的需要对齐的实体：<br>$$<br>h_{map}= \arg\max \limits_{h_2 \in G_2} \cos(h_1,h_2)<br>$$<br>&emsp;&emsp;给定实体$h_1$，计算它与$G_2$中的每个实体时间的相似性，最后$&lt;h_1,h_{maxp}&gt;$就是对齐的实体对，同时其使用了$\beta$来过滤不对齐的实体。</p>
<h3 id="Triple-Enrichment-via-Transitivity-Rule"><a href="#Triple-Enrichment-via-Transitivity-Rule" class="headerlink" title="Triple Enrichment via Transitivity Rule"></a>Triple Enrichment via Transitivity Rule</h3><p>&emsp;&emsp;这里使用了一个小 $trick$ ,通过传递关系来丰富三元组。</p>
<h2 id="Experiments-and-Results"><a href="#Experiments-and-Results" class="headerlink" title="Experiments and Results"></a>Experiments and Results</h2><p>&emsp;&emsp;本文从 DBpedia (DBP)、LinkedGeoData (LGD)、Geonames (GEO) 和 YAGO 四个 KG 中抽取构建了三个数据集，分别是DBP-LGD、DBP-GEO和DBP-YAGO。具体的数据统计如下：<img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/003/KG-paper-003-02.PNG" alt=""></p>
<p>&emsp;&emsp;作者使用hits@k(k=1,10)来评估模型性能，对比了TransE、MTransE、JAPE三种模型，使用30%的对齐实体作为MTransE、JAPE的预对齐种子。在三种组合函数中，N-gram函数的优势较为明显。此外，基于传递规则的三元组丰富模型对结果也有一定的提升。具体结果如下：</p>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/003/KG-paper-003-03.PNG" alt=""></p>
<p>&emsp;&emsp;为了进一步衡量 attribute character embedding 捕获实体间相似信息的能力，本文设计了基于规则的实体对齐模型。本实验对比了三种不同的模型：以label的字符串相似度作为基础模型；针对数据集特点，在基础模型的基础之上增加了坐标属性（由于只包含LOCATION的实体），以此作为第二个模型；第三个模型是把本文提出的模型作为附加模型，与基础模型相结合。具体结果如下：</p>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/003/KG-paper-003-04.PNG" alt=""></p>
<p>&emsp;&emsp;本文还在KG补全任务上验证了模型的有效性。模型主要测试了链接预测和三元组分类两个标准任务，在这两个任务中，模型也取得了不错的效果。具体结果如下：</p>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/003/KG-paper-003-05.PNG" alt=""></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>&emsp;&emsp;作者的模型设计很巧妙，将大量的属性三元组充分利用起来，辅助去将不同KG的实体构建到统一向量空间。作者在其中设计很多技巧来优化结果，而且整体来说其将一个比较难的问题充分化解为几个小问题，并在每个小问题上提出创新思路，并最后能够将其完美的整合起来达到显著的性能提升。</p>
]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>NLP</tag>
        <tag>知识融合</tag>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记004-Knowledge Graph Alignment Network with Gated Multi-hop Neighborhood Aggregation</title>
    <url>/2020/08/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0004-Knowledge%20Graph%20Alignment%20Network%20with%20Gated%20Multi-hop%20Neighborhood%20Aggregation/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><blockquote>
<p>题目：《Knowledge Graph Alignment Network with Gated Multi-hop Neighborhood Aggregation》</p>
<p>来源：AAAI-2020</p>
<p>链接：<a href="https://aaai.org/ojs/index.php/AAAI/article/view/5354" target="_blank" rel="noopener">论文链接</a></p>
<p>代码：<a href="https://github.com/nju-websoft/AliNet" target="_blank" rel="noopener">Code和Dataset</a></p>
</blockquote>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>&emsp;&emsp;由于GNN网络在识别同构子图上表现非常不错，因此应用于很多基于嵌入式的实体对齐方法。但是实际上不同KGs之间对应的实体对，其<strong>周围的邻域图结构一般不是相似的</strong>，这导致GNN不能准确进行识别。如图1：</p>
<img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/004/KG-paper-004-01.PNG"  />

<p>&emsp;&emsp;上图中指向Kobe Bryant的两个实体的邻域是不一致的，邻居实体集合也是不同的（有数据显示大部分对齐的实体对都具有不同的邻域）。要对齐的实体具有不同的邻域结构，使用GNN就容易导致为这些实体生成不同的表示。</p>
<a id="more"></a>

<p>&emsp;&emsp;本文为解决这个问题，提出了<strong>AliNet</strong>，希望通过引入远距离的邻居来缓解子图的异构性，同时还引入了<strong>注意力机制</strong>来有选择地择取远距离邻居，进而减少噪音，然后引入了<strong>门机制</strong>来整合直连邻居和远距离邻居的信息，最后还设计了一个<strong>关系损失函数</strong>去改善实体表示同时让<strong>AliNet</strong>能够更好地捕获一些如三角形关系结构等的信息。</p>
<h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><h3 id="GNNs"><a href="#GNNs" class="headerlink" title="GNNs"></a>GNNs</h3><p>&emsp;&emsp;在GNNs中，节点的表示一般通过迭代邻接节点的特征向量来获取的，不同的聚合方式会到导致不同的GNNs变型。</p>
<ul>
<li><strong>GCN</strong>：其中比较受欢迎的一种变型就是<strong>vanilla GCN</strong>(Kipf and Welling 2017)，其节点i的第l层表示如下：</li>
</ul>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/004/KG-paper-004-02.PNG" alt=""></p>
<ul>
<li><strong>R-GCN</strong>：<strong>Conventional GNNs</strong>只考虑节点的连接性方面，而忽略了边的标签性质。<strong>R-GCN (Schlichtkrull et al. 2018)</strong>依靠对于不同的边设置不同的权值来解决这个问题。</li>
</ul>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/004/KG-paper-004-03.PNG" alt=""></p>
<h3 id="Entity-Alignment-of-KGs"><a href="#Entity-Alignment-of-KGs" class="headerlink" title="Entity Alignment of KGs"></a>Entity Alignment of KGs</h3><p>&emsp;&emsp;将给定的KG记为${G} = ( \mathcal {E} , \mathcal {R} , \mathcal {T} )$，三个元素分别表示实体集合、关系集合以及三元组集合，其中$\mathcal {T} = \mathcal {E} \times \mathcal {R} \times \mathcal {E} $。针对两个KG间的实体对齐问题，给定两个KG：$\mathcal {G}_1 = ( \mathcal { E }_1, \mathcal {R}_1, \mathcal {T}_1 )$和$\mathcal {G}_2 = ( \mathcal {E}_2, \mathcal {R}_2, \mathcal {T}_2 )$，然后给定部分预对齐的种子实体对：$\mathcal{A }^{ + } = { { (i, j) \in \mathcal { E}_1 \times \mathcal {E}_2 | i \equiv j} }$，其中$ \equiv $表示对齐关系。这一任务的目的是通过实体的嵌入，找出剩余对齐的实体对。</p>
<h3 id="GNNs-for-Entity-Alignment"><a href="#GNNs-for-Entity-Alignment" class="headerlink" title="GNNs for Entity Alignment"></a>GNNs for Entity Alignment</h3><p>&emsp;&emsp;目前基于GNN的实体对齐方法有：GCNAlign(Wang et al. 2018)、GMNN (Xu et al. 2019b)、 MuGNN(Cao et al. 2019)、RDGCN (Wu et al. 2019) 和AVR-GCN(Ye et al. 2019)。GCN-Align和GMNN是基于vanilla GCN设计的；RDGCN引入了对偶的关系图以增强vanilla GCN；AVG-GCN使用类似TransE的特定关系的translation操作，对R-GCN进行了扩展。在聚合之前，使用关系向量从尾实体translate成头实体的表示。作者认为<strong>针对特定关系的translation和R-GCN需要训练的参数量太大</strong>，计算成本高。而且<strong>上述模型均没有考虑到KG结构的异构问题</strong>。MuGNN模型注意到结构的不完整性，提出了基于规则的知识补全和多通道实体对齐的两种方法以缓解模式的异质性问题，然而其<strong>学习到的规则依赖于关系对齐</strong>。</p>
<p>&emsp;&emsp;以上分析都是针对已有的模型的总结，接下来讲解一些图特征对实体对齐的作用。<img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/004/KG-paper-004-04.PNG" alt=""></p>
<ul>
<li><strong>Isomorphic structures are beneficial</strong></li>
</ul>
<p>&emsp;&emsp;对于同构结构的图，在已知预先对齐的邻接节点的情况下，其相应的节点的表示是相同的，那么我们分析考虑一种自由一层的GCN网络，采用mean-pooling方式进行聚集，在上图中的（i）情况下，例如：$h_a^{(0)}=h_{a^{‘}}^{(0)}$，$h_b^{(0)} =h_{b^{‘}}^{(0)}$，$h_d^{(0)}=h_{d^{‘} }^{(0)}$，在理想情况下也有$\mathbf{h_a^{(1)}=h_{a^{‘}}^{(1)}}$，$\mathbf{h_b^{(1)} =h_{b^{‘}}^{(1)}}$，$\mathbf{h_d^{(1)}=h_{d^{‘}}^{(1)}}$。通过$\mathbf{h_b^{(1)}=\sigma(W^{(1)}} (\mathbf{h_b^{(0)}+h_a^{(0)}}+\mathbf{h_c^{(0)})/3)}$和$\mathbf{h_{b^{‘}}^{(1)}} =\sigma(\mathbf{W^{(1)}(h_{b^{‘}}^{(0)}}+\mathbf{h_{a^{‘}}^{(0)}+h_{c^{‘}}^{(0)})/3)}$得到$\mathbf{h_c^{(0)}=h_{c^{‘}}^{(0)}}$。这就表明给定部分对齐的邻居的时候，实体间的对齐信息可以在不同的GNN层中以及同构网络中传播。实际上这种理想情况很少，大多数情况下网络为异构网络如（ii）所示。</p>
<ul>
<li><strong>Only structures are not enough</strong></li>
</ul>
<p>&emsp;&emsp;传统的GNN仅依赖于结构是不可靠的。比如一些三角形结构，如图（iii）中所示，通过聚合以后，会导致a=b的情况，实际上实体a和实体b是不同的实体。后续的R-GCN虽然考虑了关系，但是依赖于关系对齐这个前提，如果两个KGs中关系没有提前对齐则会导致实体对齐的信息不能顺序的传播。</p>
<ul>
<li><strong>Compensation with distant neighborhood and relations</strong></li>
</ul>
<p>&emsp;&emsp;实际中，对齐实体对的邻居结构通常会是远近的混合形式，如图（iv）所示，为了减少异构带来的干扰，作者将远距离邻居考虑进去，相当于丰富了要对齐的实体周围的子图结构。但是不是所有邻近节点都是有用的，所以引入了注意力机制进行有所选择。为了进一步加强<strong>AliNet</strong>的表现，在不引入关系向量的情况下考虑关系的语义信息。</p>
<h2 id="模型分析"><a href="#模型分析" class="headerlink" title="模型分析"></a>模型分析</h2><p>&emsp;&emsp;在<strong>AliNet</strong>中，通过门机制来聚合K近邻的相关信息（one-hop为直连的，two-hop为间隔一个的），下本将以k=2举例来讲解，其k可以取更大的值。整体网络如下图：<img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/004/KG-paper-004-05.PNG" alt=""></p>
<h3 id="Gated-Multi-hop-Neighborhood-Aggregation"><a href="#Gated-Multi-hop-Neighborhood-Aggregation" class="headerlink" title="Gated Multi-hop Neighborhood Aggregation"></a>Gated Multi-hop Neighborhood Aggregation</h3><p>&emsp;&emsp;one-hop的节点，即直连的节点，对于GNNs网络中实体的的特征获取是非常重要的。作者对这些节点的表示，使用了<strong>vanilla GCN</strong>的层，其表示如上面方程式（1），表示为$\mathbf h_{i,1}^{(l)}$。虽然有L 层的GCN可以捕获L-hop的邻接信息，但是这的传播是低效的。</p>
<p>&emsp;&emsp;作者对于k=2的邻接点整合方程如下：<br>$$<br>\mathbf h_{i,2}^{(l)} \space = \space \sigma(\sum_{j\in N_2(i) \cup{i}}\alpha_{ij}^{(l)}W_2^{(l)}h_j^{ (l-1)}) \tag 3<br>$$<br>&emsp;&emsp;其中j为k=2的邻接节点，$\alpha_{ij}^{(l)}$ 是实体i和其邻接点j的一个可以学习的注意机制的权重，而 $W_2^{(l)}$ 是一个权重矩阵。受神经网络中<strong>跳跃连接</strong>（skip connections）的启发，作者提出使用<strong>门机制</strong>以<strong>结合1-hop邻居和2-hop邻居</strong>。实体i在第l层的隐层表示: $\mathbf{h_i^{(l)}}$计算如下：<br>$$<br>h_i^{(l)} = g(h_{i,2}^{ (l) } ) \cdot h_{i,1}^{(l)}+(1-g(h_{i,2}^{(l)}))\cdot h_{i,2}^{(l)}  \tag 4<br>$$<br>其中$g(h_{i,2}^{(l)}) = \sigma(Mh_{i,2}^{(l)}+b)$作为门机制去整合one-hop与two-hop的节点。</p>
<h3 id="Attention-for-Distant-Neighborhood"><a href="#Attention-for-Distant-Neighborhood" class="headerlink" title="Attention for Distant Neighborhood"></a>Attention for Distant Neighborhood</h3><p>&emsp;&emsp;远距离的邻接点的数目与其直连的节点的数目成指数关系。当然不是每个远距离节点都是有用的。对于k=2的聚合方式，我们得计算权重来强调有用的邻节点。<strong>GAT</strong>对每个注意力函数中的实体应用了一个<strong>共享的线性变换</strong>。然而，<strong>KG中的中心实体和其邻居可能是非常不同的</strong>，这样的<strong>共享变换会对区别它们带来负影响</strong>。因此作者使用了两个矩阵来进行线性变换，然后用softmax函数得到权重参数：<br>$$<br>c_{i j}^{(l)}=LeaklyReLU[ (M_1^{ (l) }h_i^{l})^T(M_2^{(l)}h_j^{l}) ] \tag 5<br>$$</p>
<p>$$<br>\alpha_{ij}^{(1)}=softmax_j(c_{ij}^{(1)})=\frac{exp(c_{ij}^{(1)})}{\sum_{n\in N_2(i) \cup {i}}exp(c_{ij}^{(l)})}  \tag 6<br>$$</p>
<h3 id="Contrastive-Alignment-Loss"><a href="#Contrastive-Alignment-Loss" class="headerlink" title="Contrastive Alignment Loss"></a>Contrastive Alignment Loss</h3><p>&emsp;&emsp;作者最小化<strong>contrastive alignment loss</strong>函数让对齐的实体对距离尽可能小，而不对应得到实体对距离尽可能大。<br>$$<br>L_{1} = \sum_{(i,j)\in A^{+}}||h_i-h_j|| + \sum_{(i^{‘},j^{‘})\in A^-} \alpha_1{\lbrack \lambda-\vert \vert h_i^{‘}-h_j^{‘} \vert\vert \rbrack_+}        \tag 7<br>$$<br>&emsp;&emsp;其中$A^{-}$由随机替换原先对齐的实体对的负样本集， $\lbrack\cdot\rbrack_{+}$ 表示 $max(0,\cdot)$ 。一般会用最后一层的结果表示实体，然后每一层的都传播了对齐的信息，因此我们使用所用的隐藏层的信息表示实体：<br>$$<br>h_i=\bigoplus_{l=1}^{L}norm(h_i^{ (l) } ) \tag  8<br>$$<br>&emsp;&emsp;其中 $\oplus$ 是并接操作， $norm(\cdot)$  是 $L_2$ 正则化操作。</p>
<h3 id="Relation-Semantics-Modeling"><a href="#Relation-Semantics-Modeling" class="headerlink" title="Relation Semantics Modeling"></a>Relation Semantics Modeling</h3><p>&emsp;&emsp;作者认为对于图中提供的语义关系，很本能地应该利用这些信息，上面讨论的R-GCN需要高度相似的图结构以及对其的关系。这里作者借用TransE的思想，<strong>为了避免参数过多，作者没有引入针对特定关系的嵌入</strong>。关系r的表示$\mathbf{r}$，可以通过和它相关的实体嵌入得到：<br>$$<br>r = \frac{1} {T_r} \sum_{(s,o) \in T_r}(h_s-h_o)  \tag  9<br>$$<br>&emsp;&emsp;其中 $T_r$ 是subject-object实体对的关系r，然后最小化下面的关系损失函数：<br>$$<br>L_2=\sum_{r\in R} \frac {1} {T_r} \sum_{(s,o)\in T_r}(h_s-h_o-r)  \tag {10}<br>$$</p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><ul>
<li><strong>Objective</strong></li>
</ul>
<p>&emsp;&emsp;最终的目标函数为两个损失函数的组合，其中$\alpha$为超参数，使用<a href="https://zhuanlan.zhihu.com/p/91736992" target="_blank" rel="noopener">Adam optimizer</a>优化：<br>$$<br>L = L_1+\alpha_2L_2 \tag {11}<br>$$<br>&emsp;&emsp;所有可学习的参数都使用Xavier initialization (Glorotand Bengio 2010)的初始化</p>
<ul>
<li><strong>Generalization to k-hop neighborhood</strong></li>
</ul>
<p>&emsp;&emsp;这里作者整合邻接的信息的时候使用$\rho(h_{i,1}^{(l)},h_{i,2}^{(l)})$作为k=2的整合方程(4)。我们可以用k-1个门机制函数去整合k邻的信息：<br>$$<br>h_{i}^{l} = \rho_{k-1} ( \cdots \rho_2 ( \rho_1 (h_{i,1}^{ (l) },h_{i,2}^{ (l) }),h_{i,3}^{ (l) }) \cdots)  \tag {12}<br>$$</p>
<ul>
<li><strong>Neighborhood augmentation</strong></li>
</ul>
<p>&emsp;&emsp;作者在这里为了进一步缓解KGs之间的异构性，对于预先对齐的实体i和j之间有边$e_{i,j}$，而在另一个图中对应对齐的实体i和实体j之间没有相应的边，则我们加上这样的边来缓解图之间的异构性。</p>
<ul>
<li><strong>Alignment prediction</strong></li>
</ul>
<p>&emsp;&emsp;对于实体对齐预测部分，采用了大多数采用的方法，计算另一个图中所有实体和本图中实体之间的距离的最小值作为对齐的预测值。</p>
<h2 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/004/KG-paper-004-06.PNG" alt=""></p>
<p>&emsp;&emsp;作者采用的数据集与大多数实体对齐采用的实体集大致一样。</p>
<h3 id="Comparative-Models"><a href="#Comparative-Models" class="headerlink" title="Comparative Models"></a>Comparative Models</h3><p>&emsp;&emsp;<strong>基于嵌入式的实体对齐模型</strong>：MTransE (Chen et al. 2017)、IPTransE (Zhu et al.2017)、JAPE (Sun, Hu, and Li 2017)、 AlignE (Sun et al. 2018)、GCN-Align (Wang et al. 2018)、SEA (Pei et al. 2019)、RSN(Guo, Sun, and Hu 2019)、MuGNN (Cao et al. 2019)</p>
<p>&emsp;&emsp;<strong>基于知识图谱嵌入的模型</strong>：（一般用于链接预测）TransH (Wang et al. 2014), ConvE (Dettmers et al. 2018) 、RotatE (Sun et al. 2019)</p>
<p>&emsp;&emsp;<strong>基于GCN的变形模型</strong>：GCN (Kipf and Welling 2017)、GAT (Velickovic et al. 2018) 、R-CGN(Schlichtkrull et al. 2018)。（这些模型作者重新调整相关设置，以便进入和作者模型同一管道中）</p>
<p>&emsp;&emsp;<strong>基于AliNet的变形模型</strong>：AliNet (w/o rel. loss)不使用优化关系损失、AliNet (w/o rel. loss &amp; augment.)既不优化关系损失也不进行邻接的整合</p>
<p>&emsp;&emsp;上述三种不同类型的模型就是作者比较的对象，同时作者对自己模型进行变型，分别看各个部分的效果。</p>
<h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><p>&emsp;&emsp;作者使用网格搜索法进行参数优化，具体的详情可以见论文叙述。</p>
<h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/004/KG-paper-004-07.PNG" alt=""></p>
<p>&emsp;&emsp;同时作者分析中，分别对<strong>整合的策略</strong>，对于<strong>网络层数</strong>以及<strong>选择的k值</strong>分别进行对比实验：</p>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/004/KG-paper-004-08.PNG" alt=""></p>
<hr>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/004/KG-paper-004-09.PNG" alt=""></p>
<hr>
<p><img src="https://gitee.com/jack_can/Pictrues/raw/master/KG-Papers/004/KG-paper-004-010.PNG" alt=""></p>
<p>&emsp;&emsp;作者最后还分析了由于实体的表达是由多层网络聚合而成的，然后作者分别用每层网络来表示实体，最后结果也与作者原先分析的大致相似，整合多层网络效果更好。</p>
<p>&emsp;&emsp;通过上边的比较分析，可以看到到作者提出的<strong>AliNet</strong>模型效果非常好。在考虑大多数数据集的结构都不是相似的情况下，提出了对应的解决方案，对于不同模块的处理作者花了很多心思，最后通过对比实验也验证了其理论。</p>
<p>参考资料：</p>
<p><a href="https://blog.csdn.net/byn12345/article/details/106151896" target="_blank" rel="noopener">1.论文解读 AAAI2020|AliNet</a></p>
]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>NLP</tag>
        <tag>知识融合</tag>
        <tag>论文笔记</tag>
      </tags>
  </entry>
</search>
