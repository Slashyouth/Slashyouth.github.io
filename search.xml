<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Pandas学习笔记---001</title>
    <url>/2020/05/28/Pandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0---001/</url>
    <content><![CDATA[<h1 id="Pandas学习笔记-——001"><a href="#Pandas学习笔记-——001" class="headerlink" title="Pandas学习笔记 ——001"></a>Pandas学习笔记 ——001</h1><h2 id="1-数据类型"><a href="#1-数据类型" class="headerlink" title="1. 数据类型"></a>1. 数据类型</h2><blockquote>
<p>Series：带标签的一维数组<br>DataFrame：带标签的，大小可变的，二维异构表格</p>
</blockquote>
<hr>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 生成Series</span></span><br><span class="line">s = pd.Series([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, np.nan, <span class="number">6</span>, <span class="number">8</span>])</span><br><span class="line">print(s)</span><br><span class="line"><span class="number">0</span>    <span class="number">1.0</span></span><br><span class="line"><span class="number">1</span>    <span class="number">3.0</span></span><br><span class="line"><span class="number">2</span>    <span class="number">5.0</span></span><br><span class="line"><span class="number">3</span>    NaN</span><br><span class="line"><span class="number">4</span>    <span class="number">6.0</span></span><br><span class="line"><span class="number">5</span>    <span class="number">8.0</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>
<h2 id="2-使用技巧"><a href="#2-使用技巧" class="headerlink" title="2. 使用技巧"></a>2. 使用技巧</h2><h3 id="1-sort-indedx和sort-value"><a href="#1-sort-indedx和sort-value" class="headerlink" title="1. sort_indedx和sort_value"></a>1. sort_indedx和sort_value</h3><blockquote>
<p>sort_index(axis=0, level=None, ascending=True, inplace=False, kind=‘quicksort’, na_position=‘last’, sort_remaining=True, ignore_index: bool = False,)<br>axis:0按行名排序，1按列名排序<br>ascending：默认True升序排列；False降序排列<br>inplace：默认False，否则排序之后的数据直接替换原来的数据框<br>kind：排序方法，{‘quicksort’, ‘mergesort’, ‘heapsort’}, default ‘quicksort’。似乎不用太关心<br>na_position：缺失值默认排在最后{“first”,“last”}       </p>
</blockquote>
<hr>
<blockquote>
<p> sort_values(by, axis=0, ascending=True, inplace=False, kind=”quicksort”, na_position=”last”, ignore_index=False)<br>by: str or list of str；如果axis=0，那么by=”列名”；如果axis=1，那么by=”行名” # 必须给出参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">'b'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>],<span class="string">'a'</span>:[<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>],<span class="string">'c'</span>:[<span class="number">1</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">2</span>]&#125;,index=[<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>]) </span><br><span class="line">    b   a   c</span><br><span class="line"><span class="number">2</span>   <span class="number">1</span>   <span class="number">4</span>   <span class="number">1</span></span><br><span class="line"><span class="number">0</span>   <span class="number">2</span>   <span class="number">3</span>   <span class="number">3</span></span><br><span class="line"><span class="number">1</span>   <span class="number">3</span>   <span class="number">2</span>   <span class="number">8</span></span><br><span class="line"><span class="number">3</span>   <span class="number">2</span>   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">按b列升序排序: df.sort_values(by=<span class="string">'b'</span>) <span class="comment"># 等同于df.sort_values(by='b',axis=0)</span></span><br><span class="line">先按b列降序,再按a列升序排序: df.sort_values(by=[<span class="string">'b'</span>,<span class="string">'a'</span>],axis=<span class="number">0</span>,ascending=[<span class="literal">False</span>,<span class="literal">True</span>]) <span class="comment"># 等同于df.sort_values(by=['b','a'],axis=0,ascending=[False,True])      </span></span><br><span class="line">按行<span class="number">3</span>升序排列: df.sort_values(by=<span class="number">3</span>,axis=<span class="number">1</span>) <span class="comment"># 必须指定axis=1     </span></span><br><span class="line">按行<span class="number">3</span>升序,行<span class="number">0</span>降排列:  df.sort_values(by=[<span class="number">3</span>,<span class="number">0</span>],axis=<span class="number">1</span>,ascending=[<span class="literal">True</span>,<span class="literal">False</span>])</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="2-对于无效数据的处理"><a href="#2-对于无效数据的处理" class="headerlink" title="2. 对于无效数据的处理"></a>2. 对于无效数据的处理</h3><blockquote>
<p>空值：在pandas中的空值是””<br>缺失值：在dataframe中为nan或者naT（缺失时间），在series中为none或者nan即可<br>相关函数： df.dropna()、df.fillna()、df.isnull()、df.isna()</p>
<blockquote>
<p>函数： df.dropna(axis=0, how=’any’, thresh=None, subset=None, inplace=False)<br>删除表中<strong>全部</strong>为NaN的<strong>行</strong>: df.dropna(axis=0,how=’all’)<br>删除表中<strong>含有任何</strong>NaN的<strong>行</strong>： df.dropna(axis=0,how=’any’)<br>删除表中<strong>全部</strong>为NaN的<strong>列</strong>： df.dropna(axis=1,how=’all’)<br>删除表中<strong>含有任何</strong>NaN的<strong>列</strong>:  df.dropna(axis=1,how=’any’)<br>thresh(int): axis中至少有int个非缺失值，否则删除<br>subset: array-like, optional,Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.  </p>
<hr>
<p>函数： df.fillna(self, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None)<br>method{‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default None 说明用前面数据‘ffill’/‘pad’替换后面NaN数据 ‘backfill’/‘bfill’则相反          </p>
</blockquote>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/05/19/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><a id="more"></a>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Pandas学习笔记---002</title>
    <url>/2020/05/31/Pandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0---002/</url>
    <content><![CDATA[<h1 id="Pandas学习笔记—002"><a href="#Pandas学习笔记—002" class="headerlink" title="Pandas学习笔记—002"></a>Pandas学习笔记—002</h1><h2 id="基础用法"><a href="#基础用法" class="headerlink" title="基础用法"></a>基础用法</h2><h3 id="1-df-head-n-和-df-tail"><a href="#1-df-head-n-和-df-tail" class="headerlink" title="1. df.head(n) 和 df.tail()"></a>1. df.head(n) 和 df.tail()</h3><blockquote>
<p>默认n=5，同时n还可以取负值，对于head来说就是展示df[:-n],对于tail来说就是展示df[-n:]   </p>
</blockquote>
<h3 id="2-合并重叠数据集"><a href="#2-合并重叠数据集" class="headerlink" title="2. 合并重叠数据集"></a>2. 合并重叠数据集</h3><blockquote>
<p>有时，要合并两个相似的数据集，两个数据集里的其中一个的数据比另一个多。<br>比如，展示特定经济指标的两个数据序列，其中一个是“高质量”指标，<a id="more"></a><br>另一个是“低质量”指标。一般来说，低质量序列可能包含更多的历史数据，或覆盖更广的数据。<br>因此，要合并这两个 DataFrame 对象，其中一个 DataFrame 中的缺失值将按指定条件用另一个<br>DataFrame 里类似标签中的数据进行填充。                    </p>
<blockquote>
<p>函数df1.combine_first(df2): 由df2只填df1的NaN值    </p>
</blockquote>
</blockquote>
<h3 id="3-描述性统计"><a href="#3-描述性统计" class="headerlink" title="3. 描述性统计"></a>3. 描述性统计</h3><table>
<thead>
<tr>
<th align="center">函数</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center">count</td>
<td align="center">统计非空值数量</td>
</tr>
<tr>
<td align="center">sum</td>
<td align="center">汇总值</td>
</tr>
<tr>
<td align="center">mean</td>
<td align="center">平均值</td>
</tr>
<tr>
<td align="center">mad</td>
<td align="center">平均绝对偏差</td>
</tr>
<tr>
<td align="center">median</td>
<td align="center">算数中位数</td>
</tr>
<tr>
<td align="center">min</td>
<td align="center">最小值</td>
</tr>
<tr>
<td align="center">max</td>
<td align="center">最大值</td>
</tr>
<tr>
<td align="center">mode</td>
<td align="center">众数</td>
</tr>
<tr>
<td align="center">abs</td>
<td align="center">绝对值</td>
</tr>
<tr>
<td align="center">prod</td>
<td align="center">乘积</td>
</tr>
<tr>
<td align="center">std</td>
<td align="center">贝塞尔校正的样本标准偏差</td>
</tr>
<tr>
<td align="center">var</td>
<td align="center">无偏方差</td>
</tr>
<tr>
<td align="center">sem</td>
<td align="center">平均值的标准误差</td>
</tr>
<tr>
<td align="center">skew</td>
<td align="center">样本偏度 (第三阶)</td>
</tr>
<tr>
<td align="center">kurt</td>
<td align="center">样本峰度 (第四阶)</td>
</tr>
<tr>
<td align="center">quantile</td>
<td align="center">样本分位数 (不同 % 的值)</td>
</tr>
<tr>
<td align="center">cumsum</td>
<td align="center">累加</td>
</tr>
<tr>
<td align="center">cumprod</td>
<td align="center">累乘</td>
</tr>
<tr>
<td align="center">cummax</td>
<td align="center">累积最大值</td>
</tr>
<tr>
<td align="center">cummin</td>
<td align="center">累积最小值</td>
</tr>
<tr>
<td align="center">describe</td>
<td align="center">数据总描述</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第1章 机器学习基础</title>
    <url>/2020/05/31/1.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h2 id="机器学习-概述"><a href="#机器学习-概述" class="headerlink" title="机器学习 概述"></a>机器学习 概述</h2><p><code>机器学习(Machine Learning,ML)</code> 是使用计算机来彰显数据背后的真实含义，它为了把无序的数据转换成有用的信息。是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。<br>它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。</p>
<a id="more"></a>

<ol>
<li>海量的数据</li>
<li>获取有用的信息</li>
</ol>
<h2 id="机器学习-研究意义"><a href="#机器学习-研究意义" class="headerlink" title="机器学习 研究意义"></a>机器学习 研究意义</h2><p>机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”。 “机器学习是对能通过经验自动改进的计算机算法的研究”。 “机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。” 一种经常引用的英文定义是：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
<p>机器学习已经有了十分广泛的应用，例如：数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用。</p>
<h2 id="机器学习-场景"><a href="#机器学习-场景" class="headerlink" title="机器学习 场景"></a>机器学习 场景</h2><ul>
<li><p>例如：识别动物猫</p>
<ul>
<li>模式识别（官方标准）：人们通过大量的经验，得到结论，从而判断它就是猫。</li>
<li>机器学习（数据学习）：人们通过阅读进行学习，观察它会叫、小眼睛、两只耳朵、四条腿、一条尾巴，得到结论，从而判断它就是猫。</li>
<li>深度学习（深入数据）：人们通过深入了解它，发现它会’喵喵’的叫、与同类的猫科动物很类似，得到结论，从而判断它就是猫。（深度学习常用领域：语音识别、图像识别）</li>
</ul>
</li>
<li><p>模式识别（pattern recognition）: 模式识别是最古老的（作为一个术语而言，可以说是很过时的）。</p>
<ul>
<li>我们把环境与客体统称为“模式”，识别是对模式的一种认知，是如何让一个计算机程序去做一些看起来很“智能”的事情。</li>
<li>通过融于智慧和直觉后，通过构建程序，识别一些事物，而不是人，例如: 识别数字。</li>
</ul>
</li>
<li><p>机器学习（machine learning）: 机器学习是最基础的（当下初创公司和研究实验室的热点领域之一）。</p>
<ul>
<li>在90年代初，人们开始意识到一种可以更有效地构建模式识别算法的方法，那就是用数据（可以通过廉价劳动力采集获得）去替换专家（具有很多图像方面知识的人）。</li>
<li>“机器学习”强调的是，在给计算机程序（或者机器）输入一些数据后，它必须做一些事情，那就是学习这些数据，而这个学习的步骤是明确的。</li>
<li>机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身性能的学科。</li>
</ul>
</li>
<li><p>深度学习（deep learning）: 深度学习是非常崭新和有影响力的前沿领域，我们甚至不会去思考-后深度学习时代。</p>
<ul>
<li>深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。</li>
</ul>
</li>
<li><p>参考地址： </p>
<ul>
<li><a href="http://www.csdn.net/article/2015-03-24/2824301" target="_blank" rel="noopener">深度学习 vs 机器学习 vs 模式识别</a></li>
<li><a href="http://baike.baidu.com/link?url=76P-uA4EBrC3G-I__P1tqeO7eoDS709Kp4wYuHxc7GNkz_xn0NxuAtEohbpey7LUa2zUQLJxvIKUx4bnrEfOmsWLKbDmvG1PCoRkJisMTQka6-QReTrIxdYY3v93f55q" target="_blank" rel="noopener">深度学习 百科资料</a></li>
</ul>
</li>
</ul>
<blockquote>
<p>机器学习已应用于多个领域，远远超出大多数人的想象，横跨：计算机科学、工程技术和统计学等多个学科。</p>
</blockquote>
<ul>
<li>搜索引擎: 根据你的搜索点击，优化你下次的搜索结果,是机器学习来帮助搜索引擎判断哪个结果更适合你（也判断哪个广告更适合你）。</li>
<li>垃圾邮件: 会自动的过滤垃圾广告邮件到垃圾箱内。</li>
<li>超市优惠券: 你会发现，你在购买小孩子尿布的时候，售货员会赠送你一张优惠券可以兑换6罐啤酒。</li>
<li>邮局邮寄: 手写软件自动识别寄送贺卡的地址。</li>
<li>申请贷款: 通过你最近的金融活动信息进行综合评定，决定你是否合格。</li>
</ul>
<h2 id="机器学习-组成"><a href="#机器学习-组成" class="headerlink" title="机器学习 组成"></a>机器学习 组成</h2><h3 id="主要任务"><a href="#主要任务" class="headerlink" title="主要任务"></a>主要任务</h3><ul>
<li>分类（classification）：将实例数据划分到合适的类别中。<ul>
<li>应用实例：判断网站是否被黑客入侵（二分类 ），手写数字的自动识别（多分类）</li>
</ul>
</li>
<li>回归（regression）：主要用于预测数值型数据。<ul>
<li>应用实例：股票价格波动的预测，房屋价格的预测等。</li>
</ul>
</li>
</ul>
<h3 id="监督学习（supervised-learning）"><a href="#监督学习（supervised-learning）" class="headerlink" title="监督学习（supervised learning）"></a>监督学习（supervised learning）</h3><ul>
<li>必须确定目标变量的值，以便机器学习算法可以发现特征和目标变量之间的关系。在监督学习中，给定一组数据，我们知道正确的输出结果应该是什么样子，并且知道在输入和输出之间有着一个特定的关系。 (包括：分类和回归)</li>
<li>样本集：训练数据 + 测试数据<ul>
<li>训练样本 = 特征(feature) + 目标变量(label: 分类-离散值/回归-连续值)</li>
<li>特征通常是训练样本集的列，它们是独立测量得到的。</li>
<li>目标变量: 目标变量是机器学习预测算法的测试结果。<ul>
<li>在分类算法中目标变量的类型通常是标称型(如：真与假)，而在回归算法中通常是连续型(如：1~100)。</li>
</ul>
</li>
</ul>
</li>
<li>监督学习需要注意的问题：<ul>
<li>偏置方差权衡</li>
<li>功能的复杂性和数量的训练数据</li>
<li>输入空间的维数</li>
<li>噪声中的输出值</li>
</ul>
</li>
<li><code>知识表示</code>：<ul>
<li>可以采用规则集的形式【例如：数学成绩大于90分为优秀】</li>
<li>可以采用概率分布的形式【例如：通过统计分布发现，90%的同学数学成绩，在70分以下，那么大于70分定为优秀】</li>
<li>可以使用训练样本集中的一个实例【例如：通过样本集合，我们训练出一个模型实例，得出 年轻，数学成绩中高等，谈吐优雅，我们认为是优秀】</li>
</ul>
</li>
</ul>
<h3 id="非监督学习（unsupervised-learing）"><a href="#非监督学习（unsupervised-learing）" class="headerlink" title="非监督学习（unsupervised learing）"></a>非监督学习（unsupervised learing）</h3><ul>
<li>在机器学习，无监督学习的问题是，在未加标签的数据中，试图找到隐藏的结构。因为提供给学习者的实例是未标记的，因此没有错误或报酬信号来评估潜在的解决方案。</li>
<li>无监督学习是密切相关的统计数据密度估计的问题。然而无监督学习还包括寻求，总结和解释数据的主要特点等诸多技术。在无监督学习使用的许多方法是基于用于处理数据的数据挖掘方法。</li>
<li>数据没有类别信息，也不会给定目标值。</li>
<li>非监督学习包括的类型：<ul>
<li>聚类：在无监督学习中，将数据集分成由类似的对象组成多个类的过程称为聚类。</li>
<li>密度估计：通过样本分布的紧密程度，来估计与分组的相似性。</li>
<li>此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3>这个算法可以训练程序做出某一决定。程序在某一情况下尝试所有的可能行动，记录不同行动的结果并试着找出最好的一次尝试来做决定。 属于这一类算法的有马尔可夫决策过程。<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3></li>
</ul>
</li>
</ul>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/1.MLFoundation/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B.jpg" alt="机器学习训练过程图"></p>
<h3 id="算法汇总"><a href="#算法汇总" class="headerlink" title="算法汇总"></a>算法汇总</h3><p><img src="http://data.apachecn.org/img/AiLearning/ml/1.MLFoundation/ml_algorithm.jpg" alt="算法汇总"></p>
<h2 id="机器学习-使用"><a href="#机器学习-使用" class="headerlink" title="机器学习 使用"></a>机器学习 使用</h2><blockquote>
<p>选择算法需要考虑的两个问题</p>
</blockquote>
<ol>
<li>算法场景<ul>
<li>预测明天是否下雨，因为可以用历史的天气情况做预测，所以选择监督学习算法</li>
<li>给一群陌生的人进行分组，但是我们并没有这些人的类别信息，所以选择无监督学习算法、通过他们身高、体重等特征进行处理。</li>
</ul>
</li>
<li>需要收集或分析的数据是什么</li>
</ol>
<blockquote>
<p>举例</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/1.MLFoundation/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95.jpg" alt="选择算法图"></p>
<blockquote>
<p>机器学习 开发流程</p>
</blockquote>
<ol>
<li>收集数据: 收集样本数据</li>
<li>准备数据: 注意数据的格式</li>
<li>分析数据: 为了确保数据集中没有垃圾数据；<ul>
<li>如果是算法可以处理的数据格式或可信任的数据源，则可以跳过该步骤；</li>
<li>另外该步骤需要人工干预，会降低自动化系统的价值。</li>
</ul>
</li>
<li>训练算法: [机器学习算法核心]如果使用无监督学习算法，由于不存在目标变量值，则可以跳过该步骤</li>
<li>测试算法: [机器学习算法核心]评估算法效果</li>
<li>使用算法: 将机器学习算法转为应用程序</li>
</ol>
<h2 id="机器学习-数学基础"><a href="#机器学习-数学基础" class="headerlink" title="机器学习 数学基础"></a>机器学习 数学基础</h2><ul>
<li>微积分</li>
<li>统计学/概率论</li>
<li>线性代数<h2 id="机器学习-工具"><a href="#机器学习-工具" class="headerlink" title="机器学习 工具"></a>机器学习 工具</h2></li>
</ul>
<h3 id="Python语言"><a href="#Python语言" class="headerlink" title="Python语言"></a>Python语言</h3><ol>
<li>可执行伪代码</li>
<li>Python比较流行：使用广泛、代码范例多、丰富模块库，开发周期短</li>
<li>Python语言的特色：清晰简练、易于理解</li>
<li>Python语言的缺点：唯一不足的是性能问题</li>
<li>Python相关的库<ul>
<li>科学函数库：<code>SciPy</code>、<code>NumPy</code>(底层语言：C和Fortran)</li>
<li>绘图工具库：<code>Matplotlib</code></li>
<li>数据分析库 <code>Pandas</code><h3 id="数学工具"><a href="#数学工具" class="headerlink" title="数学工具"></a>数学工具</h3></li>
</ul>
</li>
</ol>
<ul>
<li>Matlab<h2 id="附：机器学习专业术语"><a href="#附：机器学习专业术语" class="headerlink" title="附：机器学习专业术语"></a>附：机器学习专业术语</h2></li>
<li>模型（model）：计算机层面的认知</li>
<li>学习算法（learning algorithm），从数据中产生模型的方法</li>
<li>数据集（data set）：一组记录的合集</li>
<li>示例（instance）：对于某个对象的描述</li>
<li>样本（sample）：也叫示例</li>
<li>属性（attribute）：对象的某方面表现或特征</li>
<li>特征（feature）：同属性</li>
<li>属性值（attribute value）：属性上的取值</li>
<li>属性空间（attribute space）：属性张成的空间</li>
<li>样本空间/输入空间（samplespace）：同属性空间</li>
<li>特征向量（feature vector）：在属性空间里每个点对应一个坐标向量，把一个示例称作特征向量</li>
<li>维数（dimensionality）：描述样本参数的个数（也就是空间是几维的）</li>
<li>学习（learning）/训练（training）：从数据中学得模型</li>
<li>训练数据（training data）：训练过程中用到的数据</li>
<li>训练样本（training sample）:训练用到的每个样本</li>
<li>训练集（training set）：训练样本组成的集合</li>
<li>假设（hypothesis）：学习模型对应了关于数据的某种潜在规则</li>
<li>真相（ground-truth）:真正存在的潜在规律</li>
<li>学习器（learner）：模型的另一种叫法，把学习算法在给定数据和参数空间的实例化</li>
<li>预测（prediction）：判断一个东西的属性</li>
<li>标记（label）：关于示例的结果信息，比如我是一个“好人”。</li>
<li>样例（example）：拥有标记的示例</li>
<li>标记空间/输出空间（label space）：所有标记的集合</li>
<li>分类（classification）：预测是离散值，比如把人分为好人和坏人之类的学习任务</li>
<li>回归（regression）：预测值是连续值，比如你的好人程度达到了0.9，0.6之类的</li>
<li>二分类（binary classification）：只涉及两个类别的分类任务</li>
<li>正类（positive class）：二分类里的一个</li>
<li>反类（negative class）：二分类里的另外一个</li>
<li>多分类（multi-class classification）：涉及多个类别的分类</li>
<li>测试（testing）：学习到模型之后对样本进行预测的过程</li>
<li>测试样本（testing sample）：被预测的样本</li>
<li>聚类（clustering）：把训练集中的对象分为若干组</li>
<li>簇（cluster）：每一个组叫簇</li>
<li>监督学习（supervised learning）：典范–分类和回归</li>
<li>无监督学习（unsupervised learning）：典范–聚类</li>
<li>未见示例（unseen instance）：“新样本“，没训练过的样本</li>
<li>泛化（generalization）能力：学得的模型适用于新样本的能力</li>
<li>分布（distribution）：样本空间的全体样本服从的一种规律</li>
<li>独立同分布（independent and identically distributed，简称i,i,d.）:获得的每个样本都是独立地从这个分布上采样获得的。</li>
</ul>
<h2 id="机器学习基础补充"><a href="#机器学习基础补充" class="headerlink" title="机器学习基础补充"></a>机器学习基础补充</h2><h3 id="数据集的划分"><a href="#数据集的划分" class="headerlink" title="数据集的划分"></a>数据集的划分</h3><ul>
<li>训练集（Training set） —— 学习样本数据集，通过匹配一些参数来建立一个模型，主要用来训练模型。类比考研前做的解题大全。</li>
<li>验证集（validation set） —— 对学习出来的模型，调整模型的参数，如在神经网络中选择隐藏单元数。验证集还用来确定网络结构或者控制模型复杂程度的参数。类比 考研之前做的模拟考试。</li>
<li>测试集（Test set） —— 测试训练好的模型的分辨能力。类比 考研。这次真的是一考定终身。</li>
</ul>
<h3 id="模型拟合程度"><a href="#模型拟合程度" class="headerlink" title="模型拟合程度"></a>模型拟合程度</h3><ul>
<li>欠拟合（Underfitting）：模型没有很好地捕捉到数据特征，不能够很好地拟合数据，对训练样本的一般性质尚未学好。类比，光看书不做题觉得自己什么都会了，上了考场才知道自己啥都不会。</li>
<li>过拟合（Overfitting）：模型把训练样本学习“太好了”，可能把一些训练样本自身的特性当做了所有潜在样本都有的一般性质，导致泛化能力下降。类比，做课后题全都做对了，超纲题也都认为是考试必考题目，上了考场还是啥都不会。 </li>
</ul>
<p>通俗来说，欠拟合和过拟合都可以用一句话来说，欠拟合就是：“你太天真了！”，过拟合就是：“你想太多了！”。</p>
<h3 id="常见的模型指标"><a href="#常见的模型指标" class="headerlink" title="常见的模型指标"></a>常见的模型指标</h3><ul>
<li>正确率 —— 提取出的正确信息条数 / 提取出的信息条数</li>
<li>召回率 —— 提取出的正确信息条数 / 样本中的信息条数</li>
<li>F 值 —— 正确率 * 召回率 * 2 / （正确率 + 召回率）（F值即为正确率和召回率的调和平均值）</li>
</ul>
<p>举个例子如下：</p>
<p>举个例子如下：<br>某池塘有 1400 条鲤鱼，300 只虾，300 只乌龟。现在以捕鲤鱼为目的。撒了一张网，逮住了 700 条鲤鱼，200 只<br>虾， 100 只乌龟。那么这些指标分别如下：<br>正确率 = 700 / (700 + 200 + 100) = 70%<br>召回率 = 700 / 1400 = 50%<br>F 值 = 70% * 50% * 2 / (70% + 50%) = 58.3%</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><ul>
<li>分类问题 —— 说白了就是将一些未知类别的数据分到现在已知的类别中去。比如，根据你的一些信息，判断你是高富帅，还是穷屌丝。评判分类效果好坏的三个指标就是上面介绍的三个指标：正确率，召回率，F值。</li>
<li>回归问题 —— 对数值型连续随机变量进行预测和建模的监督学习算法。回归往往会通过计算 误差（Error）来确定模型的精确性。</li>
<li>聚类问题 —— 聚类是一种无监督学习任务，该算法基于数据的内部结构寻找观察样本的自然族群（即集群）。聚类问题的标准一般基于距离：簇内距离（Intra-cluster Distance） 和 簇间距离（Inter-cluster Distance） 。簇内距离是越小越好，也就是簇内的元素越相似越好；而簇间距离越大越好，也就是说簇间（不同簇）元素越不相同越好。一般的，衡量聚类问题会给出一个结合簇内距离和簇间距离的公式。</li>
</ul>
<p>下面这个图可以比较直观地展示出来：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/1.MLFoundation/ml_add_1.jpg" alt=""></p>
<h3 id="特征工程的一些小东西"><a href="#特征工程的一些小东西" class="headerlink" title="特征工程的一些小东西"></a>特征工程的一些小东西</h3><ul>
<li><p>特征选择 —— 也叫特征子集选择（FSS，Feature Subset Selection）。是指从已有的 M 个特征（Feature）中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。</p>
</li>
<li><p>特征提取 —— 特征提取是计算机视觉和图像处理中的一个概念。它指的是使用计算机提取图像信息，决定每个图像的点是否属于一个图像特征。特征提取的结果是把图像上的点分为不同的子集，这些子集往往属于孤立的点，连续的曲线或者连续的区域。</p>
</li>
</ul>
<p>下面给出一个特征工程的图：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/1.MLFoundation/ml_add_2.jpg" alt=""></p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul>
<li>Learning rate —— 学习率，通俗地理解，可以理解为步长，步子大了，很容易错过最佳结果。就是本来目标尽在咫尺，可是因为我迈的步子很大，却一下子走过了。步子小了呢，就是同样的距离，我却要走很多很多步，这样导致训练的耗时费力还不讨好。</li>
<li>一个总结的知识点很棒的链接 ：<a href="https://zhuanlan.zhihu.com/p/25197792" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25197792</a></li>
</ul>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第2章 k-近邻算法</title>
    <url>/2020/05/31/2.k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="KNN-概述"><a href="#KNN-概述" class="headerlink" title="KNN 概述"></a>KNN 概述</h2><p><code>k-近邻（kNN, k-NearestNeighbor）算法是一种基本分类与回归方法，我们这里只讨论分类问题中的 k-近邻算法。</code></p>
<p><strong>一句话总结：近朱者赤近墨者黑！</strong> </p>
<p><code>k 近邻算法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。k 近邻算法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其 k 个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k近邻算法不具有显式的学习过程。</code></p>
<a id="more"></a>

<p><code>k 近邻算法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。 k值的选择、距离度量以及分类决策规则是k近邻算法的三个基本要素。</code></p>
<h2 id="KNN-场景"><a href="#KNN-场景" class="headerlink" title="KNN 场景"></a>KNN 场景</h2><p>电影可以按照题材分类，那么如何区分 <code>动作片</code> 和 <code>爱情片</code> 呢？<br/></p>
<ol>
<li>动作片：打斗次数更多</li>
<li>爱情片：亲吻次数更多</li>
</ol>
<p>基于电影中的亲吻、打斗出现的次数，使用 k-近邻算法构造程序，就可以自动划分电影的题材类型。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/2.KNN/knn-1-movie.png" alt="电影视频案例" title="电影视频案例"></p>
<figure class="highlight inform7"><table><tr><td class="code"><pre><span class="line">现在根据上面我们得到的样本集中所有电影与未知电影的距离，按照距离递增排序，可以找到 k 个距离最近的电影。</span><br><span class="line">假定 k=3，则三个最靠近的电影依次是， He's Not Really into Dudes 、 Beautiful <span class="keyword">Woman</span> 和 California <span class="keyword">Man</span>。</span><br><span class="line">knn 算法按照距离最近的三部电影的类型，决定未知电影的类型，而这三部电影全是爱情片，因此我们判定未知电影是爱情片。</span><br></pre></td></tr></table></figure>

<h2 id="KNN-原理"><a href="#KNN-原理" class="headerlink" title="KNN 原理"></a>KNN 原理</h2><blockquote>
<p>KNN 工作原理</p>
</blockquote>
<ol>
<li>假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系。</li>
<li>输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。<ol>
<li>计算新数据与样本数据集中每条数据的距离。</li>
<li>对求得的所有距离进行排序（从小到大，越小表示越相似）。</li>
<li>取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。</li>
</ol>
</li>
<li>求 k 个数据中出现次数最多的分类标签作为新数据的分类。</li>
</ol>
<blockquote>
<p>KNN 通俗理解</p>
</blockquote>
<p>给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类。</p>
<blockquote>
<p>KNN 开发流程</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">收集数据：任何方法</span><br><span class="line">准备数据：距离计算所需要的数值，最好是结构化的数据格式</span><br><span class="line">分析数据：任何方法</span><br><span class="line">训练算法：此步骤不适用于 k-近邻算法</span><br><span class="line">测试算法：计算错误率</span><br><span class="line">使用算法：输入样本数据和结构化的输出结果，然后运行 k-近邻算法判断输入数据分类属于哪个分类，最后对计算出的分类执行后续处理</span><br></pre></td></tr></table></figure>

<blockquote>
<p>KNN 算法特点</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">优点：精度高、对异常值不敏感、无数据输入假定</span><br><span class="line">缺点：计算复杂度高、空间复杂度高</span><br><span class="line">适用数据范围：数值型和标称型</span><br></pre></td></tr></table></figure>

<h2 id="KNN-项目案例"><a href="#KNN-项目案例" class="headerlink" title="KNN 项目案例"></a>KNN 项目案例</h2><h3 id="项目案例1-优化约会网站的配对效果"><a href="#项目案例1-优化约会网站的配对效果" class="headerlink" title="项目案例1: 优化约会网站的配对效果"></a>项目案例1: 优化约会网站的配对效果</h3><p><a href="/src/py2.x/ml/2.KNN/kNN.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/2.KNN/kNN.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/2.KNN/kNN.py</a></p>
<h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p>海伦使用约会网站寻找约会对象。经过一段时间之后，她发现曾交往过三种类型的人:</p>
<ul>
<li>不喜欢的人</li>
<li>魅力一般的人</li>
<li>极具魅力的人</li>
</ul>
<p>她希望：</p>
<ol>
<li>工作日与魅力一般的人约会</li>
<li>周末与极具魅力的人约会</li>
<li>不喜欢的人则直接排除掉</li>
</ol>
<p>现在她收集到了一些约会网站未曾记录的数据信息，这更有助于匹配对象的归类。</p>
<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">收集数据：提供文本文件</span><br><span class="line">准备数据：使用 Python 解析文本文件</span><br><span class="line">分析数据：使用 Matplotlib 画二维散点图</span><br><span class="line">训练算法：此步骤不适用于 k-近邻算法</span><br><span class="line">测试算法：使用海伦提供的部分数据作为测试样本。</span><br><span class="line">        测试样本和非测试样本的区别在于：</span><br><span class="line">            测试样本是已经完成分类的数据，如果预测分类与实际类别不同，则标记为一个错误。</span><br><span class="line">使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型。</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据：提供文本文件</p>
</blockquote>
<p>海伦把这些约会对象的数据存放在文本文件 <a href="/data/2.KNN/datingTestSet2.txt">datingTestSet2.txt</a> 中，总共有 1000 行。海伦约会的对象主要包含以下 3 种特征：</p>
<ul>
<li>每年获得的飞行常客里程数</li>
<li>玩视频游戏所耗时间百分比</li>
<li>每周消费的冰淇淋公升数</li>
</ul>
<p>文本文件数据格式如下：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">40920</span>	<span class="number">8.326976</span>	<span class="number">0.953952</span>	<span class="number">3</span></span><br><span class="line"><span class="number">14488</span>	<span class="number">7.153469</span>	<span class="number">1.673904</span>	<span class="number">2</span></span><br><span class="line"><span class="number">26052</span>	<span class="number">1.441871</span>	<span class="number">0.805124</span>	<span class="number">1</span></span><br><span class="line"><span class="number">75136</span>	<span class="number">13.147394</span>	<span class="number">0.428964</span>	<span class="number">1</span></span><br><span class="line"><span class="number">38344</span>	<span class="number">1.669788</span>	<span class="number">0.134296</span>	<span class="number">1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>准备数据：使用 Python 解析文本文件</p>
</blockquote>
<p>将文本记录转换为 NumPy 的解析程序</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file2matrix</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        导入训练数据</span></span><br><span class="line"><span class="string">    parameters:</span></span><br><span class="line"><span class="string">        filename: 数据文件路径</span></span><br><span class="line"><span class="string">    return: </span></span><br><span class="line"><span class="string">        数据矩阵 returnMat 和对应的类别 classLabelVector</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="comment"># 获得文件中的数据行的行数</span></span><br><span class="line">    numberOfLines = len(fr.readlines())</span><br><span class="line">    <span class="comment"># 生成对应的空矩阵</span></span><br><span class="line">    <span class="comment"># 例如：zeros(2，3)就是生成一个 2*3的矩阵，各个位置上全是 0 </span></span><br><span class="line">    returnMat = zeros((numberOfLines, <span class="number">3</span>))  <span class="comment"># prepare matrix to return</span></span><br><span class="line">    classLabelVector = []  <span class="comment"># prepare labels return</span></span><br><span class="line">    fr = open(filename)</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment"># str.strip([chars]) --返回已移除字符串头尾指定字符所生成的新字符串</span></span><br><span class="line">        line = line.strip()</span><br><span class="line">        <span class="comment"># 以 '\t' 切割字符串</span></span><br><span class="line">        listFromLine = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="comment"># 每列的属性数据</span></span><br><span class="line">        returnMat[index, :] = listFromLine[<span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">        <span class="comment"># 每列的类别数据，就是 label 标签数据</span></span><br><span class="line">        classLabelVector.append(int(listFromLine[<span class="number">-1</span>]))</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 返回数据矩阵returnMat和对应的类别classLabelVector</span></span><br><span class="line">    <span class="keyword">return</span> returnMat, classLabelVector</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据：使用 Matplotlib 画二维散点图</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.scatter(datingDataMat[:, <span class="number">0</span>], datingDataMat[:, <span class="number">1</span>], <span class="number">15.0</span>*array(datingLabels), <span class="number">15.0</span>*array(datingLabels))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>下图中采用矩阵的第一和第二列属性得到很好的展示效果，清晰地标识了三个不同的样本分类区域，具有不同爱好的人其类别区域也不同。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/2.KNN/knn_matplotlib_2.png" alt="Matplotlib 散点图"></p>
<ul>
<li>归一化数据 （归一化是一个让权重变为统一的过程，更多细节请参考： <a href="https://www.zhihu.com/question/19951858" target="_blank" rel="noopener">https://www.zhihu.com/question/19951858</a> ）</li>
</ul>
<table>
<thead>
<tr>
<th>序号</th>
<th align="center">玩视频游戏所耗时间百分比</th>
<th align="right">每年获得的飞行常客里程数</th>
<th align="right">每周消费的冰淇淋公升数</th>
<th align="right">样本分类</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td align="center">0.8</td>
<td align="right">400</td>
<td align="right">0.5</td>
<td align="right">1</td>
</tr>
<tr>
<td>2</td>
<td align="center">12</td>
<td align="right">134 000</td>
<td align="right">0.9</td>
<td align="right">3</td>
</tr>
<tr>
<td>3</td>
<td align="center">0</td>
<td align="right">20 000</td>
<td align="right">1.1</td>
<td align="right">2</td>
</tr>
<tr>
<td>4</td>
<td align="center">67</td>
<td align="right">32 000</td>
<td align="right">0.1</td>
<td align="right">2</td>
</tr>
</tbody></table>
<p>样本3和样本4的距离：<br>$$\sqrt{(0-67)^2 + (20000-32000)^2 + (1.1-0.1)^2 }$$</p>
<p>归一化特征值，消除特征之间量级不同导致的影响</p>
<p><strong>归一化定义：</strong> 我是这样认为的，归一化就是要把你需要处理的数据经过处理后（通过某种算法）限制在你需要的一定范围内。首先归一化是为了后面数据处理的方便，其次是保正程序运行时收敛加快。 方法有如下：</p>
<p>1) 线性函数转换，表达式如下：　　</p>
<pre><code>y=(x-MinValue)/(MaxValue-MinValue)　　

说明：x、y分别为转换前、后的值，MaxValue、MinValue分别为样本的最大值和最小值。　　</code></pre><p>2) 对数函数转换，表达式如下：　　</p>
<pre><code>y=log10(x)　　

说明：以10为底的对数函数转换。

如图：

![对数函数图像](http://data.apachecn.org/img/AiLearning/ml/2.KNN/knn_1.png)</code></pre><p>3) 反余切函数转换，表达式如下：</p>
<pre><code>y=arctan(x)*2/PI　

如图：

![反余切函数图像](http://data.apachecn.org/img/AiLearning/ml/2.KNN/arctan_arccot.gif)</code></pre><p>4) 式(1)将输入值换算为[-1,1]区间的值，在输出层用式(2)换算回初始值，其中和分别表示训练样本集中负荷的最大值和最小值。　</p>
<p>在统计学中，归一化的具体作用是归纳统一样本的统计分布性。归一化在0-1之间是统计的概率分布，归一化在-1–+1之间是统计的坐标分布。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoNorm</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        归一化特征值，消除特征之间量级不同导致的影响</span></span><br><span class="line"><span class="string">    parameter:</span></span><br><span class="line"><span class="string">        dataSet: 数据集</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        归一化后的数据集 normDataSet. ranges和minVals即最小值与范围，并没有用到</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    归一化公式：</span></span><br><span class="line"><span class="string">        Y = (X-Xmin)/(Xmax-Xmin)</span></span><br><span class="line"><span class="string">        其中的 min 和 max 分别是数据集中的最小特征值和最大特征值。该函数可以自动将数字特征值转化为0到1的区间。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 计算每种属性的最大值、最小值、范围</span></span><br><span class="line">    minVals = dataSet.min(<span class="number">0</span>)</span><br><span class="line">    maxVals = dataSet.max(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 极差</span></span><br><span class="line">    ranges = maxVals - minVals</span><br><span class="line">    normDataSet = zeros(shape(dataSet))</span><br><span class="line">    m = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 生成与最小值之差组成的矩阵</span></span><br><span class="line">    normDataSet = dataSet - tile(minVals, (m, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 将最小值之差除以范围组成矩阵</span></span><br><span class="line">    normDataSet = normDataSet / tile(ranges, (m, <span class="number">1</span>))  <span class="comment"># element wise divide</span></span><br><span class="line">    <span class="keyword">return</span> normDataSet, ranges, minVals</span><br></pre></td></tr></table></figure>

<blockquote>
<p>训练算法：此步骤不适用于 k-近邻算法</p>
</blockquote>
<p>因为测试数据每一次都要与全量的训练数据进行比较，所以这个过程是没有必要的。</p>
<p>kNN 算法伪代码：</p>
<pre><code>对于每一个在数据集中的数据点：
    计算目标的数据点（需要分类的数据点）与该数据点的距离
    将距离排序：从小到大
    选取前K个最短距离
    选取这K个中最多的分类类别
    返回该类别来作为目标数据点的预测值</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataSet, labels, k)</span>:</span></span><br><span class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">#距离度量 度量公式为欧氏距离</span></span><br><span class="line">    diffMat = tile(inX, (dataSetSize,<span class="number">1</span>)) – dataSet</span><br><span class="line">    sqDiffMat = diffMat**<span class="number">2</span></span><br><span class="line">    sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">    distances = sqDistances**<span class="number">0.5</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#将距离排序：从小到大</span></span><br><span class="line">    sortedDistIndicies = distances.argsort()</span><br><span class="line">    <span class="comment">#选取前K个最短距离， 选取这K个中最多的分类类别</span></span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k)：</span><br><span class="line">        voteIlabel = labels[sortedDistIndicies[i]]</span><br><span class="line">        classCount[voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>) + <span class="number">1</span> </span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>


<blockquote>
<p>测试算法：使用海伦提供的部分数据作为测试样本。如果预测分类与实际类别不同，则标记为一个错误。</p>
</blockquote>
<p>kNN 分类器针对约会网站的测试代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">datingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对约会网站的测试方法</span></span><br><span class="line"><span class="string">    parameters:</span></span><br><span class="line"><span class="string">        none</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        错误数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 设置测试数据的的一个比例（训练数据集比例=1-hoRatio）</span></span><br><span class="line">    hoRatio = <span class="number">0.1</span>  <span class="comment"># 测试范围,一部分测试一部分作为样本</span></span><br><span class="line">    <span class="comment"># 从文件中加载数据</span></span><br><span class="line">    datingDataMat, datingLabels = file2matrix(<span class="string">'data/2.KNN/datingTestSet2.txt'</span>)  <span class="comment"># load data setfrom file</span></span><br><span class="line">    <span class="comment"># 归一化数据</span></span><br><span class="line">    normMat, ranges, minVals = autoNorm(datingDataMat)</span><br><span class="line">    <span class="comment"># m 表示数据的行数，即矩阵的第一维</span></span><br><span class="line">    m = normMat.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 设置测试的样本数量， numTestVecs:m表示训练样本的数量</span></span><br><span class="line">    numTestVecs = int(m * hoRatio)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'numTestVecs='</span>, numTestVecs</span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTestVecs):</span><br><span class="line">        <span class="comment"># 对数据测试</span></span><br><span class="line">        classifierResult = classify0(normMat[i, :], normMat[numTestVecs:m, :], datingLabels[numTestVecs:m], <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"the classifier came back with: %d, the real answer is: %d"</span> % (classifierResult, datingLabels[i])</span><br><span class="line">        <span class="keyword">if</span> (classifierResult != datingLabels[i]): errorCount += <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"the total error rate is: %f"</span> % (errorCount / float(numTestVecs))</span><br><span class="line">    <span class="keyword">print</span> errorCount</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型。</p>
</blockquote>
<p>约会网站预测函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyPerson</span><span class="params">()</span>:</span></span><br><span class="line">    resultList = [<span class="string">'not at all'</span>, <span class="string">'in small doses'</span>, <span class="string">'in large doses'</span>]</span><br><span class="line">    percentTats = float(raw_input(<span class="string">"percentage of time spent playing video games ?"</span>))</span><br><span class="line">    ffMiles = float(raw_input(<span class="string">"frequent filer miles earned per year?"</span>))</span><br><span class="line">    iceCream = float(raw_input(<span class="string">"liters of ice cream consumed per year?"</span>))</span><br><span class="line">    datingDataMat, datingLabels = file2matrix(<span class="string">'datingTestSet2.txt'</span>)</span><br><span class="line">    normMat, ranges, minVals = autoNorm(datingDataMat)</span><br><span class="line">    inArr = array([ffMiles, percentTats, iceCream])</span><br><span class="line">    classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"You will probably like this person: "</span>, resultList[classifierResult - <span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>实际运行效果如下: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>classifyPerson()</span><br><span class="line">percentage of time spent playing video games?10</span><br><span class="line">frequent flier miles earned per year?10000</span><br><span class="line">liters of ice cream consumed per year?0.5</span><br><span class="line">You will probably like this person: <span class="keyword">in</span> small doses</span><br></pre></td></tr></table></figure>



<h3 id="项目案例2-手写数字识别系统"><a href="#项目案例2-手写数字识别系统" class="headerlink" title="项目案例2: 手写数字识别系统"></a>项目案例2: 手写数字识别系统</h3><p><a href="/src/py2.x/ml/2.KNN/kNN.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/2.KNN/kNN.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/2.KNN/kNN.py</a></p>
<h4 id="项目概述-1"><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4><p>构造一个能识别数字 0 到 9 的基于 KNN 分类器的手写数字识别系统。</p>
<p>需要识别的数字是存储在文本文件中的具有相同的色彩和大小：宽高是 32 像素 * 32 像素的黑白图像。</p>
<h4 id="开发流程-1"><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight less"><table><tr><td class="code"><pre><span class="line">收集数据：提供文本文件。</span><br><span class="line">准备数据：编写函数 <span class="selector-tag">img2vector</span>(), 将图像格式转换为分类器使用的向量格式</span><br><span class="line">分析数据：在 <span class="selector-tag">Python</span> 命令提示符中检查数据，确保它符合要求</span><br><span class="line">训练算法：此步骤不适用于 <span class="selector-tag">KNN</span></span><br><span class="line">测试算法：编写函数使用提供的部分数据集作为测试样本，测试样本与非测试样本的</span><br><span class="line">         区别在于测试样本是已经完成分类的数据，如果预测分类与实际类别不同，</span><br><span class="line">         则标记为一个错误</span><br><span class="line">使用算法：本例没有完成此步骤，若你感兴趣可以构建完整的应用程序，从图像中提取</span><br><span class="line">         数字，并完成数字识别，美国的邮件分拣系统就是一个实际运行的类似系统</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 提供文本文件</p>
</blockquote>
<p>目录 <a href="/data/2.KNN/trainingDigits">trainingDigits</a> 中包含了大约 2000 个例子，每个例子内容如下图所示，每个数字大约有 200 个样本；目录 <a href="/data/2.KNN/testDigits">testDigits</a> 中包含了大约 900 个测试数据。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/2.KNN/knn_2_handWriting.png" alt="手写数字数据集的例子"></p>
<blockquote>
<p>准备数据: 编写函数 img2vector(), 将图像文本数据转换为分类器使用的向量</p>
</blockquote>
<p>将图像文本数据转换为向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span></span><br><span class="line">    returnVect = zeros((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">        lineStr = fr.readline()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">            returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j] = int(lineStr[j])</span><br><span class="line">    <span class="keyword">return</span> returnVect</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据：在 Python 命令提示符中检查数据，确保它符合要求</p>
</blockquote>
<p>在 Python 命令行中输入下列命令测试 img2vector 函数，然后与文本编辑器打开的文件进行比较: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>testVector = kNN.img2vector(<span class="string">'testDigits/0_13.txt'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>testVector[<span class="number">0</span>,<span class="number">0</span>:<span class="number">32</span>]</span><br><span class="line">array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>testVector[<span class="number">0</span>,<span class="number">32</span>:<span class="number">64</span>]</span><br><span class="line">array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>训练算法：此步骤不适用于 KNN</p>
</blockquote>
<p>因为测试数据每一次都要与全量的训练数据进行比较，所以这个过程是没有必要的。</p>
<blockquote>
<p>测试算法：编写函数使用提供的部分数据集作为测试样本，如果预测分类与实际类别不同，则标记为一个错误</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 1. 导入训练数据</span></span><br><span class="line">    hwLabels = []</span><br><span class="line">    trainingFileList = listdir(<span class="string">'data/2.KNN/trainingDigits'</span>)  <span class="comment"># load the training set</span></span><br><span class="line">    m = len(trainingFileList)</span><br><span class="line">    trainingMat = zeros((m, <span class="number">1024</span>))</span><br><span class="line">    <span class="comment"># hwLabels存储0～9对应的index位置， trainingMat存放的每个位置对应的图片向量</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        fileNameStr = trainingFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]  <span class="comment"># take off .txt</span></span><br><span class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        hwLabels.append(classNumStr)</span><br><span class="line">        <span class="comment"># 将 32*32的矩阵-&gt;1*1024的矩阵</span></span><br><span class="line">        trainingMat[i, :] = img2vector(<span class="string">'data/2.KNN/trainingDigits/%s'</span> % fileNameStr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 导入测试数据</span></span><br><span class="line">    testFileList = listdir(<span class="string">'data/2.KNN/testDigits'</span>)  <span class="comment"># iterate through the test set</span></span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    mTest = len(testFileList)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(mTest):</span><br><span class="line">        fileNameStr = testFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]  <span class="comment"># take off .txt</span></span><br><span class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        vectorUnderTest = img2vector(<span class="string">'data/2.KNN/testDigits/%s'</span> % fileNameStr)</span><br><span class="line">        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"the classifier came back with: %d, the real answer is: %d"</span> % (classifierResult, classNumStr)</span><br><span class="line">        <span class="keyword">if</span> (classifierResult != classNumStr): errorCount += <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"\nthe total number of errors is: %d"</span> % errorCount</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"\nthe total error rate is: %f"</span> % (errorCount / float(mTest))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法：本例没有完成此步骤，若你感兴趣可以构建完整的应用程序，从图像中提取数字，并完成数字识别，美国的邮件分拣系统就是一个实际运行的类似系统。</p>
</blockquote>
<h2 id="KNN-小结"><a href="#KNN-小结" class="headerlink" title="KNN 小结"></a>KNN 小结</h2><p>KNN 是什么？定义： 监督学习？ 非监督学习？</p>
<p>KNN 是一个简单的无显示学习过程，非泛化学习的监督学习模型。在分类和回归中均有应用。</p>
<h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>简单来说： 通过距离度量来计算查询点（query point）与每个训练数据点的距离，然后选出与查询点（query point）相近的K个最邻点（K nearest neighbors），使用分类决策来选出对应的标签来作为该查询点的标签。</p>
<h3 id="KNN-三要素"><a href="#KNN-三要素" class="headerlink" title="KNN 三要素"></a>KNN 三要素</h3><blockquote>
<p>K, K的取值</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>对查询点标签影响显著（效果拔群）。k值小的时候 近似误差小，估计误差大。 k值大 近似误差大，估计误差小。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>如果选择较小的 k 值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差（approximation error）会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是“学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k 值的减小就意味着整体模型变得复杂，容易发生过拟合。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>如果选择较大的 k 值，就相当于用较大的邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。 k 值的增大就意味着整体的模型变得简单。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>太大太小都不太好，可以用交叉验证（cross validation）来选取适合的k值。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>近似误差和估计误差，请看这里：<a href="https://www.zhihu.com/question/60793482" target="_blank" rel="noopener">https://www.zhihu.com/question/60793482</a></p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>距离度量 Metric/Distance Measure </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>距离度量 通常为 欧式距离（Euclidean distance），还可以是 Minkowski 距离 或者 曼哈顿距离。也可以是 地理空间中的一些距离公式。（更多细节可以参看 sklearn 中 valid_metric 部分）</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>分类决策 （decision rule）</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>分类决策 在 分类问题中 通常为通过少数服从多数 来选取票数最多的标签，在回归问题中通常为 K个最邻点的标签的平均值。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<h3 id="算法：（sklearn-上有三种）"><a href="#算法：（sklearn-上有三种）" class="headerlink" title="算法：（sklearn 上有三种）"></a>算法：（sklearn 上有三种）</h3><blockquote>
<p>Brute Force 暴力计算/线性扫描 </p>
</blockquote>
<blockquote>
<p>KD Tree 使用二叉树根据数据维度来平分参数空间。</p>
</blockquote>
<blockquote>
<p>Ball Tree 使用一系列的超球体来平分训练数据集。</p>
</blockquote>
<blockquote>
<p>树结构的算法都有建树和查询两个过程。Brute Force 没有建树的过程。</p>
</blockquote>
<blockquote>
<p>算法特点：   </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>优点： High Accuracy， No Assumption on data， not sensitive to outliers</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>缺点：时间和空间复杂度 高</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>适用范围： continuous values and nominal values</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>相似同源产物： </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>radius neighbors 根据制定的半径来找寻邻点</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>影响算法因素：</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>N 数据集样本数量(number of samples)， D 数据维度 (number of features)</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>总消耗：</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Brute Force:  O[DN^2] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>此处考虑的是最蠢的方法：把所有训练的点之间的距离都算一遍。当然有更快的实现方式, 比如 O(ND + kN)  和  O(NDK) , 最快的是 O[DN] 。感兴趣的可以阅读这个链接： <a href="https://stats.stackexchange.com/questions/219655/k-nn-computational-complexity" target="_blank" rel="noopener">k-NN computational complexity</a></p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>KD Tree: O[DN log(N)] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Ball Tree: O[DN log(N)] 跟 KD Tree 处于相同的数量级，虽然建树时间会比 KD Tree 久一点，但是在高结构的数据，甚至是高纬度的数据中，查询速度有很大的提升。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>查询所需消耗:</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Brute Force:  O[DN] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>KD Tree: 当维度比较小的时候， 比如 D&lt;20,  O[Dlog(N)] 。相反，将会趋向于 O[DN] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Ball Tree: O[Dlog(N)] </p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>当数据集比较小的时候，比如 N&lt;30的时候，Brute Force 更有优势。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>Intrinsic Dimensionality(本征维数) 和 Sparsity（稀疏度）</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>数据的 intrinsic dimensionality 是指数据所在的流形的维数 d &lt; D , 在参数空间可以是线性或非线性的。稀疏度指的是数据填充参数空间的程度(这与“稀疏”矩阵中使用的概念不同, 数据矩阵可能没有零项, 但是从这个意义上来讲,它的结构 仍然是 “稀疏” 的)。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Brute Force 的查询时间不受影响。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>对于 KD Tree 和 Ball Tree的查询时间, 较小本征维数且更稀疏的数据集的查询时间更快。KD Tree 的改善由于通过坐标轴来平分参数空间的自身特性 没有Ball Tree 显著。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>k的取值 (k 个邻点)</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>Brute Force 的查询时间基本不受影响。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>但是对于 KD Tree 和 Ball Tree , k越大，查询时间越慢。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>k 在N的占比较大的时候，使用 Brute Force 比较好。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>Number of Query Points （查询点数量， 即测试数据的数量）</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>查询点较少的时候用Brute Force。查询点较多的时候可以使用树结构算法。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>关于 sklearn 中模型的一些额外干货：</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>如果KD Tree，Ball Tree 和Brute Force 应用场景傻傻分不清楚，可以直接使用 含有algorithm=’auto’的模组。 algorithm=’auto’ 自动为您选择最优算法。<br>有 regressor 和 classifier 可以来选择。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>metric/distance measure 可以选择。 另外距离 可以通过weight 来加权。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>leaf size 对KD Tree 和 Ball Tree 的影响</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>建树时间：leaf size 比较大的时候，建树时间也就快点。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>查询时间： leaf size 太大太小都不太好。如果leaf size 趋向于 N（训练数据的样本数量），算法其实就是 brute force了。如果leaf size 太小了，趋向于1，那查询的时候 遍历树的时间就会大大增加。leaf size 建议的数值是 30，也就是默认值。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>内存： leaf size 变大，存树结构的内存变小。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>Nearest Centroid Classifier</p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>分类决策是哪个标签的质心与测试点最近，就选哪个标签。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>该模型假设在所有维度中方差相同。 是一个很好的base line。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<p>进阶版： Nearest Shrunken Centroid </p>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>可以通过shrink_threshold来设置。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>作用： 可以移除某些影响分类的特征，例如移除噪音特征的影响</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第3章 决策树</title>
    <url>/2020/06/01/3.%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<h2 id="决策树-概述"><a href="#决策树-概述" class="headerlink" title="决策树 概述"></a>决策树 概述</h2><p><code>决策树（Decision Tree）算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树。</code></p>
<p><code>决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。</code></p>
<p><code>决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。</code></p>
<a id="more"></a>
<h2 id="决策树-场景"><a href="#决策树-场景" class="headerlink" title="决策树 场景"></a>决策树 场景</h2><p>一个叫做 “二十个问题” 的游戏，游戏的规则很简单：参与游戏的一方在脑海中想某个事物，其他参与者向他提问，只允许提 20 个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围，最后得到游戏的答案。</p>
<p>一个邮件分类系统，大致工作流程如下：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/%E5%86%B3%E7%AD%96%E6%A0%91-%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg" alt="决策树-流程图" title="决策树示例流程图"></p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">首先检测发送邮件域名地址。如果地址为 myEmployer<span class="selector-class">.com</span>, 则将其放在分类 <span class="string">"无聊时需要阅读的邮件"</span>中。</span><br><span class="line">如果邮件不是来自这个域名，则检测邮件内容里是否包含单词 <span class="string">"曲棍球"</span> , 如果包含则将邮件归类到 <span class="string">"需要及时处理的朋友邮件"</span>, </span><br><span class="line">如果不包含则将邮件归类到 <span class="string">"无需阅读的垃圾邮件"</span> 。</span><br></pre></td></tr></table></figure>

<p>决策树的定义：</p>
<p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。</p>
<p>用决策树对需要测试的实例进行分类：从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。</p>
<h2 id="决策树-原理"><a href="#决策树-原理" class="headerlink" title="决策树 原理"></a>决策树 原理</h2><h3 id="决策树-须知概念"><a href="#决策树-须知概念" class="headerlink" title="决策树 须知概念"></a>决策树 须知概念</h3><h4 id="信息熵-amp-信息增益"><a href="#信息熵-amp-信息增益" class="headerlink" title="信息熵 &amp; 信息增益"></a>信息熵 &amp; 信息增益</h4><p>熵（entropy）：<br>熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。</p>
<p>信息论（information theory）中的熵（香农熵）：<br>是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。</p>
<p>信息增益（information gain）：<br>在划分数据集前后信息发生的变化称为信息增益。</p>
<h3 id="决策树-工作原理"><a href="#决策树-工作原理" class="headerlink" title="决策树 工作原理"></a>决策树 工作原理</h3><p>如何构造一个决策树?<br/><br>我们使用 createBranch() 方法，如下所示：</p>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line">def createBranch():</span><br><span class="line">'''</span><br><span class="line">此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。</span><br><span class="line">'''</span><br><span class="line"><span class="code">    检测数据集中的所有数据的分类标签是否相同:</span></span><br><span class="line"><span class="code">        If so return 类标签</span></span><br><span class="line"><span class="code">        Else:</span></span><br><span class="line"><span class="code">            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）</span></span><br><span class="line"><span class="code">            划分数据集</span></span><br><span class="line"><span class="code">            创建分支节点</span></span><br><span class="line"><span class="code">                for 每个划分的子集</span></span><br><span class="line"><span class="code">                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中</span></span><br><span class="line"><span class="code">            return 分支节点</span></span><br></pre></td></tr></table></figure>

<h3 id="决策树-开发流程"><a href="#决策树-开发流程" class="headerlink" title="决策树 开发流程"></a>决策树 开发流程</h3><figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">收集数据：可以使用任何方法。</span><br><span class="line">准备数据：树构造算法 <span class="comment">(这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)</span></span><br><span class="line">分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。</span><br><span class="line">训练算法：构造树的数据结构。</span><br><span class="line">测试算法：使用训练好的树计算错误率。</span><br><span class="line">使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。</span><br></pre></td></tr></table></figure>

<h3 id="决策树-算法特点"><a href="#决策树-算法特点" class="headerlink" title="决策树 算法特点"></a>决策树 算法特点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">优点：计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。</span><br><span class="line">缺点：容易过拟合。</span><br><span class="line">适用数据类型：数值型和标称型。</span><br></pre></td></tr></table></figure>

<h2 id="决策树-项目案例"><a href="#决策树-项目案例" class="headerlink" title="决策树 项目案例"></a>决策树 项目案例</h2><h3 id="项目案例1-判定鱼类和非鱼类"><a href="#项目案例1-判定鱼类和非鱼类" class="headerlink" title="项目案例1: 判定鱼类和非鱼类"></a>项目案例1: 判定鱼类和非鱼类</h3><h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p>根据以下 2 个特征，将动物分成两类：鱼类和非鱼类。</p>
<p>特征：</p>
<ol>
<li>不浮出水面是否可以生存</li>
<li>是否有脚蹼</li>
</ol>
<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><p><a href="/src/py2.x/ml/3.DecisionTree/DecisionTree.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">收集数据：可以使用任何方法</span><br><span class="line">准备数据：树构造算法（这里使用的是ID3算法，因此数值型数据必须离散化。）</span><br><span class="line">分析数据：可以使用任何方法，构造树完成之后，我们可以将树画出来。</span><br><span class="line">训练算法：构造树结构</span><br><span class="line">测试算法：使用习得的决策树执行分类</span><br><span class="line">使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据：可以使用任何方法</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/DT_%E6%B5%B7%E6%B4%8B%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE.png" alt="海洋生物数据"></p>
<p>我们利用 createDataSet() 函数输入数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">            [<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">            [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'no'</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>, <span class="string">'flippers'</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br></pre></td></tr></table></figure>
<blockquote>
<p>准备数据：树构造算法</p>
</blockquote>
<p>此处，由于我们输入的数据本身就是离散化数据，所以这一步就省略了。</p>
<blockquote>
<p>分析数据：可以使用任何方法，构造树完成之后，我们可以将树画出来。</p>
</blockquote>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/3.DecisionTree/%E7%86%B5%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.jpg" alt="熵的计算公式"></p>
<p>计算给定数据集的香农熵的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="comment"># 求list的长度，表示计算参与训练的数据量</span></span><br><span class="line">    numEntries = len(dataSet)</span><br><span class="line">    <span class="comment"># 计算分类标签label出现的次数</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="comment"># the the number of unique elements and their occurrence</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 将当前实例的标签存储，即每一行数据的最后一个数据代表的是标签</span></span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># 为所有可能的分类创建字典，如果当前的键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对于 label 标签的占比，求出 label 标签的香农熵</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        <span class="comment"># 使用所有类标签的发生频率计算类别出现的概率。</span></span><br><span class="line">        prob = float(labelCounts[key])/numEntries</span><br><span class="line">        <span class="comment"># 计算香农熵，以 2 为底求对数</span></span><br><span class="line">        shannonEnt -= prob * log(prob, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure>

<p>按照给定特征划分数据集</p>
<p><code>将指定特征的特征值等于 value 的行剩下列作为子数据集。</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, index, value)</span>:</span></span><br><span class="line">    <span class="string">"""splitDataSet(通过遍历dataSet数据集，求出index对应的colnum列的值为value的行)</span></span><br><span class="line"><span class="string">        就是依据index列进行分类，如果index列的数据等于 value的时候，就要将 index 划分到我们创建的新的数据集中</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 数据集                 待划分的数据集</span></span><br><span class="line"><span class="string">        index 表示每一行的index列        划分数据集的特征</span></span><br><span class="line"><span class="string">        value 表示index列对应的value值   需要返回的特征的值。</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        index列为value的数据集【该数据集需要排除index列】</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet: </span><br><span class="line">        <span class="comment"># index列为value的数据集【该数据集需要排除index列】</span></span><br><span class="line">        <span class="comment"># 判断index列的值是否为value</span></span><br><span class="line">        <span class="keyword">if</span> featVec[index] == value:</span><br><span class="line">            <span class="comment"># chop out index used for splitting</span></span><br><span class="line">            <span class="comment"># [:index]表示前index行，即若 index 为2，就是取 featVec 的前 index 行</span></span><br><span class="line">            reducedFeatVec = featVec[:index]</span><br><span class="line">            <span class="string">'''</span></span><br><span class="line"><span class="string">            请百度查询一下： extend和append的区别</span></span><br><span class="line"><span class="string">            music_media.append(object) 向列表中添加一个对象object</span></span><br><span class="line"><span class="string">            music_media.extend(sequence) 把一个序列seq的内容添加到列表中 (跟 += 在list运用类似， music_media += sequence)</span></span><br><span class="line"><span class="string">            1、使用append的时候，是将object看作一个对象，整体打包添加到music_media对象中。</span></span><br><span class="line"><span class="string">            2、使用extend的时候，是将sequence看作一个序列，将这个序列和music_media序列合并，并放在其后面。</span></span><br><span class="line"><span class="string">            music_media = []</span></span><br><span class="line"><span class="string">            music_media.extend([1,2,3])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果：</span></span><br><span class="line"><span class="string">            #[1, 2, 3]</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            music_media.append([4,5,6])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果：</span></span><br><span class="line"><span class="string">            #[1, 2, 3, [4, 5, 6]]</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            music_media.extend([7,8,9])</span></span><br><span class="line"><span class="string">            print music_media</span></span><br><span class="line"><span class="string">            #结果：</span></span><br><span class="line"><span class="string">            #[1, 2, 3, [4, 5, 6], 7, 8, 9]</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line">            reducedFeatVec.extend(featVec[index+<span class="number">1</span>:])</span><br><span class="line">            <span class="comment"># [index+1:]表示从跳过 index 的 index+1行，取接下来的数据</span></span><br><span class="line">            <span class="comment"># 收集结果值 index列为value的行【该行需要排除index列】</span></span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br></pre></td></tr></table></figure>

<p>选择最好的数据集划分方式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""chooseBestFeatureToSplit(选择最好的特征)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dataSet 数据集</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bestFeature 最优的特征列</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 求第一行有多少列的 Feature, 最后一列是label列嘛</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    <span class="comment"># 数据集的原始信息熵</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    <span class="comment"># 最优的信息增益值, 和最优的Featurn编号</span></span><br><span class="line">    bestInfoGain, bestFeature = <span class="number">0.0</span>, <span class="number">-1</span></span><br><span class="line">    <span class="comment"># iterate over all the features</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">        <span class="comment"># create a list of all the examples of this feature</span></span><br><span class="line">        <span class="comment"># 获取对应的feature下的所有数据</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        <span class="comment"># get a set of unique values</span></span><br><span class="line">        <span class="comment"># 获取剔重后的集合，使用set对list数据进行去重</span></span><br><span class="line">        uniqueVals = set(featList)</span><br><span class="line">        <span class="comment"># 创建一个临时的信息熵</span></span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 遍历某一列的value集合，计算该列的信息熵 </span></span><br><span class="line">        <span class="comment"># 遍历当前特征中的所有唯一属性值，对每个唯一属性值划分一次数据集，计算数据集的新熵值，并对所有唯一特征值得到的熵求和。</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            <span class="comment"># 计算概率</span></span><br><span class="line">            prob = len(subDataSet)/float(len(dataSet))</span><br><span class="line">            <span class="comment"># 计算信息熵</span></span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">        <span class="comment"># gain[信息增益]: 划分数据集前后的信息变化， 获取信息熵最大的值</span></span><br><span class="line">        <span class="comment"># 信息增益是熵的减少或者是数据无序度的减少。最后，比较所有特征中的信息增益，返回最好特征划分的索引值。</span></span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'infoGain='</span>, infoGain, <span class="string">'bestFeature='</span>, i, baseEntropy, newEntropy</span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure>

<figure class="highlight haxe"><table><tr><td class="code"><pre><span class="line">问：上面的 <span class="keyword">new</span><span class="type">Entropy</span> 为什么是根据子集计算的呢？</span><br><span class="line">答：因为我们在根据一个特征计算香农熵的时候，该特征的分类值是相同，这个特征这个分类的香农熵为 <span class="number">0</span>；</span><br><span class="line">这就是为什么计算新的香农熵的时候使用的是子集。</span><br></pre></td></tr></table></figure>

<blockquote>
<p>训练算法：构造树的数据结构</p>
</blockquote>
<p>创建树的函数代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, labels)</span>:</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    <span class="comment"># 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行</span></span><br><span class="line">    <span class="comment"># 第一个停止条件：所有的类标签完全相同，则直接返回该类标签。</span></span><br><span class="line">    <span class="comment"># count() 函数是统计括号中的值在list中出现的次数</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果</span></span><br><span class="line">    <span class="comment"># 第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。</span></span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择最优的列，得到最优列对应的label含义</span></span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    <span class="comment"># 获取label的名称</span></span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    <span class="comment"># 初始化myTree</span></span><br><span class="line">    myTree = &#123;bestFeatLabel: &#123;&#125;&#125;</span><br><span class="line">    <span class="comment"># 注：labels列表是可变对象，在PYTHON函数中作为参数时传址引用，能够被全局修改</span></span><br><span class="line">    <span class="comment"># 所以这行代码导致函数外的同名变量被删除了元素，造成例句无法执行，提示'no surfacing' is not in list</span></span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    <span class="comment"># 取出最优列，然后它的branch做分类</span></span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        <span class="comment"># 求出剩余的标签label</span></span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        <span class="comment"># 遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree()</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)</span><br><span class="line">        <span class="comment"># print 'myTree', value, myTree</span></span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法：使用决策树执行分类</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree, featLabels, testVec)</span>:</span></span><br><span class="line">    <span class="string">"""classify(给输入的节点，进行分类)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        inputTree  决策树模型</span></span><br><span class="line"><span class="string">        featLabels Feature标签对应的名称</span></span><br><span class="line"><span class="string">        testVec    测试输入的数据</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        classLabel 分类的结果值，需要映射label才能知道名称</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 获取tree的根节点对于的key值</span></span><br><span class="line">    firstStr = list(inputTree.keys())[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 通过key得到根节点对应的value</span></span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    <span class="comment"># 判断根节点名称获取根节点在label中的先后顺序，这样就知道输入的testVec怎么开始对照树来做分类</span></span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    <span class="comment"># 测试数据，找到根节点对应的label位置，也就知道从输入的数据的第几位来开始分类</span></span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'+++'</span>, firstStr, <span class="string">'xxx'</span>, secondDict, <span class="string">'---'</span>, key, <span class="string">'&gt;&gt;&gt;'</span>, valueOfFeat</span><br><span class="line">    <span class="comment"># 判断分枝是否结束: 判断valueOfFeat是否是dict类型</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(valueOfFeat, dict):</span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。</p>
</blockquote>
<h3 id="项目案例2-使用决策树预测隐形眼镜类型"><a href="#项目案例2-使用决策树预测隐形眼镜类型" class="headerlink" title="项目案例2: 使用决策树预测隐形眼镜类型"></a>项目案例2: 使用决策树预测隐形眼镜类型</h3><p><a href="/src/py2.x/ml/3.DecisionTree/DecisionTree.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/3.DecisionTree/DecisionTree.py</a></p>
<h4 id="项目概述-1"><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4><p>隐形眼镜类型包括硬材质、软材质以及不适合佩戴隐形眼镜。我们需要使用决策树预测患者需要佩戴的隐形眼镜类型。</p>
<h4 id="开发流程-1"><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4><ol>
<li>收集数据: 提供的文本文件。</li>
<li>解析数据: 解析 tab 键分隔的数据行</li>
<li>分析数据: 快速检查数据，确保正确地解析数据内容，使用 createPlot() 函数绘制最终的树形图。</li>
<li>训练算法: 使用 createTree() 函数。</li>
<li>测试算法: 编写测试函数验证决策树可以正确分类给定的数据实例。</li>
<li>使用算法: 存储树的数据结构，以便下次使用时无需重新构造树。</li>
</ol>
<blockquote>
<p>收集数据：提供的文本文件</p>
</blockquote>
<p>文本文件数据格式如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">young</span>	<span class="string">myope</span>	<span class="literal">no</span>	<span class="string">reduced</span>	<span class="literal">no</span> <span class="string">lenses</span></span><br><span class="line"><span class="string">pre</span>	<span class="string">myope</span>	<span class="literal">no</span>	<span class="string">reduced</span>	<span class="literal">no</span> <span class="string">lenses</span></span><br><span class="line"><span class="string">presbyopic</span>	<span class="string">myope</span>	<span class="literal">no</span>	<span class="string">reduced</span>	<span class="literal">no</span> <span class="string">lenses</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>解析数据：解析 tab 键分隔的数据行</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lecses = [inst.strip().split(<span class="string">'\t'</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readlines()]</span><br><span class="line">lensesLabels = [<span class="string">'age'</span>, <span class="string">'prescript'</span>, <span class="string">'astigmatic'</span>, <span class="string">'tearRate'</span>]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据：快速检查数据，确保正确地解析数据内容，使用 createPlot() 函数绘制最终的树形图。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>treePlotter.createPlot(lensesTree)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>训练算法：使用 createTree() 函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>lensesTree = trees.createTree(lenses, lensesLabels)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lensesTree</span><br><span class="line">&#123;<span class="string">'tearRate'</span>: &#123;<span class="string">'reduced'</span>: <span class="string">'no lenses'</span>, <span class="string">'normal'</span>: &#123;<span class="string">'astigmatic'</span>:&#123;<span class="string">'yes'</span>:</span><br><span class="line">&#123;<span class="string">'prescript'</span>:&#123;<span class="string">'hyper'</span>:&#123;<span class="string">'age'</span>:&#123;<span class="string">'pre'</span>:<span class="string">'no lenses'</span>, <span class="string">'presbyopic'</span>:</span><br><span class="line"><span class="string">'no lenses'</span>, <span class="string">'young'</span>:<span class="string">'hard'</span>&#125;&#125;, <span class="string">'myope'</span>:<span class="string">'hard'</span>&#125;&#125;, <span class="string">'no'</span>:&#123;<span class="string">'age'</span>:&#123;<span class="string">'pre'</span>:</span><br><span class="line"><span class="string">'soft'</span>, <span class="string">'presbyopic'</span>:&#123;<span class="string">'prescript'</span>: &#123;<span class="string">'hyper'</span>:<span class="string">'soft'</span>, <span class="string">'myope'</span>:</span><br><span class="line"><span class="string">'no lenses'</span>&#125;&#125;, <span class="string">'young'</span>:<span class="string">'soft'</span>&#125;&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 编写测试函数验证决策树可以正确分类给定的数据实例。</p>
</blockquote>
<blockquote>
<p>使用算法: 存储树的数据结构，以便下次使用时无需重新构造树。</p>
</blockquote>
<p>使用 pickle 模块存储决策树</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree, filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = open(filename, <span class="string">'wb'</span>)</span><br><span class="line">    pickle.dump(inputTree, fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = open(filename, <span class="string">'rb'</span>)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure>


<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a>      </li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第5章 Logistic回归</title>
    <url>/2020/06/01/5.Logistic%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h2 id="Logistic-回归-概述"><a href="#Logistic-回归-概述" class="headerlink" title="Logistic 回归 概述"></a>Logistic 回归 概述</h2><p><code>Logistic 回归 或者叫逻辑回归 虽然名字有回归，但是它是用来做分类的。其主要思想是: 根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类。</code></p>
<a id="more"></a>
<h2 id="须知概念"><a href="#须知概念" class="headerlink" title="须知概念"></a>须知概念</h2><h3 id="Sigmoid-函数"><a href="#Sigmoid-函数" class="headerlink" title="Sigmoid 函数"></a>Sigmoid 函数</h3><h4 id="回归-概念"><a href="#回归-概念" class="headerlink" title="回归 概念"></a>回归 概念</h4><p>假设现在有一些数据点，我们用一条直线对这些点进行拟合（这条直线称为最佳拟合直线），这个拟合的过程就叫做回归。进而可以得到对这些点的拟合直线方程，那么我们根据这个回归方程，怎么进行分类呢？请看下面。</p>
<h4 id="二值型输出分类函数"><a href="#二值型输出分类函数" class="headerlink" title="二值型输出分类函数"></a>二值型输出分类函数</h4><p>我们想要的函数应该是: 能接受所有的输入然后预测出类别。例如，在两个类的情况下，上述函数输出 0 或 1.或许你之前接触过具有这种性质的函数，该函数称为 <code>海维塞得阶跃函数(Heaviside step function)</code>，或者直接称为 <code>单位阶跃函数</code>。然而，海维塞得阶跃函数的问题在于: 该函数在跳跃点上从 0 瞬间跳跃到 1，这个瞬间跳跃过程有时很难处理。幸好，另一个函数也有类似的性质（可以输出 0 或者 1 的性质），且数学上更易处理，这就是 Sigmoid 函数。 Sigmoid 函数具体的计算公式如下: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_1.png" alt="Sigmoid 函数计算公式"></p>
<p>下图给出了 Sigmoid 函数在不同坐标尺度下的两条曲线图。当 x 为 0 时，Sigmoid 函数值为 0.5 。随着 x 的增大，对应的 Sigmoid 值将逼近于 1 ; 而随着 x 的减小， Sigmoid 值将逼近于 0 。如果横坐标刻度足够大， Sigmoid 函数看起来很像一个阶跃函数。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_3.png" alt="Sigmoid 函数在不同坐标下的图片"></p>
<p>因此，为了实现 Logistic 回归分类器，我们可以在每个特征上都乘以一个回归系数（如下公式所示），然后把所有结果值相加，将这个总和代入 Sigmoid 函数中，进而得到一个范围在 0~1 之间的数值。任何大于 0.5 的数据被分入 1 类，小于 0.5 即被归入 0 类。所以，Logistic 回归也是一种概率估计，比如这里Sigmoid 函数得出的值为0.5，可以理解为给定数据和参数，数据被分入 1 类的概率为0.5。想对Sigmoid 函数有更多了解，可以点开<a href="https://www.desmos.com/calculator/bgontvxotm" target="_blank" rel="noopener">此链接</a>跟此函数互动。</p>
<h3 id="基于最优化方法的回归系数确定"><a href="#基于最优化方法的回归系数确定" class="headerlink" title="基于最优化方法的回归系数确定"></a>基于最优化方法的回归系数确定</h3><p>Sigmoid 函数的输入记为 z ，由下面公式得到: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_2.png" alt="Sigmoid 函数计算公式"></p>
<p>如果采用向量的写法，上述公式可以写成 <img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_4.png" alt="Sigmoid 函数计算公式向量形式"> ，它表示将这两个数值向量对应元素相乘然后全部加起来即得到 z 值。其中的向量 x 是分类器的输入数据，向量 w 也就是我们要找到的最佳参数（系数），从而使得分类器尽可能地精确。为了寻找该最佳参数，需要用到最优化理论的一些知识。我们这里使用的是——梯度上升法（Gradient Ascent）。</p>
<h3 id="梯度上升法"><a href="#梯度上升法" class="headerlink" title="梯度上升法"></a>梯度上升法</h3><h4 id="梯度的介绍"><a href="#梯度的介绍" class="headerlink" title="梯度的介绍"></a>梯度的介绍</h4><p>需要一点点向量方面的数学知识</p>
<figure class="highlight fix"><table><tr><td class="code"><pre><span class="line"><span class="attr">向量 </span>=<span class="string"> 值 + 方向  </span></span><br><span class="line"><span class="string">梯度 = 向量</span></span><br><span class="line"><span class="string">梯度 = 梯度值 + 梯度方向</span></span><br></pre></td></tr></table></figure>

<h4 id="梯度上升法的思想"><a href="#梯度上升法的思想" class="headerlink" title="梯度上升法的思想"></a>梯度上升法的思想</h4><p>要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为 ▽ ，则函数 f(x, y) 的梯度由下式表示: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_5.png" alt="梯度上升计算公式"></p>
<p>这个梯度意味着要沿 x 的方向移动 <img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_6.png" alt="f(x, y)对x求偏导"> ，沿 y 的方向移动 <img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_7.png" alt="f(x, y)对y求偏导"> 。其中，函数f(x, y) 必须要在待计算的点上有定义并且可微。下图是一个具体的例子。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_8.png" alt="梯度上升"></p>
<p>上图展示的，梯度上升算法到达每个点后都会重新估计移动的方向。从 P0 开始，计算完该点的梯度，函数就根据梯度移动到下一点 P1。在 P1 点，梯度再次被重新计算，并沿着新的梯度方向移动到 P2 。如此循环迭代，直到满足停止条件。迭代过程中，梯度算子总是保证我们能选取到最佳的移动方向。</p>
<p>上图中的梯度上升算法沿梯度方向移动了一步。可以看到，梯度算子总是指向函数值增长最快的方向。这里所说的是移动方向，而未提到移动量的大小。该量值称为步长，记作 α 。用向量来表示的话，梯度上升算法的迭代公式如下: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_9.png" alt="梯度上升迭代公式"></p>
<p>该公式将一直被迭代执行，直至达到某个停止条件为止，比如迭代次数达到某个指定值或者算法达到某个可以允许的误差范围。</p>
<p>介绍一下几个相关的概念：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">例如：y = w0 + w1x1 + w2x2 + ... + wnxn</span><br><span class="line">梯度：参考上图的例子，二维图像，x方向代表第一个系数，也就是 w1，y方向代表第二个系数也就是 w2，这样的向量就是梯度。</span><br><span class="line">α：上面的梯度算法的迭代公式中的阿尔法，这个代表的是移动步长（step length）。移动步长会影响最终结果的拟合程度，最好的方法就是随着迭代次数更改移动步长。</span><br><span class="line">步长通俗的理解，<span class="number">100</span>米，如果我一步走<span class="number">10</span>米，我需要走<span class="number">10</span>步；如果一步走<span class="number">20</span>米，我只需要走<span class="number">5</span>步。这里的一步走多少米就是步长的意思。</span><br><span class="line">▽f(w)：代表沿着梯度变化的方向。</span><br></pre></td></tr></table></figure>


<p>   问：有人会好奇为什么有些书籍上说的是梯度下降法（Gradient Decent）?</p>
<p>   答： 其实这个两个方法在此情况下本质上是相同的。关键在于代价函数（cost function）或者叫目标函数（objective function）。如果目标函数是损失函数，那就是最小化损失函数来求函数的最小值，就用梯度下降。 如果目标函数是似然函数（Likelihood function），就是要最大化似然函数来求函数的最大值，那就用梯度上升。在逻辑回归中， 损失函数和似然函数无非就是互为正负关系。</p>
<p>   只需要在迭代公式中的加法变成减法。因此，对应的公式可以写成</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_10.png" alt="梯度下降迭代公式"></p>
<p><strong>局部最优现象 （Local Optima）</strong></p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_20.png" alt="梯度下降图_4"></p>
<p>上图表示参数 θ 与误差函数 J(θ) 的关系图 (这里的误差函数是损失函数，所以我们要最小化损失函数)，红色的部分是表示 J(θ) 有着比较高的取值，我们需要的是，能够让 J(θ) 的值尽量的低。也就是深蓝色的部分。θ0，θ1 表示 θ 向量的两个维度（此处的θ0，θ1是x0和x1的系数，也对应的是上文w0和w1）。</p>
<p>可能梯度下降的最终点并非是全局最小点，可能是一个局部最小点，如我们上图中的右边的梯度下降曲线，描述的是最终到达一个局部最小点，这是我们重新选择了一个初始点得到的。</p>
<p>看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。</p>
<h2 id="Logistic-回归-原理"><a href="#Logistic-回归-原理" class="headerlink" title="Logistic 回归 原理"></a>Logistic 回归 原理</h2><h3 id="Logistic-回归-工作原理"><a href="#Logistic-回归-工作原理" class="headerlink" title="Logistic 回归 工作原理"></a>Logistic 回归 工作原理</h3><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">每个回归系数初始化为 <span class="number">1</span></span><br><span class="line">重复 R 次:</span><br><span class="line">    计算整个数据集的梯度</span><br><span class="line">    使用 步长 x 梯度 更新回归系数的向量</span><br><span class="line">返回回归系数</span><br></pre></td></tr></table></figure>

<h3 id="Logistic-回归-开发流程"><a href="#Logistic-回归-开发流程" class="headerlink" title="Logistic 回归 开发流程"></a>Logistic 回归 开发流程</h3><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 采用任意方法收集数据</span></span><br><span class="line"><span class="section">准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。</span></span><br><span class="line"><span class="section">分析数据: 采用任意方法对数据进行分析。</span></span><br><span class="line"><span class="section">训练算法: 大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。</span></span><br><span class="line"><span class="section">测试算法: 一旦训练步骤完成，分类将会很快。</span></span><br><span class="line"><span class="section">使用算法: 首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。</span></span><br></pre></td></tr></table></figure>

<h3 id="Logistic-回归-算法特点"><a href="#Logistic-回归-算法特点" class="headerlink" title="Logistic 回归 算法特点"></a>Logistic 回归 算法特点</h3><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">优点: 计算代价不高，易于理解和实现。</span></span><br><span class="line"><span class="section">缺点: 容易欠拟合，分类精度可能不高。</span></span><br><span class="line"><span class="section">适用数据类型: 数值型和标称型数据。</span></span><br></pre></td></tr></table></figure>

<h3 id="附加-方向导数与梯度"><a href="#附加-方向导数与梯度" class="headerlink" title="附加 方向导数与梯度"></a>附加 方向导数与梯度</h3><p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6.png" alt="方向导数与梯度"></p>
<h2 id="Logistic-回归-项目案例"><a href="#Logistic-回归-项目案例" class="headerlink" title="Logistic 回归 项目案例"></a>Logistic 回归 项目案例</h2><h3 id="项目案例1-使用-Logistic-回归在简单数据集上的分类"><a href="#项目案例1-使用-Logistic-回归在简单数据集上的分类" class="headerlink" title="项目案例1: 使用 Logistic 回归在简单数据集上的分类"></a>项目案例1: 使用 Logistic 回归在简单数据集上的分类</h3><p><a href="/src/py2.x/ml/5.Logistic/logistic.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/5.Logistic/logistic.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/5.Logistic/logistic.py</a></p>
<h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p>在一个简单的数据集上，采用梯度上升法找到 Logistic 回归分类器在此数据集上的最佳回归系数</p>
<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 可以使用任何方法</span></span><br><span class="line"><span class="section">准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳</span></span><br><span class="line"><span class="section">分析数据: 画出决策边界</span></span><br><span class="line"><span class="section">训练算法: 使用梯度上升找到最佳参数</span></span><br><span class="line"><span class="section">测试算法: 使用 Logistic 回归进行分类</span></span><br><span class="line"><span class="section">使用算法: 对简单数据集中数据进行分类</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 可以使用任何方法</p>
</blockquote>
<p>我们采用存储在 TestSet.txt 文本文件中的数据，存储格式如下: </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">-0.017612</span>	<span class="number">14.053064</span>	<span class="number">0</span></span><br><span class="line"><span class="number">-1.395634</span>	<span class="number">4.662541</span>	<span class="number">1</span></span><br><span class="line"><span class="number">-0.752157</span>	<span class="number">6.538620</span>	<span class="number">0</span></span><br><span class="line"><span class="number">-1.322371</span>	<span class="number">7.152853</span>	<span class="number">0</span></span><br><span class="line"><span class="number">0.423363</span>	<span class="number">11.054677</span>	<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>绘制在图中，如下图所示: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_11.png" alt="简单数据集绘制在图上"></p>
<blockquote>
<p>准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 解析数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(file_name)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Desc: </span></span><br><span class="line"><span class="string">        加载并解析数据</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        file_name -- 要解析的文件路径</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dataMat -- 原始数据的特征</span></span><br><span class="line"><span class="string">        labelMat -- 原始数据的标签，也就是每条样本对应的类别。即目标向量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># dataMat为原始数据， labelMat为原始数据的标签</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = open(file_name)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = line.strip().split()</span><br><span class="line">        <span class="comment"># 为了方便计算，我们将 X0 的值设为 1.0 ，也就是在每一行的开头添加一个 1.0 作为 X0</span></span><br><span class="line">        dataMat.append([<span class="number">1.0</span>, float(lineArr[<span class="number">0</span>]), float(lineArr[<span class="number">1</span>])])</span><br><span class="line">        labelMat.append(int(lineArr[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据: 采用任意方法对数据进行分析，此处不需要</p>
</blockquote>
<blockquote>
<p>训练算法: 使用梯度上升找到最佳参数</p>
</blockquote>
<p>定义sigmoid阶跃函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sigmoid阶跃函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(inX)</span>:</span></span><br><span class="line">    <span class="comment"># return 1.0 / (1 + exp(-inX))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tanh是Sigmoid的变形，与 sigmoid 不同的是，tanh 是0均值的。因此，实际应用中，tanh 会比 sigmoid 更好。</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * <span class="number">1.0</span>/(<span class="number">1</span>+exp(<span class="number">-2</span>*inX)) - <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>Logistic 回归梯度上升优化算法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正常的处理方案</span></span><br><span class="line"><span class="comment"># 两个参数：第一个参数==&gt; dataMatIn 是一个2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本。</span></span><br><span class="line"><span class="comment"># 第二个参数==&gt; classLabels 是类别标签，它是一个 1*100 的行向量。为了便于矩阵计算，需要将该行向量转换为列向量，做法是将原向量转置，再将它赋值给labelMat。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn, classLabels)</span>:</span></span><br><span class="line">    <span class="comment"># 转化为矩阵[[1,1,2],[1,1,2]....]</span></span><br><span class="line">    dataMatrix = mat(dataMatIn)             <span class="comment"># 转换为 NumPy 矩阵</span></span><br><span class="line">    <span class="comment"># 转化为矩阵[[0,1,0,1,0,1.....]]，并转制[[0],[1],[0].....]</span></span><br><span class="line">    <span class="comment"># transpose() 行列转置函数</span></span><br><span class="line">    <span class="comment"># 将行向量转化为列向量   =&gt;  矩阵的转置</span></span><br><span class="line">    labelMat = mat(classLabels).transpose() <span class="comment"># 首先将数组转换为 NumPy 矩阵，然后再将行向量转置为列向量</span></span><br><span class="line">    <span class="comment"># m-&gt;数据量，样本数 n-&gt;特征数</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    <span class="comment"># print m, n, '__'*10, shape(dataMatrix.transpose()), '__'*100</span></span><br><span class="line">    <span class="comment"># alpha代表向目标移动的步长</span></span><br><span class="line">    alpha = <span class="number">0.001</span></span><br><span class="line">    <span class="comment"># 迭代次数</span></span><br><span class="line">    maxCycles = <span class="number">500</span></span><br><span class="line">    <span class="comment"># 生成一个长度和特征数相同的矩阵，此处n为3 -&gt; [[1],[1],[1]]</span></span><br><span class="line">    <span class="comment"># weights 代表回归系数， 此处的 ones((n,1)) 创建一个长度和特征数相同的矩阵，其中的数全部都是 1</span></span><br><span class="line">    weights = ones((n,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):              <span class="comment">#heavy on matrix operations</span></span><br><span class="line">        <span class="comment"># m*3 的矩阵 * 3*1 的矩阵 ＝ m*1的矩阵</span></span><br><span class="line">        <span class="comment"># 那么乘上矩阵的意义，就代表：通过公式得到的理论值</span></span><br><span class="line">        <span class="comment"># 参考地址： 矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/31050145</span></span><br><span class="line">        <span class="comment"># print 'dataMatrix====', dataMatrix </span></span><br><span class="line">        <span class="comment"># print 'weights====', weights</span></span><br><span class="line">        <span class="comment"># n*3   *  3*1  = n*1</span></span><br><span class="line">        h = sigmoid(dataMatrix*weights)     <span class="comment"># 矩阵乘法</span></span><br><span class="line">        <span class="comment"># print 'hhhhhhh====', h</span></span><br><span class="line">        <span class="comment"># labelMat是实际值</span></span><br><span class="line">        error = (labelMat - h)              <span class="comment"># 向量相减</span></span><br><span class="line">        <span class="comment"># 0.001* (3*m)*(m*1) 表示在每一个列上的一个误差情况，最后得出 x1,x2,xn的系数的偏移量</span></span><br><span class="line">        weights = weights + alpha * dataMatrix.transpose() * error <span class="comment"># 矩阵乘法，最后得到回归系数</span></span><br><span class="line">    <span class="keyword">return</span> array(weights)</span><br></pre></td></tr></table></figure>

<p>大家看到这儿可能会有一些疑惑，就是，我们在迭代中更新我们的回归系数，后边的部分是怎么计算出来的？为什么会是 alpha * dataMatrix.transpose() * error ?因为这就是我们所求的梯度，也就是对 f(w) 对 w 求一阶导数。具体推导如下:</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_21.png" alt="f(w)对w求一阶导数"><br>可参考<a href="http://blog.csdn.net/achuo/article/details/51160101" target="_blank" rel="noopener">http://blog.csdn.net/achuo/article/details/51160101</a></p>
<p>画出数据集和 Logistic 回归最佳拟合直线的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotBestFit</span><span class="params">(dataArr, labelMat, weights)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Desc:</span></span><br><span class="line"><span class="string">            将我们得到的数据可视化展示出来</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dataArr:样本数据的特征</span></span><br><span class="line"><span class="string">            labelMat:样本数据的类别标签，即目标变量</span></span><br><span class="line"><span class="string">            weights:回归系数</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    n = shape(dataArr)[<span class="number">0</span>]</span><br><span class="line">    xcord1 = []; ycord1 = []</span><br><span class="line">    xcord2 = []; ycord2 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> int(labelMat[i])== <span class="number">1</span>:</span><br><span class="line">            xcord1.append(dataArr[i,<span class="number">1</span>]); ycord1.append(dataArr[i,<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xcord2.append(dataArr[i,<span class="number">1</span>]); ycord2.append(dataArr[i,<span class="number">2</span>])</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.scatter(xcord1, ycord1, s=<span class="number">30</span>, c=<span class="string">'red'</span>, marker=<span class="string">'s'</span>)</span><br><span class="line">    ax.scatter(xcord2, ycord2, s=<span class="number">30</span>, c=<span class="string">'green'</span>)</span><br><span class="line">    x = arange(<span class="number">-3.0</span>, <span class="number">3.0</span>, <span class="number">0.1</span>)</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    y的由来，卧槽，是不是没看懂？</span></span><br><span class="line"><span class="string">    首先理论上是这个样子的。</span></span><br><span class="line"><span class="string">    dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])</span></span><br><span class="line"><span class="string">    w0*x0+w1*x1+w2*x2=f(x)</span></span><br><span class="line"><span class="string">    x0最开始就设置为1叻， x2就是我们画图的y值，而f(x)被我们磨合误差给算到w0,w1,w2身上去了</span></span><br><span class="line"><span class="string">    所以： w0+w1*x+w2*y=0 =&gt; y = (-w0-w1*x)/w2   </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    y = (-weights[<span class="number">0</span>]-weights[<span class="number">1</span>]*x)/weights[<span class="number">2</span>]</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    plt.xlabel(<span class="string">'X'</span>); plt.ylabel(<span class="string">'Y'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 使用 Logistic 回归进行分类</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testLR</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 1.收集并准备数据</span></span><br><span class="line">    dataMat, labelMat = loadDataSet(<span class="string">"data/5.Logistic/TestSet.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print dataMat, '---\n', labelMat</span></span><br><span class="line">    <span class="comment"># 2.训练模型，  f(x)=a1*x1+b2*x2+..+nn*xn中 (a1,b2, .., nn).T的矩阵值</span></span><br><span class="line">    <span class="comment"># 因为数组没有是复制n份， array的乘法就是乘法</span></span><br><span class="line">    dataArr = array(dataMat)</span><br><span class="line">    <span class="comment"># print dataArr</span></span><br><span class="line">    weights = gradAscent(dataArr, labelMat)</span><br><span class="line">    <span class="comment"># weights = stocGradAscent0(dataArr, labelMat)</span></span><br><span class="line">    <span class="comment"># weights = stocGradAscent1(dataArr, labelMat)</span></span><br><span class="line">    <span class="comment"># print '*'*30, weights</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据可视化</span></span><br><span class="line">    plotBestFit(dataArr, labelMat, weights)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法: 对简单数据集中数据进行分类</p>
</blockquote>
<h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p>梯度上升算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理 100 个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为 <code>随机梯度上升算法</code>。由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习(online learning)算法。与 “在线学习” 相对应，一次处理所有数据被称作是 “批处理” （batch） 。</p>
<p>随机梯度上升算法可以写成如下的伪代码: </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">所有回归系数初始化为 <span class="number">1</span></span><br><span class="line">对数据集中每个样本</span><br><span class="line">    计算该样本的梯度</span><br><span class="line">    使用 alpha x gradient 更新回归系数值</span><br><span class="line">返回回归系数值</span><br></pre></td></tr></table></figure>

<p>以下是随机梯度上升算法的实现代码: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机梯度上升</span></span><br><span class="line"><span class="comment"># 梯度上升优化算法在每次更新数据集时都需要遍历整个数据集，计算复杂都较高</span></span><br><span class="line"><span class="comment"># 随机梯度上升一次只用一个样本点来更新回归系数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span><span class="params">(dataMatrix, classLabels)</span>:</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># n*1的矩阵</span></span><br><span class="line">    <span class="comment"># 函数ones创建一个全1的数组</span></span><br><span class="line">    weights = ones(n)   <span class="comment"># 初始化长度为n的数组，元素全部为 1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,此处求出的 h 是一个具体的数值，而不是一个矩阵</span></span><br><span class="line">        h = sigmoid(sum(dataMatrix[i]*weights))</span><br><span class="line">        <span class="comment"># print 'dataMatrix[i]===', dataMatrix[i]</span></span><br><span class="line">        <span class="comment"># 计算真实类别与预测类别之间的差值，然后按照该差值调整回归系数</span></span><br><span class="line">        error = classLabels[i] - h</span><br><span class="line">        <span class="comment"># 0.01*(1*1)*(1*n)</span></span><br><span class="line">        <span class="keyword">print</span> weights, <span class="string">"*"</span>*<span class="number">10</span> , dataMatrix[i], <span class="string">"*"</span>*<span class="number">10</span> , error</span><br><span class="line">        weights = weights + alpha * error * dataMatrix[i]</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>

<p>可以看到，随机梯度上升算法与梯度上升算法在代码上很相似，但也有一些区别: 第一，后者的变量 h 和误差 error 都是向量，而前者则全是数值；第二，前者没有矩阵的转换过程，所有变量的数据类型都是 NumPy 数组。</p>
<p>判断优化算法优劣的可靠方法是看它是否收敛，也就是说参数是否达到了稳定值，是否还会不断地变化？下图展示了随机梯度上升算法在 200 次迭代过程中回归系数的变化情况。其中的系数2，也就是 X2 只经过了 50 次迭代就达到了稳定值，但系数 1 和 0 则需要更多次的迭代。如下图所示: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_12.png" alt="回归系数与迭代次数的关系图"></p>
<p>针对这个问题，我们改进了之前的随机梯度上升算法，如下: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机梯度上升算法（随机化）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataMatrix, classLabels, numIter=<span class="number">150</span>)</span>:</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    weights = ones(n)   <span class="comment"># 创建与列数相同的矩阵的系数矩阵，所有的元素都是1</span></span><br><span class="line">    <span class="comment"># 随机梯度, 循环150,观察是否收敛</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">        <span class="comment"># [0, 1, 2 .. m-1]</span></span><br><span class="line">        dataIndex = range(m)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># i和j的不断增大，导致alpha的值不断减少，但是不为0</span></span><br><span class="line">            alpha = <span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.0001</span>    <span class="comment"># alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001</span></span><br><span class="line">            <span class="comment"># 随机产生一个 0～len()之间的一个值</span></span><br><span class="line">            <span class="comment"># random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。</span></span><br><span class="line">            randIndex = int(random.uniform(<span class="number">0</span>,len(dataIndex)))</span><br><span class="line">            <span class="comment"># sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn</span></span><br><span class="line">            h = sigmoid(sum(dataMatrix[dataIndex[randIndex]]*weights))</span><br><span class="line">            error = classLabels[dataIndex[randIndex]] - h</span><br><span class="line">            <span class="comment"># print weights, '__h=%s' % h, '__'*20, alpha, '__'*20, error, '__'*20, dataMatrix[randIndex]</span></span><br><span class="line">            weights = weights + alpha * error * dataMatrix[dataIndex[randIndex]]</span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>

<p>上面的改进版随机梯度上升算法，我们修改了两处代码。</p>
<p>第一处改进为 alpha 的值。alpha 在每次迭代的时候都会调整，这回缓解上面波动图的数据波动或者高频波动。另外，虽然 alpha 会随着迭代次数不断减少，但永远不会减小到 0，因为我们在计算公式中添加了一个常数项。</p>
<p>第二处修改为 randIndex 更新，这里通过随机选取样本拉来更新回归系数。这种方法将减少周期性的波动。这种方法每次随机从列表中选出一个值，然后从列表中删掉该值（再进行下一次迭代）。</p>
<p>程序运行之后能看到类似于下图的结果图。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/5.Logistic/LR_13.png" alt="改进随机梯度下降结果图"></p>
<h3 id="项目案例2-从疝气病症预测病马的死亡率"><a href="#项目案例2-从疝气病症预测病马的死亡率" class="headerlink" title="项目案例2: 从疝气病症预测病马的死亡率"></a>项目案例2: 从疝气病症预测病马的死亡率</h3><p><a href="/src/py2.x/ml/5.Logistic/logistic.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/5.Logistic/logistic.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/5.Logistic/logistic.py</a></p>
<h4 id="项目概述-1"><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4><p>使用 Logistic 回归来预测患有疝病的马的存活问题。疝病是描述马胃肠痛的术语。然而，这种病不一定源自马的胃肠问题，其他问题也可能引发马疝病。这个数据集中包含了医院检测马疝病的一些指标，有的指标比较主观，有的指标难以测量，例如马的疼痛级别。</p>
<h4 id="开发流程-1"><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 给定数据文件</span></span><br><span class="line"><span class="section">准备数据: 用 Python 解析文本文件并填充缺失值</span></span><br><span class="line"><span class="section">分析数据: 可视化并观察数据</span></span><br><span class="line"><span class="section">训练算法: 使用优化算法，找到最佳的系数</span></span><br><span class="line"><span class="section">测试算法: 为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，</span></span><br><span class="line">         通过改变迭代的次数和步长的参数来得到更好的回归系数</span><br><span class="line"><span class="section">使用算法: 实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事，</span></span><br><span class="line">         这可以作为留给大家的一道习题</span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 给定数据文件</p>
</blockquote>
<p>病马的训练数据已经给出来了，如下形式存储在文本文件中:</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1.000000</span>	<span class="number">1.000000</span>	<span class="number">39.200000</span>	<span class="number">88.000000</span>	<span class="number">20.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">4.000000</span>	<span class="number">1.000000</span>	<span class="number">3.000000</span>	<span class="number">4.000000</span>	<span class="number">2.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">4.000000</span>	<span class="number">2.000000</span>	<span class="number">50.000000</span>	<span class="number">85.000000</span>	<span class="number">2.000000</span>	<span class="number">2.000000</span>	<span class="number">0.000000</span></span><br><span class="line"><span class="number">2.000000</span>	<span class="number">1.000000</span>	<span class="number">38.300000</span>	<span class="number">40.000000</span>	<span class="number">24.000000</span>	<span class="number">1.000000</span>	<span class="number">1.000000</span>	<span class="number">3.000000</span>	<span class="number">1.000000</span>	<span class="number">3.000000</span>	<span class="number">3.000000</span>	<span class="number">1.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">1.000000</span>	<span class="number">1.000000</span>	<span class="number">33.000000</span>	<span class="number">6.700000</span>	<span class="number">0.000000</span>	<span class="number">0.000000</span>	<span class="number">1.000000</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>准备数据: 用 Python 解析文本文件并填充缺失值</p>
</blockquote>
<p>处理数据中的缺失值</p>
<p>假设有100个样本和20个特征，这些数据都是机器收集回来的。若机器上的某个传感器损坏导致一个特征无效时该怎么办？此时是否要扔掉整个数据？这种情况下，另外19个特征怎么办？<br>它们是否还可以用？答案是肯定的。因为有时候数据相当昂贵，扔掉和重新获取都是不可取的，所以必须采用一些方法来解决这个问题。</p>
<p>下面给出了一些可选的做法：</p>
<ul>
<li>使用可用特征的均值来填补缺失值；</li>
<li>使用特殊值来填补缺失值，如 -1；</li>
<li>忽略有缺失值的样本；</li>
<li>使用有相似样本的均值添补缺失值；</li>
<li>使用另外的机器学习算法预测缺失值。</li>
</ul>
<p>现在，我们对下一节要用的数据集进行预处理，使其可以顺利地使用分类算法。在预处理需要做两件事: </p>
<ul>
<li><p>所有的缺失值必须用一个实数值来替换，因为我们使用的 NumPy 数据类型不允许包含缺失值。我们这里选择实数 0 来替换所有缺失值，恰好能适用于 Logistic 回归。这样做的直觉在于，我们需要的是一个在更新时不会影响系数的值。回归系数的更新公式如下:</p>
<p>  weights = weights + alpha * error * dataMatrix[dataIndex[randIndex]]</p>
<p>  如果 dataMatrix 的某个特征对应值为 0，那么该特征的系数将不做更新，即:</p>
<p>  weights = weights</p>
<p>  另外，由于 Sigmoid(0) = 0.5 ，即它对结果的预测不具有任何倾向性，因此我们上述做法也不会对误差造成任何影响。基于上述原因，将缺失值用 0 代替既可以保留现有数据，也不需要对优化算法进行修改。此外，该数据集中的特征取值一般不为 0，因此在某种意义上说它也满足 “特殊值” 这个要求。</p>
</li>
<li><p>如果在测试数据集中发现了一条数据的类别标签已经缺失，那么我们的简单做法是将该条数据丢弃。这是因为类别标签与特征不同，很难确定采用某个合适的值来替换。采用 Logistic 回归进行分类时这种做法是合理的，而如果采用类似 kNN 的方法，则保留该条数据显得更加合理。</p>
</li>
</ul>
<p>原始的数据集经过预处理后，保存成两个文件: horseColicTest.txt 和 horseColicTraining.txt 。 </p>
<blockquote>
<p>分析数据: 可视化并观察数据</p>
</blockquote>
<p>将数据使用 MatPlotlib 打印出来，观察数据是否是我们想要的格式</p>
<blockquote>
<p>训练算法: 使用优化算法，找到最佳的系数</p>
</blockquote>
<p>下面给出 原始的梯度上升算法，随机梯度上升算法，改进版随机梯度上升算法 的代码: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正常的处理方案</span></span><br><span class="line"><span class="comment"># 两个参数：第一个参数==&gt; dataMatIn 是一个2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本。</span></span><br><span class="line"><span class="comment"># 第二个参数==&gt; classLabels 是类别标签，它是一个 1*100 的行向量。为了便于矩阵计算，需要将该行向量转换为列向量，做法是将原向量转置，再将它赋值给labelMat。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn, classLabels)</span>:</span></span><br><span class="line">    <span class="comment"># 转化为矩阵[[1,1,2],[1,1,2]....]</span></span><br><span class="line">    dataMatrix = mat(dataMatIn)             <span class="comment"># 转换为 NumPy 矩阵</span></span><br><span class="line">    <span class="comment"># 转化为矩阵[[0,1,0,1,0,1.....]]，并转制[[0],[1],[0].....]</span></span><br><span class="line">    <span class="comment"># transpose() 行列转置函数</span></span><br><span class="line">    <span class="comment"># 将行向量转化为列向量   =&gt;  矩阵的转置</span></span><br><span class="line">    labelMat = mat(classLabels).transpose() <span class="comment"># 首先将数组转换为 NumPy 矩阵，然后再将行向量转置为列向量</span></span><br><span class="line">    <span class="comment"># m-&gt;数据量，样本数 n-&gt;特征数</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    <span class="comment"># print m, n, '__'*10, shape(dataMatrix.transpose()), '__'*100</span></span><br><span class="line">    <span class="comment"># alpha代表向目标移动的步长</span></span><br><span class="line">    alpha = <span class="number">0.001</span></span><br><span class="line">    <span class="comment"># 迭代次数</span></span><br><span class="line">    maxCycles = <span class="number">500</span></span><br><span class="line">    <span class="comment"># 生成一个长度和特征数相同的矩阵，此处n为3 -&gt; [[1],[1],[1]]</span></span><br><span class="line">    <span class="comment"># weights 代表回归系数， 此处的 ones((n,1)) 创建一个长度和特征数相同的矩阵，其中的数全部都是 1</span></span><br><span class="line">    weights = ones((n,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):              <span class="comment">#heavy on matrix operations</span></span><br><span class="line">        <span class="comment"># m*3 的矩阵 * 3*1 的单位矩阵 ＝ m*1的矩阵</span></span><br><span class="line">        <span class="comment"># 那么乘上单位矩阵的意义，就代表：通过公式得到的理论值</span></span><br><span class="line">        <span class="comment"># 参考地址： 矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/31050145</span></span><br><span class="line">        <span class="comment"># print 'dataMatrix====', dataMatrix </span></span><br><span class="line">        <span class="comment"># print 'weights====', weights</span></span><br><span class="line">        <span class="comment"># n*3   *  3*1  = n*1</span></span><br><span class="line">        h = sigmoid(dataMatrix*weights)     <span class="comment"># 矩阵乘法</span></span><br><span class="line">        <span class="comment"># print 'hhhhhhh====', h</span></span><br><span class="line">        <span class="comment"># labelMat是实际值</span></span><br><span class="line">        error = (labelMat - h)              <span class="comment"># 向量相减</span></span><br><span class="line">        <span class="comment"># 0.001* (3*m)*(m*1) 表示在每一个列上的一个误差情况，最后得出 x1,x2,xn的系数的偏移量</span></span><br><span class="line">        weights = weights + alpha * dataMatrix.transpose() * error <span class="comment"># 矩阵乘法，最后得到回归系数</span></span><br><span class="line">    <span class="keyword">return</span> array(weights)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机梯度上升</span></span><br><span class="line"><span class="comment"># 梯度上升优化算法在每次更新数据集时都需要遍历整个数据集，计算复杂都较高</span></span><br><span class="line"><span class="comment"># 随机梯度上升一次只用一个样本点来更新回归系数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span><span class="params">(dataMatrix, classLabels)</span>:</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># n*1的矩阵</span></span><br><span class="line">    <span class="comment"># 函数ones创建一个全1的数组</span></span><br><span class="line">    weights = ones(n)   <span class="comment"># 初始化长度为n的数组，元素全部为 1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,此处求出的 h 是一个具体的数值，而不是一个矩阵</span></span><br><span class="line">        h = sigmoid(sum(dataMatrix[i]*weights))</span><br><span class="line">        <span class="comment"># print 'dataMatrix[i]===', dataMatrix[i]</span></span><br><span class="line">        <span class="comment"># 计算真实类别与预测类别之间的差值，然后按照该差值调整回归系数</span></span><br><span class="line">        error = classLabels[i] - h</span><br><span class="line">        <span class="comment"># 0.01*(1*1)*(1*n)</span></span><br><span class="line">        <span class="keyword">print</span> weights, <span class="string">"*"</span>*<span class="number">10</span> , dataMatrix[i], <span class="string">"*"</span>*<span class="number">10</span> , error</span><br><span class="line">        weights = weights + alpha * error * dataMatrix[i]</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机梯度上升算法（随机化）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataMatrix, classLabels, numIter=<span class="number">150</span>)</span>:</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    weights = ones(n)   <span class="comment"># 创建与列数相同的矩阵的系数矩阵，所有的元素都是1</span></span><br><span class="line">    <span class="comment"># 随机梯度, 循环150,观察是否收敛</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">        <span class="comment"># [0, 1, 2 .. m-1]</span></span><br><span class="line">        dataIndex = range(m)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># i和j的不断增大，导致alpha的值不断减少，但是不为0</span></span><br><span class="line">            alpha = <span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.0001</span>    <span class="comment"># alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001</span></span><br><span class="line">            <span class="comment"># 随机产生一个 0～len()之间的一个值</span></span><br><span class="line">            <span class="comment"># random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。</span></span><br><span class="line">            randIndex = int(random.uniform(<span class="number">0</span>,len(dataIndex)))</span><br><span class="line">            <span class="comment"># sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn</span></span><br><span class="line">            h = sigmoid(sum(dataMatrix[dataIndex[randIndex]]*weights))</span><br><span class="line">            error = classLabels[dataIndex[randIndex]] - h</span><br><span class="line">            <span class="comment"># print weights, '__h=%s' % h, '__'*20, alpha, '__'*20, error, '__'*20, dataMatrix[randIndex]</span></span><br><span class="line">            weights = weights + alpha * error * dataMatrix[dataIndex[randIndex]]</span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长的参数来得到更好的回归系数</p>
</blockquote>
<p>Logistic 回归分类函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分类函数，根据回归系数和特征向量来计算 Sigmoid的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyVector</span><span class="params">(inX, weights)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Desc: </span></span><br><span class="line"><span class="string">        最终的分类函数，根据回归系数和特征向量来计算 Sigmoid 的值，大于0.5函数返回1，否则返回0</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        inX -- 特征向量，features</span></span><br><span class="line"><span class="string">        weights -- 根据梯度下降/随机梯度下降 计算得到的回归系数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        如果 prob 计算大于 0.5 函数返回 1</span></span><br><span class="line"><span class="string">        否则返回 0</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    prob = sigmoid(sum(inX*weights))</span><br><span class="line">    <span class="keyword">if</span> prob &gt; <span class="number">0.5</span>: <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开测试集和训练集,并对数据进行格式化处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colicTest</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        打开测试集和训练集，并对数据进行格式化处理</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        errorRate -- 分类错误率</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    frTrain = open(<span class="string">'data/5.Logistic/horseColicTraining.txt'</span>)</span><br><span class="line">    frTest = open(<span class="string">'data/5.Logistic/horseColicTest.txt'</span>)</span><br><span class="line">    trainingSet = []</span><br><span class="line">    trainingLabels = []</span><br><span class="line">    <span class="comment"># 解析训练数据集中的数据特征和Labels</span></span><br><span class="line">    <span class="comment"># trainingSet 中存储训练数据集的特征，trainingLabels 存储训练数据集的样本对应的分类标签</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTrain.readlines():</span><br><span class="line">        currLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        lineArr = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        trainingSet.append(lineArr)</span><br><span class="line">        trainingLabels.append(float(currLine[<span class="number">21</span>]))</span><br><span class="line">    <span class="comment"># 使用 改进后的 随机梯度下降算法 求得在此数据集上的最佳回归系数 trainWeights</span></span><br><span class="line">    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, <span class="number">500</span>)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    numTestVec = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 读取 测试数据集 进行测试，计算分类错误的样本条数和最终的错误率</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTest.readlines():</span><br><span class="line">        numTestVec += <span class="number">1.0</span></span><br><span class="line">        currLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        lineArr = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        <span class="keyword">if</span> int(classifyVector(array(lineArr), trainWeights)) != int(currLine[<span class="number">21</span>]):</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    errorRate = (float(errorCount) / numTestVec)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"the error rate of this test is: %f"</span> % errorRate</span><br><span class="line">    <span class="keyword">return</span> errorRate</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 colicTest() 10次并求结果的平均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiTest</span><span class="params">()</span>:</span></span><br><span class="line">    numTests = <span class="number">10</span></span><br><span class="line">    errorSum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(numTests):</span><br><span class="line">        errorSum += colicTest()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"after %d iterations the average error rate is: %f"</span> % (numTests, errorSum/float(numTests))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法: 实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事，这可以作为留给大家的一道习题</p>
</blockquote>
<h1 id="额外内容-可选读"><a href="#额外内容-可选读" class="headerlink" title="额外内容(可选读)"></a>额外内容(可选读)</h1><p>在上文中，当Sigmoid函数大于 0.5 的数据被分入 1 类，小于 0.5 即被归入 0 类。其实0.5也是可以改动的。 比如大于 0.9 的数据被分入 1 类，小于 0.9 即被归入 0 类。</p>
<h2 id="Logistic回归-和-最大熵模型"><a href="#Logistic回归-和-最大熵模型" class="headerlink" title="Logistic回归 和 最大熵模型"></a>Logistic回归 和 最大熵模型</h2><p>Logistic回归和最大熵模型 都属于对数线性模型 （log linear model）。 当类标签（class label）只有两个的时候，最大熵模型就是 logistic 回归模型。 学习它们的模型一般采用极大似然估计或者正则化的极大似然估计。Logistic 回归和最大熵模型学习可以形式化为无约束最优化问题。（关于最大熵模型，可以阅读《统计学习方法》 第六章。）</p>
<h2 id="其他算法"><a href="#其他算法" class="headerlink" title="其他算法"></a>其他算法</h2><p>除了梯度下降，随机梯度下降，还有Conjugate Gradient，BFGS，L-BFGS，他们不需要指定alpha值（步长），而且比梯度下降更快，在现实中应用的也比较多。 当然这些算法相比随机梯度要复杂。</p>
<p>综上这些算法都有一个共通的缺点就是他们都是不断去逼近真实值，永远只是一个真实值的近似值而已。</p>
<h2 id="多标签分类"><a href="#多标签分类" class="headerlink" title="多标签分类"></a>多标签分类</h2><p>逻辑回归也可以用作于多标签分类。 思路如下：</p>
<p>假设我们标签A中有a0,a1,a2….an个标签，对于每个标签 ai (ai 是标签A之一)，我们训练一个逻辑回归分类器。</p>
<p>即，训练该标签的逻辑回归分类器的时候，将ai看作一类标签，非ai的所有标签看作一类标签。那么相当于整个数据集里面只有两类标签：ai 和其他。</p>
<p>剩下步骤就跟我们训练正常的逻辑回归分类器一样了。</p>
<p>测试数据的时候，将查询点套用在每个逻辑回归分类器中的Sigmoid 函数，取值最高的对应标签为查询点的标签。</p>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第4章 朴素贝叶斯</title>
    <url>/2020/06/01/4.%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
    <content><![CDATA[<h2 id="朴素贝叶斯-概述"><a href="#朴素贝叶斯-概述" class="headerlink" title="朴素贝叶斯 概述"></a>朴素贝叶斯 概述</h2><p><code>贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种: 朴素贝叶斯分类。</code></p>
<a id="more"></a>
<h2 id="贝叶斯理论-amp-条件概率"><a href="#贝叶斯理论-amp-条件概率" class="headerlink" title="贝叶斯理论 &amp; 条件概率"></a>贝叶斯理论 &amp; 条件概率</h2><h3 id="贝叶斯理论"><a href="#贝叶斯理论" class="headerlink" title="贝叶斯理论"></a>贝叶斯理论</h3><p>我们现在有一个数据集，它由两类数据组成，数据分布如下图所示：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83.png" alt="朴素贝叶斯示例数据分布" title="参数已知的概率分布"></p>
<p>我们现在用 p1(x,y) 表示数据点 (x,y) 属于类别 1（图中用圆点表示的类别）的概率，用 p2(x,y) 表示数据点 (x,y) 属于类别 2（图中三角形表示的类别）的概率，那么对于一个新数据点 (x,y)，可以用下面的规则来判断它的类别：</p>
<ul>
<li>如果 p1(x,y) &gt; p2(x,y) ，那么类别为1</li>
<li>如果 p2(x,y) &gt; p1(x,y) ，那么类别为2</li>
</ul>
<p>也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。</p>
<h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>如果你对 p(x,y|c1) 符号很熟悉，那么可以跳过本小节。</p>
<p>有一个装了 7 块石头的罐子，其中 3 块是白色的，4 块是黑色的。如果从罐子中随机取出一块石头，那么是白色石头的可能性是多少？由于取石头有 7 种可能，其中 3 种为白色，所以取出白色石头的概率为 3/7 。那么取到黑色石头的概率又是多少呢？很显然，是 4/7 。我们使用 P(white) 来表示取到白色石头的概率，其概率值可以通过白色石头数目除以总的石头数目来得到。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/NB_2.png" alt="包含 7 块石头的集合"></p>
<p>如果这 7 块石头如下图所示，放在两个桶中，那么上述概率应该如何计算？</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/NB_3.png" alt="7块石头放入两个桶中"></p>
<p>计算 P(white) 或者 P(black) ，如果事先我们知道石头所在桶的信息是会改变结果的。这就是所谓的条件概率（conditional probablity）。假定计算的是从 B 桶取到白色石头的概率，这个概率可以记作 P(white|bucketB) ，我们称之为“在已知石头出自 B 桶的条件下，取出白色石头的概率”。很容易得到，P(white|bucketA) 值为 2/4 ，P(white|bucketB) 的值为 1/3 。</p>
<p>条件概率的计算公式如下：</p>
<p>P(white|bucketB) = P(white and bucketB) / P(bucketB)</p>
<p>首先，我们用 B 桶中白色石头的个数除以两个桶中总的石头数，得到 P(white and bucketB) = 1/7 .其次，由于 B 桶中有 3 块石头，而总石头数为 7 ，于是 P(bucketB) 就等于 3/7 。于是又 P(white|bucketB) = P(white and bucketB) / P(bucketB) = (1/7) / (3/7) = 1/3 。</p>
<p>另外一种有效计算条件概率的方法称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知 P(x|c)，要求 P(c|x)，那么可以使用下面的计算方法：</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/NB_4.png" alt="计算p(c|x)的方法"></p>
<h3 id="使用条件概率来分类"><a href="#使用条件概率来分类" class="headerlink" title="使用条件概率来分类"></a>使用条件概率来分类</h3><p>上面我们提到贝叶斯决策理论要求计算两个概率 p1(x, y) 和 p2(x, y):</p>
<ul>
<li>如果 p1(x, y) &gt; p2(x, y), 那么属于类别 1;</li>
<li>如果 p2(x, y) &gt; p1(X, y), 那么属于类别 2.</li>
</ul>
<p>这并不是贝叶斯决策理论的所有内容。使用 p1() 和 p2() 只是为了尽可能简化描述，而真正需要计算和比较的是 p(c1|x, y) 和 p(c2|x, y) .这些符号所代表的具体意义是: 给定某个由 x、y 表示的数据点，那么该数据点来自类别 c1 的概率是多少？数据点来自类别 c2 的概率又是多少？注意这些概率与概率 p(x, y|c1) 并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到: </p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/NB_5.png" alt="应用贝叶斯准则"></p>
<p>使用上面这些定义，可以定义贝叶斯分类准则为:</p>
<ul>
<li>如果 P(c1|x, y) &gt; P(c2|x, y), 那么属于类别 c1;</li>
<li>如果 P(c2|x, y) &gt; P(c1|x, y), 那么属于类别 c2.</li>
</ul>
<p>在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。</p>
<p>我们假设特征之间  <strong>相互独立</strong> 。所谓 <b>独立(independence)</b> 指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，比如说，“我们”中的“我”和“们”出现的概率与这两个字相邻没有任何关系。这个假设正是朴素贝叶斯分类器中 朴素(naive) 一词的含义。朴素贝叶斯分类器中的另一个假设是，<b>每个特征同等重要</b>。</p>
<p><b>Note:</b> 朴素贝叶斯分类器通常有两种实现方式: 一种基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。</p>
<h2 id="朴素贝叶斯-场景"><a href="#朴素贝叶斯-场景" class="headerlink" title="朴素贝叶斯 场景"></a>朴素贝叶斯 场景</h2><p>机器学习的一个重要应用就是文档的自动分类。</p>
<p>在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。</p>
<p>朴素贝叶斯是上面介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。下面我们会进行一些朴素贝叶斯分类的实践项目。</p>
<h2 id="朴素贝叶斯-原理"><a href="#朴素贝叶斯-原理" class="headerlink" title="朴素贝叶斯 原理"></a>朴素贝叶斯 原理</h2><h3 id="朴素贝叶斯-工作原理"><a href="#朴素贝叶斯-工作原理" class="headerlink" title="朴素贝叶斯 工作原理"></a>朴素贝叶斯 工作原理</h3><figure class="highlight 1c"><table><tr><td class="code"><pre><span class="line">提取所有文档中的词条并进行去重</span><br><span class="line">获取文档的所有类别</span><br><span class="line">计算每个类别中的文档数目</span><br><span class="line">对每篇训练文档: </span><br><span class="line">    对每个类别: </span><br><span class="line">        如果词条出现在文档中--&gt;增加该词条的计数值（for循环或者矩阵相加）</span><br><span class="line">        增加所有词条的计数值（此类别下词条总数）</span><br><span class="line">对每个类别: </span><br><span class="line">    对每个词条: </span><br><span class="line">        将该词条的数目除以总词条数目得到的条件概率（P(词条<span class="string">|类别)）</span></span><br><span class="line">返回该文档属于每个类别的条件概率（P(类别<span class="string">|文档的所有词条)）</span></span><br></pre></td></tr></table></figure>

<h3 id="朴素贝叶斯-开发流程"><a href="#朴素贝叶斯-开发流程" class="headerlink" title="朴素贝叶斯 开发流程"></a>朴素贝叶斯 开发流程</h3><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 可以使用任何方法。</span></span><br><span class="line"><span class="section">准备数据: 需要数值型或者布尔型数据。</span></span><br><span class="line"><span class="section">分析数据: 有大量特征时，绘制特征作用不大，此时使用直方图效果更好。</span></span><br><span class="line"><span class="section">训练算法: 计算不同的独立特征的条件概率。</span></span><br><span class="line"><span class="section">测试算法: 计算错误率。</span></span><br><span class="line"><span class="section">使用算法: 一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。</span></span><br></pre></td></tr></table></figure>

<h3 id="朴素贝叶斯-算法特点"><a href="#朴素贝叶斯-算法特点" class="headerlink" title="朴素贝叶斯 算法特点"></a>朴素贝叶斯 算法特点</h3><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">优点: 在数据较少的情况下仍然有效，可以处理多类别问题。</span></span><br><span class="line"><span class="section">缺点: 对于输入数据的准备方式较为敏感。</span></span><br><span class="line"><span class="section">适用数据类型: 标称型数据。</span></span><br></pre></td></tr></table></figure>

<h2 id="朴素贝叶斯-项目案例"><a href="#朴素贝叶斯-项目案例" class="headerlink" title="朴素贝叶斯 项目案例"></a>朴素贝叶斯 项目案例</h2><h3 id="项目案例1-屏蔽社区留言板的侮辱性言论"><a href="#项目案例1-屏蔽社区留言板的侮辱性言论" class="headerlink" title="项目案例1: 屏蔽社区留言板的侮辱性言论"></a>项目案例1: 屏蔽社区留言板的侮辱性言论</h3><p><a href="/src/py2.x/ml/4.NaiveBayes/bayes.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py</a></p>
<h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p>构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此问题建立两个类别: 侮辱类和非侮辱类，使用 1 和 0 分别表示。</p>
<h4 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 可以使用任何方法</span></span><br><span class="line"><span class="section">准备数据: 从文本中构建词向量</span></span><br><span class="line"><span class="section">分析数据: 检查词条确保解析的正确性</span></span><br><span class="line"><span class="section">训练算法: 从词向量计算概率</span></span><br><span class="line"><span class="section">测试算法: 根据现实情况修改分类器</span></span><br><span class="line"><span class="section">使用算法: 对社区留言板言论进行分类</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 可以使用任何方法</p>
</blockquote>
<p>本例是我们自己构造的词表:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    创建数据集</span></span><br><span class="line"><span class="string">    :return: 单词列表postingList, 所属类别classVec</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    postingList = [[<span class="string">'my'</span>, <span class="string">'dog'</span>, <span class="string">'has'</span>, <span class="string">'flea'</span>, <span class="string">'problems'</span>, <span class="string">'help'</span>, <span class="string">'please'</span>], <span class="comment">#[0,0,1,1,1......]</span></span><br><span class="line">                   [<span class="string">'maybe'</span>, <span class="string">'not'</span>, <span class="string">'take'</span>, <span class="string">'him'</span>, <span class="string">'to'</span>, <span class="string">'dog'</span>, <span class="string">'park'</span>, <span class="string">'stupid'</span>],</span><br><span class="line">                   [<span class="string">'my'</span>, <span class="string">'dalmation'</span>, <span class="string">'is'</span>, <span class="string">'so'</span>, <span class="string">'cute'</span>, <span class="string">'I'</span>, <span class="string">'love'</span>, <span class="string">'him'</span>],</span><br><span class="line">                   [<span class="string">'stop'</span>, <span class="string">'posting'</span>, <span class="string">'stupid'</span>, <span class="string">'worthless'</span>, <span class="string">'garbage'</span>],</span><br><span class="line">                   [<span class="string">'mr'</span>, <span class="string">'licks'</span>, <span class="string">'ate'</span>, <span class="string">'my'</span>, <span class="string">'steak'</span>, <span class="string">'how'</span>, <span class="string">'to'</span>, <span class="string">'stop'</span>, <span class="string">'him'</span>],</span><br><span class="line">                   [<span class="string">'quit'</span>, <span class="string">'buying'</span>, <span class="string">'worthless'</span>, <span class="string">'dog'</span>, <span class="string">'food'</span>, <span class="string">'stupid'</span>]]</span><br><span class="line">    classVec = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 1 is abusive, 0 not</span></span><br><span class="line">    <span class="keyword">return</span> postingList, classVec</span><br></pre></td></tr></table></figure>

<blockquote>
<p>准备数据: 从文本中构建词向量</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    获取所有单词的集合</span></span><br><span class="line"><span class="string">    :param dataSet: 数据集</span></span><br><span class="line"><span class="string">    :return: 所有单词的集合(即不含重复元素的单词列表)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    vocabSet = set([])  <span class="comment"># create empty set</span></span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 操作符 | 用于求两个集合的并集</span></span><br><span class="line">        vocabSet = vocabSet | set(document)  <span class="comment"># union of the two sets</span></span><br><span class="line">    <span class="keyword">return</span> list(vocabSet)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2Vec</span><span class="params">(vocabList, inputSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    遍历查看该单词是否出现，出现该单词则将该单词置1</span></span><br><span class="line"><span class="string">    :param vocabList: 所有单词集合列表</span></span><br><span class="line"><span class="string">    :param inputSet: 输入数据集</span></span><br><span class="line"><span class="string">    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 创建一个和词汇表等长的向量，并将其元素都设置为0</span></span><br><span class="line">    returnVec = [<span class="number">0</span>] * len(vocabList)<span class="comment"># [0,0......]</span></span><br><span class="line">    <span class="comment"># 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"the word: %s is not in my Vocabulary!"</span> % word</span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<p>检查函数执行情况，检查词表，不出现重复单词，需要的话，可以对其进行排序。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>listOPosts, listClasses = bayes.loadDataSet()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>myVocabList = bayes.createVocabList(listOPosts)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>myVocabList</span><br><span class="line">[<span class="string">'cute'</span>, <span class="string">'love'</span>, <span class="string">'help'</span>, <span class="string">'garbage'</span>, <span class="string">'quit'</span>, <span class="string">'I'</span>, <span class="string">'problems'</span>, <span class="string">'is'</span>, <span class="string">'park'</span>, </span><br><span class="line"><span class="string">'stop'</span>, <span class="string">'flea'</span>, <span class="string">'dalmation'</span>, <span class="string">'licks'</span>, <span class="string">'food'</span>, <span class="string">'not'</span>, <span class="string">'him'</span>, <span class="string">'buying'</span>, <span class="string">'posting'</span>, <span class="string">'has'</span>, <span class="string">'worthless'</span>, <span class="string">'ate'</span>, <span class="string">'to'</span>, <span class="string">'maybe'</span>, <span class="string">'please'</span>, <span class="string">'dog'</span>, <span class="string">'how'</span>, </span><br><span class="line"><span class="string">'stupid'</span>, <span class="string">'so'</span>, <span class="string">'take'</span>, <span class="string">'mr'</span>, <span class="string">'steak'</span>, <span class="string">'my'</span>]</span><br></pre></td></tr></table></figure>

<p>检查函数有效性。例如：myVocabList 中索引为 2 的元素是什么单词？应该是是 help 。该单词在第一篇文档中出现了，现在检查一下看看它是否出现在第四篇文档中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>bayes.setOfWords2Vec(myVocabList, listOPosts[<span class="number">0</span>])</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bayes.setOfWords2Vec(myVocabList, listOPosts[<span class="number">3</span>])</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>训练算法: 从词向量计算概率</p>
</blockquote>
<p>现在已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。接下来我们重写贝叶斯准则，将之前的 x, y 替换为 <b>w</b>. 粗体的 <b>w</b> 表示这是一个向量，即它由多个值组成。在这个例子中，数值个数与词汇表中的词个数相同。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/NB_6.png" alt="重写贝叶斯准则"></p>
<p>我们使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。</p>
<p>问: 上述代码实现中，为什么没有计算P(w)？</p>
<p>答：根据上述公式可知，我们右边的式子等同于左边的式子，由于对于每个ci，P(w)是固定的。并且我们只需要比较左边式子值的大小来决策分类，那么我们就可以简化为通过比较右边分子值得大小来做决策分类。</p>
<p>首先可以通过类别 i (侮辱性留言或者非侮辱性留言)中的文档数除以总的文档数来计算概率 p(ci) 。接下来计算 p(<b>w</b> | ci) ，这里就要用到朴素贝叶斯假设。如果将 w 展开为一个个独立特征，那么就可以将上述概率写作 p(w0, w1, w2…wn | ci) 。这里假设所有词都互相独立，该假设也称作条件独立性假设（例如 A 和 B 两个人抛骰子，概率是互不影响的，也就是相互独立的，A 抛 2点的同时 B 抛 3 点的概率就是 1/6 * 1/6），它意味着可以使用 p(w0 | ci)p(w1 | ci)p(w2 | ci)…p(wn | ci) 来计算上述概率，这样就极大地简化了计算的过程。</p>
<p>朴素贝叶斯分类器训练函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练数据原版</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵 [[1,0,1,1,1....],[],[]...]</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 文件数</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    <span class="comment"># 单词数</span></span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率，即trainCategory中所有的1的个数，</span></span><br><span class="line">    <span class="comment"># 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = sum(trainCategory) / float(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    p0Num = zeros(numWords) <span class="comment"># [0,0,0,.....]</span></span><br><span class="line">    p1Num = zeros(numWords) <span class="comment"># [0,0,0,.....]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数</span></span><br><span class="line">    p0Denom = <span class="number">0.0</span></span><br><span class="line">    p1Denom = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="comment"># 是否是侮辱性文件</span></span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果是侮辱性文件，对侮辱性文件的向量进行加和</span></span><br><span class="line">            p1Num += trainMatrix[i] <span class="comment">#[0,1,1,....] + [0,1,1,....]-&gt;[0,2,2,...]</span></span><br><span class="line">            <span class="comment"># 对向量中的所有元素进行求和，也就是计算所有侮辱性文件中出现的单词总数</span></span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表</span></span><br><span class="line">    <span class="comment"># 即 在1类别下，每个单词出现的概率</span></span><br><span class="line">    p1Vect = p1Num / p1Denom<span class="comment"># [1,2,3,5]/90-&gt;[1/90,...]</span></span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表</span></span><br><span class="line">    <span class="comment"># 即 在0类别下，每个单词出现的概率</span></span><br><span class="line">    p0Vect = p0Num / p0Denom</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 根据现实情况修改分类器</p>
</blockquote>
<p>在利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算 p(w0|1) * p(w1|1) * p(w2|1)。如果其中一个概率值为 0，那么最后的乘积也为 0。为降低这种影响，可以将所有词的出现数初始化为 1，并将分母初始化为 2 （取1 或 2 的目的主要是为了保证分子和分母不为0，大家可以根据业务需求进行更改）。</p>
<p>另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积 p(w0|ci) * p(w1|ci) * p(w2|ci)… p(wn|ci) 时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（用 Python 尝试相乘许多很小的数，最后四舍五入后会得到 0）。一种解决办法是对乘积取自然对数。在代数中有 ln(a * b) = ln(a) + ln(b), 于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。</p>
<p>下图给出了函数 f(x) 与 ln(f(x)) 的曲线。可以看出，它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。</p>
<p><img src="http://data.apachecn.org/img/AiLearning/ml/4.NaiveBayesian/NB_7.png" alt="函数图像"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练数据优化版本</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 总文件数</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    <span class="comment"># 总单词数</span></span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = sum(trainCategory) / float(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    <span class="comment"># p0Num 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Num 侮辱的统计</span></span><br><span class="line">    p0Num = ones(numWords)<span class="comment">#[0,0......]-&gt;[1,1,1,1,1.....]</span></span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span></span><br><span class="line">    <span class="comment"># p0Denom 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Denom 侮辱的统计</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 累加辱骂词的频次</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 对每篇文章的辱骂的频次 进行统计汇总</span></span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line">    p1Vect = log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line">    p0Vect = log(p0Num / p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></figure>


<blockquote>
<p>使用算法: 对社区留言板言论进行分类</p>
</blockquote>
<p>朴素贝叶斯分类函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span><span class="params">(vec2Classify, p0Vec, p1Vec, pClass1)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    使用算法：</span></span><br><span class="line"><span class="string">        # 将乘法转换为加法</span></span><br><span class="line"><span class="string">        乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)</span></span><br><span class="line"><span class="string">        加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -&gt; log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))</span></span><br><span class="line"><span class="string">    :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量</span></span><br><span class="line"><span class="string">    :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line"><span class="string">    :param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line"><span class="string">    :param pClass1: 类别1，侮辱性文件的出现概率</span></span><br><span class="line"><span class="string">    :return: 类别1 or 0</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))</span></span><br><span class="line">    <span class="comment"># 大家可能会发现，上面的计算公式，没有除以贝叶斯准则的公式的分母，也就是 P(w) （P(w) 指的是此文档在所有的文档中出现的概率）就进行概率大小的比较了，</span></span><br><span class="line">    <span class="comment"># 因为 P(w) 针对的是包含侮辱和非侮辱的全部文档，所以 P(w) 是相同的。</span></span><br><span class="line">    <span class="comment"># 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。</span></span><br><span class="line">    <span class="comment"># 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来</span></span><br><span class="line">    p1 = sum(vec2Classify * p1Vec) + log(pClass1) <span class="comment"># P(w|c1) * P(c1) ，即贝叶斯准则的分子</span></span><br><span class="line">    p0 = sum(vec2Classify * p0Vec) + log(<span class="number">1.0</span> - pClass1) <span class="comment"># P(w|c0) * P(c0) ，即贝叶斯准则的分子·</span></span><br><span class="line">    <span class="keyword">if</span> p1 &gt; p0:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testingNB</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    测试朴素贝叶斯算法</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 1. 加载数据集</span></span><br><span class="line">    listOPosts, listClasses = loadDataSet()</span><br><span class="line">    <span class="comment"># 2. 创建单词集合</span></span><br><span class="line">    myVocabList = createVocabList(listOPosts)</span><br><span class="line">    <span class="comment"># 3. 计算单词是否出现并创建数据矩阵</span></span><br><span class="line">    trainMat = []</span><br><span class="line">    <span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOPosts:</span><br><span class="line">        <span class="comment"># 返回m*len(myVocabList)的矩阵， 记录的都是0，1信息</span></span><br><span class="line">        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))</span><br><span class="line">    <span class="comment"># 4. 训练数据</span></span><br><span class="line">    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))</span><br><span class="line">    <span class="comment"># 5. 测试数据</span></span><br><span class="line">    testEntry = [<span class="string">'love'</span>, <span class="string">'my'</span>, <span class="string">'dalmation'</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="keyword">print</span> testEntry, <span class="string">'classified as: '</span>, classifyNB(thisDoc, p0V, p1V, pAb)</span><br><span class="line">    testEntry = [<span class="string">'stupid'</span>, <span class="string">'garbage'</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="keyword">print</span> testEntry, <span class="string">'classified as: '</span>, classifyNB(thisDoc, p0V, p1V, pAb)</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 项目案例2: 使用朴素贝叶斯过滤垃圾邮件</span></span><br><span class="line"></span><br><span class="line">[完整代码地址](/src/py2.x/ml/<span class="number">4.</span>NaiveBayes/bayes.py): &lt;https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/<span class="number">4.</span>NaiveBayes/bayes.py&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 项目概述</span></span><br><span class="line"></span><br><span class="line">完成朴素贝叶斯的一个最著名的应用: 电子邮件垃圾过滤。</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 开发流程</span></span><br><span class="line"></span><br><span class="line">使用朴素贝叶斯对电子邮件进行分类</span><br></pre></td></tr></table></figure>
<p>收集数据: 提供文本文件<br>准备数据: 将文本文件解析成词条向量<br>分析数据: 检查词条确保解析的正确性<br>训练算法: 使用我们之前建立的 trainNB() 函数<br>测试算法: 使用朴素贝叶斯进行交叉验证<br>使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="quote">&gt; 收集数据: 提供文本文件</span></span><br><span class="line"></span><br><span class="line">文本文件内容如下:</span><br></pre></td></tr></table></figure>
<p>Hi Peter,</p>
<p>With Jose out of town, do you want to<br>meet once in a while to keep things<br>going and do some interesting stuff?</p>
<p>Let me know<br>Eugene</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 准备数据: 将文本文件解析成词条向量</span><br><span class="line"></span><br><span class="line">使用正则表达式来切分文本</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mySent = <span class="string">'This book is the best book on Python or M.L. I have ever laid eyes upon.'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> re</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>regEx = re.compile(<span class="string">'\\W*'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>listOfTokens = regEx.split(mySent)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>listOfTokens</span><br><span class="line">[<span class="string">'This'</span>, <span class="string">'book'</span>, <span class="string">'is'</span>, <span class="string">'the'</span>, <span class="string">'best'</span>, <span class="string">'book'</span>, <span class="string">'on'</span>, <span class="string">'Python'</span>, <span class="string">'or'</span>, <span class="string">'M.L.'</span>, <span class="string">'I'</span>, <span class="string">'have'</span>, <span class="string">'ever'</span>, <span class="string">'laid'</span>, <span class="string">'eyes'</span>, <span class="string">'upon'</span>, <span class="string">''</span>]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<blockquote>
<p>训练算法: 使用我们之前建立的 trainNB0() 函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练数据优化版本</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 总文件数</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    <span class="comment"># 总单词数</span></span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = sum(trainCategory) / float(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    <span class="comment"># p0Num 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Num 侮辱的统计</span></span><br><span class="line">    p0Num = ones(numWords)<span class="comment">#[0,0......]-&gt;[1,1,1,1,1.....]</span></span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span></span><br><span class="line">    <span class="comment"># p0Denom 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Denom 侮辱的统计</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 累加辱骂词的频次</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 对每篇文章的辱骂的频次 进行统计汇总</span></span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line">    p1Vect = log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line">    p0Vect = log(p0Num / p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 使用朴素贝叶斯进行交叉验证</p>
</blockquote>
<p>文件解析及完整的垃圾邮件测试函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 切分文本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span><span class="params">(bigString)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        接收一个大字符串并将其解析为字符串列表</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        bigString -- 大字符串</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        去掉少于 2 个字符的字符串，并将所有字符串转换为小写，返回字符串列表</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="comment"># 使用正则表达式来切分句子，其中分隔符是除单词、数字外的任意字符串</span></span><br><span class="line">    listOfTokens = re.split(<span class="string">r'\W*'</span>, bigString)</span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> listOfTokens <span class="keyword">if</span> len(tok) &gt; <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spamTest</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Desc:</span></span><br><span class="line"><span class="string">        对贝叶斯垃圾邮件分类器进行自动化处理。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        none</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        对测试集中的每封邮件进行分类，若邮件分类错误，则错误数加 1，最后返回总的错误百分比。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    docList = []</span><br><span class="line">    classList = []</span><br><span class="line">    fullText = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">26</span>):</span><br><span class="line">        <span class="comment"># 切分，解析数据，并归类为 1 类别</span></span><br><span class="line">        wordList = textParse(open(<span class="string">'data/4.NaiveBayes/email/spam/%d.txt'</span> % i).read())</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        classList.append(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 切分，解析数据，并归类为 0 类别</span></span><br><span class="line">        wordList = textParse(open(<span class="string">'data/4.NaiveBayes/email/ham/%d.txt'</span> % i).read())</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 创建词汇表    </span></span><br><span class="line">    vocabList = createVocabList(docList)</span><br><span class="line">    trainingSet = range(<span class="number">50</span>)</span><br><span class="line">    testSet = []</span><br><span class="line">    <span class="comment"># 随机取 10 个邮件用来测试</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="comment"># random.uniform(x, y) 随机生成一个范围为 x ~ y 的实数</span></span><br><span class="line">        randIndex = int(random.uniform(<span class="number">0</span>, len(trainingSet)))</span><br><span class="line">        testSet.append(trainingSet[randIndex])</span><br><span class="line">        <span class="keyword">del</span>(trainingSet[randIndex])</span><br><span class="line">    trainMat = []</span><br><span class="line">    trainClasses = []</span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> trainingSet:</span><br><span class="line">        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))</span><br><span class="line">        trainClasses.append(classList[docIndex])</span><br><span class="line">    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testSet:</span><br><span class="line">        wordVector = setOfWords2Vec(vocabList, docList[docIndex])</span><br><span class="line">        <span class="keyword">if</span> classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'the errorCount is: '</span>, errorCount</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'the testSet length is :'</span>, len(testSet)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'the error rate is :'</span>, float(errorCount)/len(testSet)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上</p>
</blockquote>
<h3 id="项目案例3-使用朴素贝叶斯分类器从个人广告中获取区域倾向"><a href="#项目案例3-使用朴素贝叶斯分类器从个人广告中获取区域倾向" class="headerlink" title="项目案例3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向"></a>项目案例3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向</h3><p><a href="/src/py2.x/ml/4.NaiveBayes/bayes.py">完整代码地址</a>: <a href="https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning/blob/master/src/py2.x/ml/4.NaiveBayes/bayes.py</a></p>
<h4 id="项目概述-1"><a href="#项目概述-1" class="headerlink" title="项目概述"></a>项目概述</h4><p>广告商往往想知道关于一个人的一些特定人口统计信息，以便能更好地定向推销广告。</p>
<p>我们将分别从美国的两个城市中选取一些人，通过分析这些人发布的信息，来比较这两个城市的人们在广告用词上是否不同。如果结论确实不同，那么他们各自常用的词是哪些，从人们的用词当中，我们能否对不同城市的人所关心的内容有所了解。</p>
<h4 id="开发流程-1"><a href="#开发流程-1" class="headerlink" title="开发流程"></a>开发流程</h4><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口</span></span><br><span class="line"><span class="section">准备数据: 将文本文件解析成词条向量</span></span><br><span class="line"><span class="section">分析数据: 检查词条确保解析的正确性</span></span><br><span class="line"><span class="section">训练算法: 使用我们之前建立的 trainNB0() 函数</span></span><br><span class="line"><span class="section">测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果</span></span><br><span class="line"><span class="section">使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口</p>
</blockquote>
<p>也就是导入 RSS 源，我们使用 python 下载文本，在<a href="http://code.google.com/p/feedparser/" target="_blank" rel="noopener">http://code.google.com/p/feedparser/</a> 下浏览相关文档，安装 feedparse，首先解压下载的包，并将当前目录切换到解压文件所在的文件夹，然后在 python 提示符下输入：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>python setup.py install</span><br></pre></td></tr></table></figure>

<blockquote>
<p>准备数据: 将文本文件解析成词条向量</p>
</blockquote>
<p>文档词袋模型</p>
<p>我们将每个词的出现与否作为一个特征，这可以被描述为 <b>词集模型(set-of-words model)</b>。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为 <b>词袋模型(bag-of-words model)</b>。在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数 setOfWords2Vec() 稍加修改，修改后的函数为 bagOfWords2Vec() 。</p>
<p>如下给出了基于词袋模型的朴素贝叶斯代码。它与函数 setOfWords2Vec() 几乎完全相同，唯一不同的是每当遇到一个单词时，它会增加词向量中的对应值，而不只是将对应的数值设为 1 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2VecMN</span><span class="params">(vocaList, inputSet)</span>:</span></span><br><span class="line">    returnVec = [<span class="number">0</span>] * len(vocabList)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocaList:</span><br><span class="line">            returnVec[vocabList.index(word)] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建一个包含在所有文档中出现的不重复词的列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    vocabSet=set([])    <span class="comment">#创建一个空集</span></span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</span><br><span class="line">        vocabSet=vocabSet|set(document)   <span class="comment">#创建两个集合的并集</span></span><br><span class="line">    <span class="keyword">return</span> list(vocabSet)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2VecMN</span><span class="params">(vocabList,inputSet)</span>:</span></span><br><span class="line">    returnVec=[<span class="number">0</span>]*len(vocabList)  <span class="comment">#创建一个其中所含元素都为0的向量</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">                returnVec[vocabList.index(word)]+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br><span class="line"></span><br><span class="line"><span class="comment">#文件解析</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span><span class="params">(bigString)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    listOfTokens=re.split(<span class="string">r'\W*'</span>,bigString)</span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> listOfTokens <span class="keyword">if</span> len(tok)&gt;<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>分析数据: 检查词条确保解析的正确性</p>
</blockquote>
<blockquote>
<p>训练算法: 使用我们之前建立的 trainNB0() 函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练数据优化版本</span></span><br><span class="line"><span class="string">    :param trainMatrix: 文件单词矩阵</span></span><br><span class="line"><span class="string">    :param trainCategory: 文件对应的类别</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 总文件数</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    <span class="comment"># 总单词数</span></span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 侮辱性文件的出现概率</span></span><br><span class="line">    pAbusive = sum(trainCategory) / float(numTrainDocs)</span><br><span class="line">    <span class="comment"># 构造单词出现次数列表</span></span><br><span class="line">    <span class="comment"># p0Num 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Num 侮辱的统计 </span></span><br><span class="line">    <span class="comment"># 避免单词列表中的任何一个单词为0，而导致最后的乘积为0，所以将每个单词的出现次数初始化为 1</span></span><br><span class="line">    p0Num = ones(numWords)<span class="comment">#[0,0......]-&gt;[1,1,1,1,1.....]</span></span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）</span></span><br><span class="line">    <span class="comment"># p0Denom 正常的统计</span></span><br><span class="line">    <span class="comment"># p1Denom 侮辱的统计</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 累加辱骂词的频次</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 对每篇文章的辱骂的频次 进行统计汇总</span></span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表</span></span><br><span class="line">    p1Vect = log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表</span></span><br><span class="line">    p0Vect = log(p0Num / p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br></pre></td></tr></table></figure>

<blockquote>
<p>测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#RSS源分类器及高频词去除函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcMostFreq</span><span class="params">(vocabList,fullText)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> operator</span><br><span class="line">    freqDict=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> vocabList:  <span class="comment">#遍历词汇表中的每个词</span></span><br><span class="line">        freqDict[token]=fullText.count(token)  <span class="comment">#统计每个词在文本中出现的次数</span></span><br><span class="line">    sortedFreq=sorted(freqDict.iteritems(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)  <span class="comment">#根据每个词出现的次数从高到底对字典进行排序</span></span><br><span class="line">    <span class="keyword">return</span> sortedFreq[:<span class="number">30</span>]   <span class="comment">#返回出现次数最高的30个单词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">localWords</span><span class="params">(feed1,feed0)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> feedparser</span><br><span class="line">    docList=[];classList=[];fullText=[]</span><br><span class="line">    minLen=min(len(feed1[<span class="string">'entries'</span>]),len(feed0[<span class="string">'entries'</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(minLen):</span><br><span class="line">        wordList=textParse(feed1[<span class="string">'entries'</span>][i][<span class="string">'summary'</span>])   <span class="comment">#每次访问一条RSS源</span></span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">1</span>)</span><br><span class="line">        wordList=textParse(feed0[<span class="string">'entries'</span>][i][<span class="string">'summary'</span>])</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">0</span>)</span><br><span class="line">    vocabList=createVocabList(docList)</span><br><span class="line">    top30Words=calcMostFreq(vocabList,fullText)</span><br><span class="line">    <span class="keyword">for</span> pairW <span class="keyword">in</span> top30Words:</span><br><span class="line">        <span class="keyword">if</span> pairW[<span class="number">0</span>] <span class="keyword">in</span> vocabList:vocabList.remove(pairW[<span class="number">0</span>])    <span class="comment">#去掉出现次数最高的那些词</span></span><br><span class="line">    trainingSet=range(<span class="number">2</span>*minLen);testSet=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        randIndex=int(random.uniform(<span class="number">0</span>,len(trainingSet)))</span><br><span class="line">        testSet.append(trainingSet[randIndex])</span><br><span class="line">        <span class="keyword">del</span>(trainingSet[randIndex])</span><br><span class="line">    trainMat=[];trainClasses=[]</span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> trainingSet:</span><br><span class="line">        trainMat.append(bagOfWords2VecMN(vocabList,docList[docIndex]))</span><br><span class="line">        trainClasses.append(classList[docIndex])</span><br><span class="line">    p0V,p1V,pSpam=trainNBO(array(trainMat),array(trainClasses))</span><br><span class="line">    errorCount=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testSet:</span><br><span class="line">        wordVector=bagOfWords2VecMN(vocabList,docList[docIndex])</span><br><span class="line">        <span class="keyword">if</span> classifyNB(array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]:</span><br><span class="line">            errorCount+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'the error rate is:'</span>,float(errorCount)/len(testSet)</span><br><span class="line">    <span class="keyword">return</span> vocabList,p0V,p1V</span><br><span class="line"></span><br><span class="line"><span class="comment">#朴素贝叶斯分类函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span><span class="params">(vec2Classify,p0Vec,p1Vec,pClass1)</span>:</span></span><br><span class="line">    p1=sum(vec2Classify*p1Vec)+log(pClass1)</span><br><span class="line">    p0=sum(vec2Classify*p0Vec)+log(<span class="number">1.0</span>-pClass1)</span><br><span class="line">    <span class="keyword">if</span> p1&gt;p0:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词</p>
</blockquote>
<p>函数 localWords() 使用了两个 RSS 源作为参数，RSS 源要在函数外导入，这样做的原因是 RSS 源会随时间而改变，重新加载 RSS 源就会得到新的数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>reload(bayes)</span><br><span class="line">&lt;module <span class="string">'bayes'</span> <span class="keyword">from</span> <span class="string">'bayes.pyc'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> feedparser</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ny=feedparser.parse(<span class="string">'http://newyork.craigslist.org/stp/index.rss'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sy=feedparser.parse(<span class="string">'http://sfbay.craigslist.org/stp/index.rss'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabList,pSF,pNY=bayes.localWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabList,pSF,pNY=bayes.localWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabList,pSF,pNY=bayes.localWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.55</span></span><br></pre></td></tr></table></figure>
<p>为了得到错误率的精确估计，应该多次进行上述实验，然后取平均值</p>
<p>接下来，我们要分析一下数据，显示地域相关的用词</p>
<p>可以先对向量pSF与pNY进行排序，然后按照顺序打印出来，将下面的代码添加到文件中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#最具表征性的词汇显示函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTopWords</span><span class="params">(ny,sf)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> operator</span><br><span class="line">    vocabList,p0V,p1V=localWords(ny,sf)</span><br><span class="line">    topNY=[];topSF=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(p0V)):</span><br><span class="line">        <span class="keyword">if</span> p0V[i]&gt;<span class="number">-6.0</span>:topSF.append((vocabList[i],p0V[i]))</span><br><span class="line">        <span class="keyword">if</span> p1V[i]&gt;<span class="number">-6.0</span>:topNY.append((vocabList[i],p1V[i]))</span><br><span class="line">    sortedSF=sorted(topSF,key=<span class="keyword">lambda</span> pair:pair[<span class="number">1</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**"</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> sortedSF:</span><br><span class="line">        <span class="keyword">print</span> item[<span class="number">0</span>]</span><br><span class="line">    sortedNY=sorted(topNY,key=<span class="keyword">lambda</span> pair:pair[<span class="number">1</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**"</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> sortedNY:</span><br><span class="line">        <span class="keyword">print</span> item[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>函数 getTopWords() 使用两个 RSS 源作为输入，然后训练并测试朴素贝叶斯分类器，返回使用的概率值。然后创建两个列表用于元组的存储，与之前返回排名最高的 X 个单词不同，这里可以返回大于某个阈值的所有词，这些元组会按照它们的条件概率进行排序。</p>
<p>保存 bayes.py 文件，在python提示符下输入：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>reload(bayes)</span><br><span class="line">&lt;module <span class="string">'bayes'</span> <span class="keyword">from</span> <span class="string">'bayes.pyc'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bayes.getTopWords(ny,sf)</span><br><span class="line">the error rate <span class="keyword">is</span>: <span class="number">0.55</span></span><br><span class="line">SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**</span><br><span class="line">how</span><br><span class="line">last</span><br><span class="line">man</span><br><span class="line">...</span><br><span class="line">veteran</span><br><span class="line">still</span><br><span class="line">ends</span><br><span class="line">late</span><br><span class="line">off</span><br><span class="line">own</span><br><span class="line">know</span><br><span class="line">NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**</span><br><span class="line">someone</span><br><span class="line">meet</span><br><span class="line">...</span><br><span class="line">apparel</span><br><span class="line">recalled</span><br><span class="line">starting</span><br><span class="line">strings</span><br></pre></td></tr></table></figure>

<p>当注释掉用于移除高频词的三行代码，然后比较注释前后的分类性能，去掉这几行代码之后，错误率为54%，，而保留这些代码得到的错误率为70%。这里观察到，这些留言中出现次数最多的前30个词涵盖了所有用词的30%，vocabList的大小约为3000个词，也就是说，词汇表中的一小部分单词却占据了所有文本用词的一大部分。产生这种现象的原因是因为语言中大部分都是冗余和结构辅助性内容。另一个常用的方法是不仅移除高频词，同时从某个预定高频词中移除结构上的辅助词，该词表称为停用词表。</p>
<p>从最后输出的单词，可以看出程序输出了大量的停用词，可以移除固定的停用词看看结果如何，这样做的话，分类错误率也会降低。</p>
<hr>
<ul>
<li><strong>本文转载于 <a href="http://www.apachecn.org/" target="_blank" rel="noopener">ApacheCN</a></strong></li>
<li><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">GitHub地址</a>: <a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
